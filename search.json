[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 3386 Regression Analysis",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 3386 - Regression Analysis.\nPrerequisites: MTH 2311 - Linear Algebra, MTH 2321 - Calculus III, and STA 3381 - Probability and Statistics\n\nCourse Description:\nA development of regression techniques including simple linear regression, multiple regression, logistic regression and Poisson regression with emphasis on model assumptions, parameter estimation, variable selection and diagnostics."
  },
  {
    "objectID": "01_Intro.html#the-probabilistic-model",
    "href": "01_Intro.html#the-probabilistic-model",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.1 The Probabilistic Model",
    "text": "1.1 The Probabilistic Model\nMost students who take an intro stats course are familiar with the idea of a random variable. Students are usually introduced to random variables using the notation \\(X\\).\nIn the technical sense, capital \\(X\\) should denote the random variable (that is, the function itself), while lowercase \\(x\\) denotes the values of that random variable. We will be loose on that convention, so you will see the lowercase used almost extensively even when discussing the random variable itself.\n\n\n\n\n\n\nReview: Random Variable\n\n\n\nA random variable is a function that assigns a numeric value to the outcomes in the sample space.\n\n\nIn regression, the random variable of interest is usually denoted as \\(y\\).\nWe want to predict or model (explain) this variable. Thus, we call this the response (or dependent) variable.\nIf we have measurements of this random variable, then we can express each value \\(y\\) as the mean value of \\(y\\) plus some random error.\nThat is, we can model the variable as \\[\n    y = E(y) + \\varepsilon\n\\tag{1.1}\\]\nwhere \\[\\begin{align*}\n    y = &\\text{ dependent variable}\\\\\n    E(y) =& \\text{ mean (or expected) value of } y\\\\\n    \\varepsilon =& \\text{ random error}\n\\end{align*}\\]\nThis model is referred to as a probabilistic model for \\(y\\). The term “probabilistic” is used because, under certain assumptions, we can make probability-based statements about the extent of the difference between \\(y\\) and \\(E(y)\\).\nFor example, we might assert that the error term, \\[\n    \\varepsilon = y - E(y)\n\\] follows a normal distribution.\nIn practice, we will use sample data to estimate the parameters of the probabilistic model—specifically, the mean \\(E(y)\\) and the random error \\(\\varepsilon\\).\nWe will later discuss a common assumption in regression: that the mean error is zero.\nIn other words, \\[\n    E(\\varepsilon) = 0\n\\]\nGiven this assumption, our best estimate of \\(\\varepsilon\\) is zero. Therefore, we only need to estimate \\(E(y)\\).\nThe simplest method of estimating \\(E(y)\\) is to use the sample mean of \\(y\\) which we will denote as \\[\\begin{align*}\n    \\bar y= \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{align*}\\]\nIf we desired to predict a value of \\(y\\), then our best prediction would be just the sample mean: \\[\\begin{align*}\n    \\hat y = \\bar y\n\\end{align*}\\] where \\(\\hat y\\) denotes a predicted value of \\(y\\).\nThis would be the case with univariate data (we only have one variable in our data: \\(y\\)).\nUnfortunately, this simple model does not take into consideration a number of variables, called independent variables, that may help predict the response variable.\nIndependent variables are also called predictor or explanatory variables.\nThe process of identifying the mathematical model that describes the relationship between \\(y\\) and a set of independent variables, and that best fits the data, is known as regression analysis."
  },
  {
    "objectID": "01_Intro.html#sec-regoverview",
    "href": "01_Intro.html#sec-regoverview",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.2 Overview of Regression Analysis",
    "text": "1.2 Overview of Regression Analysis\nWe will denote the independent variables as \\[\\begin{align*}\n    x_1, x_2, \\ldots, x_k\n\\end{align*}\\] where \\(k\\) is the number of independent variables.\nThe goal of regression analysis is to create a prediction equation that accurately relates \\(y\\) to independent variables, allowing us to predict \\(y\\) for given values of \\(x_1, x_2, \\ldots, x_k\\) with minimal prediction error.\nWhen predicting \\(y\\), we also need a measure of the reliability of our prediction, indicating how large the prediction error might be.\nThese elements form the core of regression analysis.\nBeyond predicting \\(y\\), a regression model can also estimate the mean value of \\(y\\) for specific values of \\(x_1, x_2, \\ldots, x_k\\) and explore the relationship between \\(y\\) and one or more independent variables.\nThe process of regression analysis typically involves six key steps:\n\nHypothesize the form of the model for \\(E(y)\\).\nCollect sample data.\nEstimate the model’s unknown parameters using the sample data.\nDefine the probability distribution of the random error term, estimate any unknown parameters, and validate the assumptions made about this distribution.\nStatistically assess the model’s usefulness.\nIf the model is effective, use it for prediction, estimation, and other purposes."
  },
  {
    "objectID": "01_Intro.html#collecting-the-data-for-regression",
    "href": "01_Intro.html#collecting-the-data-for-regression",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.3 Collecting the Data for Regression",
    "text": "1.3 Collecting the Data for Regression\nThe first step listed above will be discussed later.\nOnce you’ve proposed a model for \\(E(y)\\), the next step is to gather sample data to estimate the model.\nThis means collecting data on both the response variable \\(y\\) and the independent variables \\(x_1, x_2, \\ldots, x_k\\) for each observation in your sample. In regression analysis, the sample includes data on multiple variables: \\[\ny, x_1, x_2, \\ldots, x_k\n\\] This is known as multivariate data.\nRegression data can be either observational or experimental:\nFor observational data no control is exerted over the independent variables (\\(x\\)’s). For example, recording people’s ages and their corresponding blood pressure levels without influencing either.\nFor experimental data the independent variables are controlled or manipulated. For instance, setting different fertilizer amounts for crops to observe the impact on growth.\nSuppose you want to model a student’s annual GPA (\\(y\\)). One approach is to randomly select a sample of \\(n=100\\) students and record their GPA along with the values of each predictor variable.\nData for the first three students in the sample are shown in Table 1.1.\n\n\nTable 1.1: Values of the response variable and predictor variables for the first three students.\n\n\n\nStudent 1\nStudent 2\nStudent 3\n\n\n\n\nAnnual GPA \\(y\\)\n3.8\n2.7\n3.5\n\n\nStudy Hours per Week, \\(x_1\\)\n15\n5\n10\n\n\nClass Attendance, \\(x_2\\) (days)\n30\n20\n25\n\n\nExtracurriculars, \\(x_3\\)\n2\n1\n3\n\n\nAge, \\(x_4\\) (years)\n21\n19\n22\n\n\nEmployed, \\(x_5\\) (1 if yes, 0 if no)\n0\n1\n0\n\n\nLives On Campus, \\(x_6\\) (1 if yes, 0 if no)\n1\n0\n1\n\n\n\n\nIn this example, the \\(x\\) values, like study hours, class attendance, and extracurricular activities, are not predetermined before observing GPA \\(y\\); thus, the \\(x\\) values are uncontrolled. Therefore, the sample data are observational.\n\nDetermining Sample Size for Regression with Observational Data\nWhen applying regression to observational data, the required sample size for estimating the mean \\(E(y)\\) depends on three key factors:\n\nEstimated population standard deviation\nConfidence level\nDesired margin of error (half-width of the confidence interval)\n\nHowever, unlike the univariate case, \\(E(y)\\) is modeled as a function of multiple independent variables, which adds complexity. The sample size must be large enough to estimate and test all parameters in the model.\nTo ensure a sufficient sample size, a common guideline is to select a sample size \\(n\\) that is at least 10 times the number of parameters in the model.\nFor instance, if a university registrar’s office uses the following model for the annual GPA \\(y\\) of a current student:\n\\[E(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_6 x_6\\]\nwhere \\(x_1, x_2, \\dots, x_6\\) are defined in Table 1.1, the model includes six \\(\\beta\\) parameters (excluding \\(\\beta_0\\)). Therefore, they should include at least:\n\\[ 10 \\times 6 = 60 \\]\nstudents in the sample.\n\n\nExperimental Data\nThe second type of data in regression, experimental data, is generated through designed experiments where the independent variables are set in advance (i.e., controlled) before observing the value of \\(y\\).\nFor instance, consider a scenario where a researcher wants to study the effect of two independent variables—say, fertilizer amount \\(x_1\\) and irrigation level \\(x_2\\)—on the growth rate \\(y\\) of plants. The researcher could choose three levels of fertilizer (10g, 20g, and 30g) and three levels of irrigation (1L, 2L, and 3L) and measure the growth rate in one plant for each of the \\(3\\times 3=9\\) fertilizer–irrigation combinations (see Table 1.2 below).\n\n\nTable 1.2: Values of the response variable and two independent variables for the growth rate of plants.\n\n\nFertilizer, \\(x_1\\)\nIrrigation, \\(x_2\\)\nGrowth Rate, \\(y\\)\n\n\n\n\n10g\n1L\n5.2\n\n\n10g\n2L\n6.1\n\n\n10g\n3L\n5.8\n\n\n20g\n1L\n7.0\n\n\n20g\n2L\n7.5\n\n\n20g\n3L\n7.3\n\n\n30g\n1L\n8.4\n\n\n30g\n2L\n8.7\n\n\n30g\n3L\n8.1\n\n\n\n\nIn this experiment, the settings of the independent variables are controlled, in contrast to the uncontrolled nature of observational data, like in the real estate sales example.\nIn many studies, it is often not possible to control the values of the \\(x\\)’s, so most data collected for regression are observational.\nSo, why do we differentiate between these two types of data? We will learn that inferences from regression studies based on observational data have more limitations than those based on experimental data. Specifically, establishing a cause-and-effect relationship between variables is much more challenging with observational data than with experimental data."
  },
  {
    "objectID": "02_Fitting.html#the-straight-line-probabilistic-model",
    "href": "02_Fitting.html#the-straight-line-probabilistic-model",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.1 The Straight-Line Probabilistic Model",
    "text": "2.1 The Straight-Line Probabilistic Model\nWhen studying environmental science, consider modeling the monthly carbon dioxide (CO₂) emissions \\(y\\) of a city as a function of its monthly industrial activity \\(x\\).\nThe first question to ask is: Do you believe an exact (deterministic) relationship exists between these two variables?\nIn other words, can we predict the exact value of CO₂ emissions if industrial activity is known?\nIt’s unlikely. CO₂ emissions depend on various factors beyond industrial activity, such as weather conditions, regulatory policies, and transportation levels. Even with multiple variables in the model, it’s improbable that we could predict monthly emissions precisely.\nThere will almost certainly be some variation in emissions due to random phenomena that cannot be fully explained or modeled.\nTherefore, we should propose a probabilistic model for CO₂ emissions that accounts for this random variation:\n\\[\ny = E(y) + \\varepsilon\n\\]\nThe random error component,\\(\\varepsilon\\), captures all unexplained variations in emissions caused by omitted variables or unpredictable random factors.\nThe random error plays a key role in hypothesis testing, determining confidence intervals for the model’s deterministic portion, and estimating the prediction error when using the model to predict future values of \\(y\\).\nLet’s start with the simplest probabilistic model—a first-order linear model that graphs as a straight line."
  },
  {
    "objectID": "02_Fitting.html#a-first-order-linear-model",
    "href": "02_Fitting.html#a-first-order-linear-model",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.2 A First-Order Linear Model",
    "text": "2.2 A First-Order Linear Model\nThe first-order linear model is expessed as \\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nwhere:\n\n\\(y\\) is the dependent variable (also known as the response variable)\n\\(x\\) is the independent variable (used as a predictor of \\(y\\))\n\\(E(y) = \\beta_0 + \\beta_1 x\\) is the deterministic component\n\\(\\varepsilon\\) is the random error component\n\\(\\beta_0\\) is the y-intercept of the line (the point where the line intersects the y-axis)\n\\(\\beta_1\\) is the slope of the line (the change in the mean of \\(y\\) for every 1-unit increase in \\(x\\))\n\nWe use Greek symbols \\(\\beta_0\\) and \\(\\beta_1\\) to denote the y-intercept and slope of the line. These are population parameters with values that would only be known if we had access to the entire population of \\((x, y)\\) measurements.\n\n\n\n\n\n\nReview: Greek Letters in Notation\n\n\n\nUsually, in Statistics, lower-case Greek letters are used to denote population parameters. In our model above, we have an exception. The Greek letter \\(\\varepsilon\\) is not a parameter, but a random variable (parameters are not random variables in frequentist statistics).\n\n\nAs discussed in Section 1.2, regression can be viewed as a six-step process. For now, we’ll focus on steps 2-6, using the simple linear regression model. We’ll explore more complex models later."
  },
  {
    "objectID": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "href": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.3 Fitting the Model: The Method of Least Squares",
    "text": "2.3 Fitting the Model: The Method of Least Squares\nSuppose we have the data shown in Table 2.1 below and plotted in the scatterplot in Figure 2.1.\n\n\nTable 2.1: Data for Scatterplot\n\n\nx\ny\n\n\n\n\n1\n2\n\n\n2\n1.4\n\n\n2.75\n1.6\n\n\n4\n1.25\n\n\n6\n1\n\n\n7\n0.5\n\n\n8\n0.5\n\n\n10\n0.4\n\n\n\n\n\nlibrary(tidyverse)\nx = c(1, 2, 2.75, 4, 6, 7, 8, 10)\ny = c(2, 1.4, 1.6, 1.25, 1, 0.5, 0.5, 0.4)\ndat = tibble(x, y)\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point()\n\n\n\n\nFigure 2.1: Scatterplot of the data in Table 2.1\n\n\n\n\nWe hypothesize that a straight-line model relates y to x, as follows:\n\\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nHow can we use the data from the eight observations in Table 2.1 to estimate the unknown y-intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\))?\nWe can start by trying some lines and see how well they fit the data. But how do we measure how well a line fits the data?\nA quantitative method to evaluate how well a straight line fits a set of data is by measuring the deviations of the data points from the line.\n\n\n\n\n\n\nReview: Deviations of Response Variable\n\n\n\n\\(y\\) is the variable of interest, so we are focused on the differences between observed \\(y\\) and the predicted value of \\(y\\)\n\n\nWe calculate the magnitude of the deviations (the differences between observed and predicted values of \\(y\\)).\nThese deviations, or prediction errors, represent the vertical distances between observed and predicted values of \\(y\\).\nSuppose we try to fit the line \\[\n    \\hat{y} =2-.2x\n\\tag{2.1}\\]\nThis line can be seen in Figure 2.2.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 2, slope = -0.2, color = \"red\")\n\n\n\n\nFigure 2.2: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.1\n\n\n\n\nThe observed and predicted values of \\(y\\), their differences, and their squared differences are shown in the table below.\n\n\nTable 2.2: Deviations and squared deviations of the line in Equation 2.1 and the data in Table 2.1.\n\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\((y - \\hat{y})\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n\n1\n2\n1.8\n0.2\n0.004\n\n\n2\n1.4\n1.6\n-0.2\n0.004\n\n\n2.75\n1.6\n1.45\n0.15\n0.0225\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.8\n0.2\n0.04\n\n\n7\n0.5\n0.6\n-0.1\n0.01\n\n\n8\n0.5\n0.4\n0.1\n0.01\n\n\n10\n0.4\n0\n0.4\n0.16\n\n\n\n\nNote that the sum of the errors (SE) is 0.8, and the sum of squares of the errors (SSE), which emphasizes larger deviations from the line, is 0.325.\nWe can try another line to see if we do better at predicting \\(y\\) (that is, have smaller SSE).\nLet’s try the line \\[\n    \\hat{y} =1.8-.15x\n\\tag{2.2}\\]\nThis line can be seen in Figure 2.3.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 1.8, slope = -0.15, color = \"red\")\n\n\n\n\nFigure 2.3: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.2\n\n\n\n\nThe fit results are shown in Table 2.3.\n\n\nTable 2.3: Deviations and squared deviations of the line in Equation 2.2 and the data in Table 2.1.\n\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\(y - \\hat{y}\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n\n1\n2\n1.65\n0.35\n0.1225\n\n\n2\n1.4\n1.5\n-0.1\n0.01\n\n\n2.75\n1.6\n1.3875\n0.2125\n0.04515625\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.9\n0.1\n0.01\n\n\n7\n0.5\n0.75\n-0.25\n0.0625\n\n\n8\n0.5\n0.6\n-0.1\n0.01\n\n\n10\n0.4\n0.3\n0.1\n0.01\n\n\n\n\nThe SSE for this line is 0.2727, which is lower than the SSE for the previous line, indicating a better fit.\nWhile we could try additional lines to achieve a lower SSE, there are infinitely many possibilities since \\(\\beta_0\\) and \\(\\beta_1\\) can take any real value.\nUsing Calculus, we can attempt to minimize the SSE for the generic line \\[\\begin{align*}\n    \\hat{y} = b_0 +b_1 x\n\\end{align*}\\]\nWe will denote the sum of the squared distances with \\(Q\\): \\[\nQ=\\sum \\left(y_i-\\hat{y}_i\\right)^2\n\\tag{2.3}\\]\nWe determine the “best” line as the one that minimizes \\(Q\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo minimize \\(Q\\), we differentiate it with respect to \\(b_{0}\\) and \\(b_{1}\\): \\[\\begin{align*}\n\\frac{\\partial Q}{\\partial b_{0}} & =-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{1}} & =-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\n\\end{align*}\\]\nSetting these partial derivatives equal to 0, we have \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\\\\\n-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\n\\end{align*}\\] Looking at the first equation, we can simplify as \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum y_{i}-\\sum b_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}-nb_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}=nb_{0}+b_{1}\\sum x_{i}\n\\end{align*}\\]\nSimplifying the second equation gives us \\[\\begin{align*}\n-2\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}-b_0\\sum x_{i}-b_1\\sum x_{i}^{2}=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align*}\\]\n\n\n\nThe two equations \\[\n\\begin{align}\n\\sum y_{i} & =nb_0+b_1\\sum x_{i}\\nonumber\\\\\n\\sum x_{i}y_{i} & =b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align}\n\\tag{2.4}\\]\nare called the normal equations.\nWe now have two equations and two unknowns (\\(b_0\\) and \\(b_1\\)). We can solve the equations simultaneously. We solve the first equation for \\(b_0\\) which gives us \\[\\begin{align*}\nb_0 & =\\frac{1}{n}\\left(\\sum y_{i}-b_1\\sum x_{i}\\right)\\\\\n& =\\bar{y}-b_1\\bar{x}.\n\\end{align*}\\]\nWe now substitute this into the second equation in Equation 2.4. Solving this for \\(b_1\\) gives us \\[\\begin{align*}\n& \\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n& \\quad\\Longrightarrow\\sum x_{i}y_{i}=\\left(\\bar{y}-b_1\\bar{x}\\right)\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n&\\quad\\Longrightarrow b_1=\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}.\n\\end{align*}\\]\nThe equations \\[\n\\begin{align}\nb_0 & =\\bar{y}-b_1\\bar{x}\\\\\nb_1 & =\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{2.5}\\] are called the least squares estimators.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo show these estimators are the minimum, we take the second partial derivatives of \\(Q\\): \\[\\begin{align*}\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{0}\\right)^{2}} & =2n\\\\\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{1}\\right)^{2}} & =2\\sum x_{i}^{2}\n\\end{align*}\\] Since these second partial derivatives are both positives, then we know the least squares estimators are the minimum.\n\n\n\nThe least squares estimators in Equation 2.5 can be expressed in simpler terms if we let \\[\\begin{align*}\nSS_{xx} &= \\sum \\left(x_i-\\bar x\\right)^2 \\\\\nSS_{xy} &= \\sum \\left(x_i-\\bar x\\right)\\left(y_i - \\bar y\\right)\n\\end{align*}\\]\nThe least squares estimates become \\[\\begin{align}\n{b_1=\\frac{SS_{xy}}{SS_{xx}}}\\\\\n{b_0=\\bar{y}-b_1\\bar{x}}\n\\end{align}\\]\nTo recap: The straight line model for the response \\(y\\) in terms of \\(x\\) is \\[\\begin{align*}\n{y = \\beta_0 + \\beta_1 x + \\varepsilon}\n\\end{align*}\\]\nThe line of means is \\[\\begin{align*}\n{E(y) = \\beta_0 + \\beta_1 x }\n\\end{align*}\\]\nThe fitted line (also called the least squares line) is \\[\\begin{align*}\n{\\hat{y} = b_0 + b_1 x }\n\\end{align*}\\]\nFor a given data point, \\((x_i, y_i)\\), the observed value of \\(y\\) is denoted as \\(y_i\\) and the predicted value of \\(y\\) is obtained by substituting \\(x_i\\) into the prediction equation: \\[\\begin{align*}\n{\\hat{y}_i = b_0 + b_1 x_i }\n\\end{align*}\\]\nThe deviation of the \\(i\\)th value of \\(y\\) from its predicted value, called the \\(i\\)th residual, is \\[\\begin{align*}\n{ \\left(y_i-\\hat{y}_i\\right) }\n\\end{align*}\\] Thus, SSE is just the sum of the squared residuals."
  },
  {
    "objectID": "03_Properties.html#properties-of-the-least-squares-estimators",
    "href": "03_Properties.html#properties-of-the-least-squares-estimators",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.1 Properties of the Least Squares Estimators",
    "text": "3.1 Properties of the Least Squares Estimators\n\n3.1.1 Linear Estimators\nNote that the least squares estimators are linear functions of the observations \\(y_{1},\\ldots,y_{n}\\). That is, both \\(b_0\\) and \\(b_1\\) can be written as a linear combination of the \\(y\\)’s.\nSince \\(y\\) is the variable that we want to model, we call an estimator for some parameter that takes the form of a linear combination of \\(y\\) a linear estimator.\n\n\n3.1.2 \\(b_1\\) as a Linear Estimator\nWe can express \\(b_1\\) as \\[\n\\begin{align}\nb_1 & =\\sum k_{i}y_{i}\n\\end{align}\n\\tag{3.1}\\] where \\[\\begin{align*}\nk_{i} & =\\frac{x_{i}-\\bar{x}}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align*}\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe first note that \\(\\sum\\left(x_{i}-\\bar{x}\\right)=0\\).\nWe now rewrite \\(b_1\\) as \\[\\begin{align*}\nb_1 & =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}-\\bar{y}\\sum\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\left(\\frac{1}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}\n\\end{align*}\\]\n\n\n\nFrom Equation 3.1, we see that \\(b_1\\) is a linear combination of the \\(y\\)’s since \\(k_{i}\\) are known constants (recall that \\(x_{i}\\) are treated as known constants).\n\n\n3.1.3 \\(b_0\\) as a Linear Estimator\nWe can write \\(b_0\\) as \\[\n\\begin{align}\nb_0 & =\\sum c_{i}y_{i}\n\\end{align}\n\\tag{3.2}\\] where \\[\\begin{align*}\nc_{i} & =\\frac{1}{n}-\\bar{x}k_{i}\n\\end{align*}\\]\nTherefore, \\(b_0\\) is a linear combination of \\(y_{i}\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe can rewrite \\(b_0\\) as \\[\\begin{align*}\nb_0 & =\\bar{y}-b_{1}\\bar{x}\\\\\n& =\\frac{1}{n}\\sum y_{i}-\\bar{x}\\sum k_{i}y_{i}\\\\\n& =\\sum\\left(\\frac{1}{n}-\\bar{x}k_{i}\\right)y_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "03_Properties.html#model-assumptions",
    "href": "03_Properties.html#model-assumptions",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.2 Model Assumptions",
    "text": "3.2 Model Assumptions\nLet’s now focus on the random component \\(\\varepsilon\\) in the probabilistic model and its connection to the errors in estimating \\(\\beta_0\\) and \\(\\beta_1\\).\nSpecifically, we’ll explore how the probability distribution of \\(\\varepsilon\\) influences the accuracy of the model in representing the true relationship between the dependent variable \\(y\\) and the independent variable \\(x\\).\nWe make four key assumptions about the probability distribution of \\(\\varepsilon\\):\n\nThe mean of \\(\\varepsilon\\)’s probability distribution is 0. This means that, on average, the errors cancel out over an infinitely large number of experiments for each value of the independent variable \\(x\\). Consequently, the mean value of \\(y\\), \\(E(y)\\), for a given \\(x\\) is \\(E(y) = \\beta_0 + \\beta_1 x\\).\nThe variance of \\(\\varepsilon\\)’s probability distribution is constant across all values of the independent variable \\(x\\). For our linear model, this implies that the variance of \\(\\varepsilon\\) is a constant, say, \\(\\sigma^2\\), regardless of the value of \\(x\\).\nThe probability distribution of \\(\\varepsilon\\) is normal.\nThe errors for different observations are independent. In other words, the error for one value of \\(y\\) does not influence the errors for other \\(y\\) values.\n\nThe implications of the first three assumptions can be seen in Figure 3.1;, which shows distributions of errors for four particular values of \\(x\\).\n\n\n\n\n\nFigure 3.1: The probability distribution of \\(\\varepsilon\\). At every value of \\(x\\), there is a distribution of \\(y\\) values that satisfy the four assumptions listed above. The red points are observed values.\n\n\n\n\nFrom Figure 3.1, we see that the probability distributions of the errors are normal, with a mean of 0 and a constant variance \\(\\sigma^2\\).\nThe line in the middle of the curve that goes to the regression line represents the mean value of \\(y\\) for a given value of \\(x\\). The line of means is given by the equation: \\[\nE(y) = \\beta_0 + \\beta_1 x\n\\]\nThese assumptions allow us to create measures of reliability for the least squares estimators and to develop hypothesis tests to evaluate the utility of the least squares line.\nVarious diagnostic techniques are available for checking the validity of these assumptions, and these diagnostics suggest remedies when the assumptions seem invalid.\nTherefore, it is crucial to apply these diagnostic tools in every regression analysis. We will discuss these techniques in detail later.\nIn practice, the assumptions do not need to hold exactly for least squares estimators and test statistics to have the reliability we expect from a regression analysis. The assumptions will be sufficiently satisfied for many real-world applications."
  },
  {
    "objectID": "03_Properties.html#an-estimator-of-sigma2",
    "href": "03_Properties.html#an-estimator-of-sigma2",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.3 An Estimator of \\(\\sigma^2\\)",
    "text": "3.3 An Estimator of \\(\\sigma^2\\)\nThe variability of random error, measured by its variance \\(\\sigma^2\\), plays a crucial role in the accuracy of estimating model parameters \\(\\beta_0\\) and \\(\\beta_1\\), as well as in the precision of predictions when using \\(\\hat{y}\\) to estimate \\(y\\) for a given value of \\(x\\). As a result, it is expected that \\(\\sigma^2\\) will appear in the formulas for confidence intervals and test statistics.\nIn most real-world scenarios, \\(\\sigma^2\\) is unknown and must be estimated using the available data. The best estimate for \\(\\sigma^2\\) is \\(s^2\\), calculated by dividing the sum of squares of residuals by the associated degrees of freedom (df). The sum of squares of residuals is given by: \\[\nSSE = \\sum \\left(y_i - \\hat{y}_i\\right)^2\n\\]\nIn a simple linear regression model, 2 degrees of freedom are used to estimate the y-intercept and slope, leaving \\((n - 2)\\) degrees of freedom for estimating the error variance. Thus, the estimate of \\(\\sigma^2\\) is: \\[\n\\begin{align*}\ns^2 &= \\frac{SSE}{n-2}\\\\\n&= \\frac{\\sum \\left(y_i - \\hat{y}_ i\\right)^2}{n-2}\n\\end{align*}\n\\]\nThis \\(s^2\\) serves as the basis for further statistical analysis, including the construction of confidence intervals and hypothesis testing.\nThe value of \\(s^2\\) is referred to as the mean square error (MSE).\nThe value \\[\\begin{align*}\ns &= \\sqrt{s^2}\n\\end{align*}\\] is referred to as the standard error of the regression model or as the root MSE (RMSE).\nUsing the empirical rule, we expect approximately 95% of the observed \\(y\\) values to lie within \\(2s\\) of their respective least squares predicted values, \\(\\hat y\\).\n\n\n\n\n\n\nReview: The Empirical Rule\n\n\n\nRecall the empirical rule applies to distributions that are mound-shaped and symmetric. It state that approximately 68% of the distribution is within one standard deviation of the mean, approximately 95% of the distribution is within two standard deviations of the mean, and approximatley 99.7% of the distribution is withing three standard deviations of the mean. Since we assume \\(\\varepsilon\\) is normally distributed, then the empirical rule holds.\n\n\n\nExample 3.2 (Example 3.1 - revisited) We can use the summary() function with the fit from lm to obtain the summary stats of the fit.\n\nfit = lm(mpg~wt, data = mtcars)\n\nfit |&gt; summary()\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe standard error (RMSE) of the fit is 3.046.\nWe can obtain the MSE with the following code:\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nwt         1 847.73  847.73  91.375 1.294e-10 ***\nResiduals 30 278.32    9.28                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe MSE is 9.28."
  },
  {
    "objectID": "04_Sampling.html#properties-of-the-constants-k_i-and-c_i",
    "href": "04_Sampling.html#properties-of-the-constants-k_i-and-c_i",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.1 Properties of the constants \\(k_i\\) and \\(c_i\\)",
    "text": "4.1 Properties of the constants \\(k_i\\) and \\(c_i\\)\nIn the previous section, we examined how the least squares estimators are linear combinations of the response variable \\(y\\). Let’s know look at the properties of the coefficients in Equation 3.1 and Equation 3.2. We will not present the proofs in this course but they are not complicated.\nThe coefficients \\(k_{i}\\) have the following properties: \\[\n\\begin{align}\n\\sum k_{i} & =0\n\\end{align}\n\\tag{4.1}\\]\n\\[\n\\begin{align}\n\\sum k_{i}x_i & =1\n\\end{align}\n\\tag{4.2}\\]\n\\[\n\\begin{align}\n\\sum k_{i}^{2} & =\\frac{1}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.3}\\]\nLikewise, the coefficients \\(c_{i}\\) have the following properties: \\[\n\\begin{align}\n\\sum c_{i} & =1\n\\end{align}\n\\tag{4.4}\\]\n\\[\n\\begin{align}\n\\sum c_{i}x_i & =0\n\\end{align}\n\\tag{4.5}\\]\n\\[\n\\begin{align}\n\\sum c_{i}^{2} & =\\frac{1}{n}+\\frac{\\left(\\bar{x}\\right)^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.6}\\]\n\n\n\n\n\n\nReview: The Expected Value of a Linear Combination\n\n\n\nRecall that the expected value of a linear combination of the random variable \\(Y\\) is \\[\nE(aY+b)=aE(Y)+b\n\\] where \\(a\\) and \\(b\\) are constants."
  },
  {
    "objectID": "04_Sampling.html#expected-values-of-b_0-and-b_1",
    "href": "04_Sampling.html#expected-values-of-b_0-and-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.2 Expected Values of \\(b_0\\) and \\(b_1\\)",
    "text": "4.2 Expected Values of \\(b_0\\) and \\(b_1\\)\nBefore finding the expectations, recall \\(E\\left(y_i\\right)=\\beta_{0}+\\beta_{1}x_i\\).\n\n4.2.1 Expected Value of \\(b_1\\)\nThe expected value of \\(b_1\\) is \\[\n\\begin{align*}\nE\\left[b_1\\right] & =E\\left[\\underbrace{\\sum k_{i}y_i}_{(3.1)}\\right]\\\\\n& =\\sum k_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\underbrace{\\sum k_{i}}_{(4.1)}+\\beta_{1}\\underbrace{\\sum k_{i}x_i}_{(4.2)}\\\\\n& =\\beta_{1}\n\\end{align*}\n\\]\n\n\n4.2.2 Expected Value of \\(b_0\\)\nThe expected value of \\(b_0\\) is \\[\n\\begin{align*}\nE\\left[b_0\\right] & =E\\left[\\underbrace{\\sum c_{i}y_i}_{(3.2)}\\right]\\\\\n& =\\sum c_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\underbrace{\\sum c_{i}}_{(4.4)}+\\beta_{1}\\underbrace{\\sum c_{i}x_i}_{(4.5)}\\\\\n& =\\beta_{0}\n\\end{align*}\n\\]\nTherefore, \\(b_0\\) is an unbiased estimator of \\(\\beta_0\\) and \\(b_1\\) is an unbiased estimator of \\(\\beta_1\\).\n\n\n\n\n\n\nReview: Unbiased Estimator\n\n\n\nRecall that an unbiased estimator for some parameter is an estimator that has an expected value equal to that parameter."
  },
  {
    "objectID": "04_Sampling.html#variances-of-b_0-and-b_1",
    "href": "04_Sampling.html#variances-of-b_0-and-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.3 Variances of \\(b_0\\) and \\(b_1\\)",
    "text": "4.3 Variances of \\(b_0\\) and \\(b_1\\)\nTo find the variances, we will use a result from mathematical statistics: Let \\(Y_{1},\\ldots,Y_{n}\\) be uncorrelated random variables and let \\(a_{1},\\ldots,a_{n}\\) be constants. Then \\[\n\\begin{align}\nVar\\left[\\sum a_{i}Y_i\\right] & =\\sum a_{i}^{2}Var\\left[Y_i\\right]\n\\end{align}\n\\tag{4.7}\\]\nRecall that we assume the response variables \\(y_i\\)’s are independent.\nTechnically, we assume the \\(y_i\\)’s are uncorrelated. In general, uncorrelated does not imply independent. However, if the random variables are jointly normally distributed (recall our third assumption of the model), then uncorrelated does imply independent.\nAlso, note that \\[\n\\begin{align*}\n    Var\\left[Y\\right]& = Var\\left[\\beta_0 + \\beta_1 x + \\varepsilon\\right]\\\\\n    & = Var\\left[\\varepsilon\\right]\\\\\n    & = \\sigma^2\n\\end{align*}\n\\]\n\n4.3.1 Variance of \\(b_1\\)\nThe variance of \\(b_1\\) is \\[\n\\begin{align}\nVar\\left[b_1\\right] & =Var\\left[\\underbrace{\\sum k_{i}y_i}_{(3.1)}\\right]\\\\\n& =\\underbrace{\\sum k_{i}^{2}}_{(4.3)}Var\\left[y_i\\right]\\\\\n& =\\frac{\\sigma^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.8}\\]\n\n\n4.3.2 Variance of \\(b_0\\)\nThe variance of \\(b_0\\) is \\[\n\\begin{align}\nVar\\left[b_0\\right] & =Var\\left[\\underbrace{\\sum c_{i}y_i}_{(3.2)}\\right]\\nonumber\\\\\n& =\\underbrace{\\sum c_{i}^{2}}_{(4.6)}Var\\left[y_i\\right]\\\\\n& =\\sigma^{2}\\left[\\frac{1}{n}+\\frac{\\left(\\bar{x}\\right)^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\\right]\n\\end{align}\n\\tag{4.9}\\]"
  },
  {
    "objectID": "04_Sampling.html#best-linear-unbiased-estimators-blues",
    "href": "04_Sampling.html#best-linear-unbiased-estimators-blues",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.4 Best Linear Unbiased Estimators (BLUEs)",
    "text": "4.4 Best Linear Unbiased Estimators (BLUEs)\nWe see from Equation 3.1 and Equation 3.2 that \\(b_0\\) and \\(b_1\\) are linear estimators.\nAny estimator for \\(\\beta_{1}\\), which we will denote as \\(\\hat{\\beta}_{0}\\), that takes the form \\[\n\\begin{align*}\n\\hat{\\beta}_{1} & =\\sum a_{i}y_i\n\\end{align*}\n\\] where \\(a_{i}\\) is some constant, is called a linear estimator.\nOf all linear estimators for \\(\\beta_0\\) and \\(\\beta_1\\) that are unbiased, the least squares estimators, \\(b_0\\) and \\(b_1\\), have the smallest variance.\nThis is summarized in the following well known theorem:\n\nTheorem 4.1 (Gauss Markov Theorem) For the simple linear regression model, the least squares estimators \\(b_0\\) and \\(b_1\\) are unbiased and have minimum variance among all unbiased linear estimators.\n\nAn estimator that is linear, unbiased, and has the smallest variance of all unbiased linear estimators is called the best linear unbiased estimator (BLUE).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nProof of the Gauss Markov Theorem:\nFor all linear estimators that are unbiased, we must have \\[\n\\begin{align*}\nE\\left[\\hat{\\beta}_{1}\\right] & =E\\left[\\sum a_{i}y_i\\right]\\\\\n& =\\sum a_{i}E\\left[y_i\\right]\\\\\n& =\\beta_{1}\n\\end{align*}\n\\] Since \\(E\\left[y_i\\right]=\\beta_{0}+\\beta_{1}x_i\\), then we must have \\[\n\\begin{align*}\nE\\left[\\hat{\\beta}_{1}\\right] & =\\sum a_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\sum a_{i}+\\beta_{1}\\sum a_{i}x_i\\\\\n& =\\beta_{1}\n\\end{align*}\n\\] Therefore, \\[\n\\begin{align*}\n\\sum a_{i} & =0\\\\\n\\sum a_{i}x_i & =1\n\\end{align*}\n\\] We now examine the variance of \\(\\hat{\\beta}_{1}\\): \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =\\sum a_{i}^{2}Var\\left[y_i\\right]\\\\\n& =\\sigma^{2}\\sum a_{i}^{2}\n\\end{align*}\n\\] Let’s now define \\(a_{i}=k_{i}+d_{i}\\) where \\(k_{i}\\) is defined in Equation 3.1. and \\(d_{i}\\) is some arbitrary constant.\nWe will show that adding a constant (whether negative or positive) to \\(k_i\\) cannot make the variance smaller. Thus, the smallest variance of the linear estimator \\(\\hat{\\beta}_1\\) is when \\(a_i=k_i\\).\nThe variance of \\(\\hat{\\beta}_{1}\\) can now be written as \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =\\sigma^{2}\\sum a_{i}^{2}\\\\\n& =\\sigma^{2}\\sum\\left(k_{i}+d_{i}\\right)^{2}\\\\\n& =\\sigma^{2}\\sum\\left(k_{i}^{2}+2k_{i}d_{i}+d_{i}^{2}\\right)\\\\\n& =Var\\left[b_1\\right]+2\\sigma^{2}\\sum k_{i}d_{i}+\\sigma^{2}\\sum d_{i}^{2}\n\\end{align*}\n\\] Examining the second term and using the expression of \\(k_{i}\\) in Equation 3.1, we see that \\[\n\\begin{align*}\n\\sum k_{i}d_{i} & =\\sum k_{i}\\left(a_{i}-k_{i}\\right)\\\\\n& =\\sum a_{i}k_{i}-\\underbrace{\\sum k_{i}^{2}}_{(4.3)}\\\\\n& =\\sum a_{i}\\frac{x_i-\\bar{x}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum a_{i}x_i-\\bar{x}\\sum a_{i}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{1-\\bar{x}\\left(0\\right)}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =0\n\\end{align*}\n\\]\nWe now have the variance of \\(\\hat{\\beta}_{1}\\) as \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =Var\\left[b_1\\right]+\\sigma^{2}\\sum d_{i}^{2}\n\\end{align*}\n\\] This variance is minimized when \\(\\sum d_{i}^{2}=0\\) which only happens when \\(d_{i}=0\\).\nThus, the unbiased linear estimator with the smallest variance is when \\(a_{i}=k_{i}\\). That is, the least squares estimator \\(b_1\\) in Equation 3.1 has the smallest variance of all unbiased linear estimators of \\(\\beta_{1}\\).\nA similar argument can be used to show that \\(b_0\\) has the smallest variance of all unbiased linear estimators of \\(\\beta_{0}\\)."
  },
  {
    "objectID": "04_Sampling.html#sampling-distribution-for-b_1",
    "href": "04_Sampling.html#sampling-distribution-for-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.5 Sampling Distribution for \\(b_1\\)",
    "text": "4.5 Sampling Distribution for \\(b_1\\)\nNow that we see that the least squares estimator \\(b_1\\) is the BLUE for \\(\\beta_{1}\\), we will now examine the sampling distribution for \\(b_1\\).\nWe previously discussed that the mean of the sampling distribution of \\(b_1\\) is \\[\nE[b_1]=\\beta_1\n\\] with a variance of \\[\n\\begin{align}\nVar\\left[b_1\\right]\n& =\\frac{\\sigma^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.10}\\]\nNote that in our model with our four assumptions, \\(y\\) is normally distributed. That is, \\[\n\\begin{align}\n    y\\sim N\\left(\\beta_0+\\beta_1 x, \\sigma^2\\right)\n\\end{align}\n\\tag{4.11}\\]\nTo learn about the sampling distributions of the least squares estimators, we will use the following theorems from mathematical statistics:\n\nTheorem 4.2 (Sum of Independent Normal Random Variables) If \\[\nY_i\\sim N\\left(\\mu_i,\\sigma_i^2\\right)\n\\] are independent, then the linear combination \\(\\sum_i a_iY_i\\) is also normally distributed where \\(a_i\\) are constants. In particular \\[\n\\sum_i a_iY_i \\sim N\\left(\\sum_i a_i\\mu_i, \\sum_i a_i^2\\sigma_i^2\\right)\n\\]\n\n\nTheorem 4.3 (Adding a Constant to a Normal Random Variable) If \\[\nY\\sim N\\left(\\mu,\\sigma^2\\right)\n\\] then for any real constant \\(c\\), \\[\nY+c\\sim N\\left(\\mu+c,\\sigma^2\\right)\n\\]\n\nSince \\(Y\\) is normally distributed by Equation 4.11, then we can apply Theorem 4.2 which implies that \\(b_1\\) is normally distributed. That is, \\[\n\\begin{align}\nb_1 & \\sim N\\left(\\beta_{1},\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\right)\n\\end{align}\n\\tag{4.12}\\]\n\n4.5.1 Standardized Score\nSince \\(b_1\\) is normally distributed, we can standardize it so that the resulting statistic will have a standard normal distribution.\nTherefore, we have \\[\n\\begin{align}\nz=\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}}} & \\sim N\\left(0,1\\right)\n\\end{align}\n\\tag{4.13}\\]\n\n\n4.5.2 Studentized Score\nIn practice, the standardized score \\(z\\) is not useful since we do not know the value of \\(\\sigma^{2}\\). We can estimate \\(\\sigma^{2}\\) with the statistic \\[\ns^2 = \\frac{SSE}{n-2}\n\\]\nUsing this estimate for \\(\\sigma^2\\) leads us to a \\(t\\)-score: \\[\n\\begin{align}\nt=\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}}} & \\sim t\\left(n-2\\right)\n\\end{align}\n\\tag{4.14}\\]\nWe call this \\(t\\) statistic, the studentized score.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nIt is important to note the following theorem from math stats presented here without proof:\n\nTheorem 4.4 (Distribution of the sample variance of the residuals) For the sample variance of the residuals \\(s^{2}\\), the quantity \\[\\begin{align*}\n\\frac{\\left(n-2\\right)s^{2}}{\\sigma^{2}} & =\\frac{SSE}{\\sigma^{2}}\n\\end{align*}\\] is distributed as a chi-square distribution with \\(n-2\\) degrees of freedom. That is, \\[\\begin{align*}\n\\frac{SSE}{\\sigma^{2}} & \\sim\\chi^{2}\\left(n-2\\right)\n\\end{align*}\\]\n\nWe will use another important theorem form math stats (again presented without proof):\n\nTheorem 4.5 (Ratio of independent standard normal and chi-square statistics) If \\(Z\\sim N\\left(0,1\\right)\\) and \\(W\\sim\\chi^{2}\\left(\\nu\\right)\\), and \\(Z\\) and \\(W\\) are independent, then the statistic \\[\\begin{align*}\n\\frac{Z}{\\sqrt{\\frac{W}{\\nu}}}\n\\end{align*}\\] is distributed as a Student’s \\(t\\) distribution with \\(\\nu\\) degrees of freedom.\n\nWe will take the standardized score in Equation 4.13 and divide by \\[\\begin{align*}\n\\sqrt{\\frac{\\frac{\\left(n-2\\right)s^{2}}{\\sigma^{2}}}{\\left(n-2\\right)}} & =\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}\n\\end{align*}\\] to give us \\[\\begin{align*}\nt & =\\frac{\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}}{\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}}\\\\\n& =\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}}\\\\\n& =\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}\n\\end{align*}\\] which will have a Student’s \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "04_Sampling.html#sec-modelutility",
    "href": "04_Sampling.html#sec-modelutility",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.6 Assessing the Utility of the Model: Making Inferences About the Slope",
    "text": "4.6 Assessing the Utility of the Model: Making Inferences About the Slope\nSuppose that the independent variable \\(x\\) is completely unrelated to the dependent variable \\(y\\).\nWhat could be said about the values of \\(\\beta_0\\) and \\(\\beta_1\\) in the hypothesized probabilistic model \\[\\begin{align*}\n    y = \\beta_0 +\\beta_1 x + \\varepsilon\n\\end{align*}\\] if \\(x\\) contributes no information for the prediction of \\(y\\)?\nThe implication is that the mean of \\(y\\), does not change as \\(x\\) changes. In other words, the line would just be a horizontal line.\nIf \\(E(y)\\) does not change as \\(x\\) increases, then using \\(x\\) to predict \\(y\\) in the linear model is not useful.\nRegardless of the value of \\(x\\), you always predict the same value of \\(y\\). In the straight-line model, this means that the true slope, \\(\\beta_1\\), is equal to 0.\nTherefore, to test the null hypothesis that \\(x\\) contributes no information for the prediction of \\(y\\) against the alternative hypothesis that these variables are linearly related with a slope differing from 0, we test \\[\\begin{align*}\n    H_0:\\beta_1 = 0\\\\\n    H_a:\\beta_1\\ne 0\n\\end{align*}\\]\nIf the data support the alternative hypothesis, we conclude that \\(x\\) does contribute information for the prediction of \\(y\\) using the straight-line model (although the true relationship between \\(E(y)\\) and \\(x\\) could be more complex than a straight line). Thus, to some extent, this is a test of the utility of the hypothesized model.\nThe appropriate test statistic is the studentized score given above: \\[\n\\begin{align}\n    t &= \\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}\\\\\n    &=\\frac{b_1}{\\sqrt{\\frac{s^{2}}{SS_{xx}}}}\n\\end{align}\n\\tag{4.15}\\]\nAnother way to make inferences about the slope \\(\\beta_1\\) is to estimate it using a confidence interval \\[\n\\begin{align}\n    b_1 \\pm \\left(t_{\\alpha/2}\\right)s_{b_1}\n\\end{align}\n\\tag{4.16}\\] where \\[\\begin{align*}\n    s_{b_1} = \\frac{s}{\\sqrt{SS_{xx}}}\n\\end{align*}\\]\nWe can obtain the p-value for the hypothesis test by using the summary function with an lm object. For the previous example consisting of the mtcars data.\n\nExample 4.1 (Example 3.1 - revisited)  \n\nlibrary(tidyverse)\n\nfit = lm(mpg~wt, data = mtcars)\n\nWe find the least squares estimates as\n\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nFrom the output, we see the p-value is \\(1.29\\times 10^{-10}\\). So we have sufficient evidence to conclude that the true population slope is different than zero.\nTo find the confidence interval, we can use the confint function with the lm object.\n\nconfint(fit, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept) 33.450500 41.119753\nwt          -6.486308 -4.202635\n\n\nWe 95% confident that the true population slope is in the interval \\((-6.486, -4.203)\\)"
  },
  {
    "objectID": "05_Correlation.html#the-coefficient-of-correlation",
    "href": "05_Correlation.html#the-coefficient-of-correlation",
    "title": "5  Correlation Coefficient and the Coefficient of Determination",
    "section": "5.1 The Coefficient of Correlation",
    "text": "5.1 The Coefficient of Correlation\nThe claim is often made that the crime rate and the unemployment rate are “highly correlated.”\nAnother popular belief is that IQ and academic performance are “correlated.” Some people even believe that the Dow Jones Industrial Average and the lengths of fashionable skirts are “correlated.”\nThus, the term correlation implies a relationship or association between two variables.\nFor the data \\((x_i,y_i)\\), \\(i=1,\\ldots,n\\), we want a measure of how well a linear model explains a linear relationship between \\(x\\) and \\(y\\).\nRecall the quantities \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\).\n\n\n\n\n\n\nReview: Different \\(SS\\) quantities\n\n\n\nRecall that \\[\n\\begin{align*}\nSS_{xx} &= \\sum\\left(x_i-\\bar{x}\\right)^2\\\\\nSS_{yy} &= \\sum\\left(y_i-\\bar{y}\\right)^2\\\\\nSS_{xy} &= \\sum\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)\n\\end{align*}\n\\]\n\n\n\\(SS_{xx}\\) and \\(SS_{yy}\\) are measures of variability of \\(x\\) and \\(y\\), respectively. That is, they indicate how \\(x\\) and \\(y\\) varies about their mean, individually.\n\\(SS_{xy}\\) is a measure of how \\(x\\) and \\(y\\) vary together.\n\nExample 5.1 (Data from Table 2.1) For example, consider the data from Table 2.1. Let’s find \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\) in R.\n\nlibrary(tidyverse)\n\nx = c(1, 2 ,2.75, 4, 6, 7, 8, 10)\ny = c(2, 1.4, 1.6, 1.25, 1, 0.5, 0.5, 0.4)\n\ndat =  tibble(x, y)\n\nybar =  mean(y)\nxbar =  mean(x)\n\nggplot(data=dat, aes(x = x, y = y)) +\n  geom_point() +\n  xlim(0,10) +\n  ylim(0,2) +\n  geom_hline(yintercept = ybar,col=\"red\") +\n  geom_vline(xintercept = xbar, col=\"red\")\n\n\n\n\n\ndev_x = x-xbar\ndev_y = y-ybar\n\ndev_xy = dev_x*dev_y\n\ndat1 = tibble(x,y,dev_x^2,dev_y^2,dev_xy)\ndat1\n\n# A tibble: 8 × 5\n      x     y `dev_x^2` `dev_y^2`  dev_xy\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1  1     2       16.8     0.844   -3.76  \n2  2     1.4      9.57    0.102   -0.986 \n3  2.75  1.6      5.49    0.269   -1.22  \n4  4     1.25     1.20    0.0285  -0.185 \n5  6     1        0.821   0.00660 -0.0736\n6  7     0.5      3.63    0.338   -1.11  \n7  8     0.5      8.45    0.338   -1.69  \n8 10     0.4     24.1     0.464   -3.34  \n\n\n\nIn the output of dat1, dev_x^2 represents \\((x_i-\\bar{x})^2\\) and dev_y^2 represents \\((y_i-\\bar{y})^2\\) for each observation. dev_xy represents \\((x_i-\\bar{x})(y_i-\\bar{y})\\) for each observation. Note that each value is negative. This is because as \\(x\\) is below \\(\\bar{x}\\), \\(y\\) is above \\(\\bar{y}\\).\nLikewise, as \\(X\\) is above \\(\\bar{x}\\), \\(Y\\) is below \\(\\bar{y}\\). In the ggplot above, the two red lines represent \\(\\bar{x}\\) (the vertical red line) and \\(\\bar{y}\\) (the horizontal red line). You can see how the observations are below or above these lines.\nWe can find the values of \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\) by\n\n#SS_XX\ndev_x^2 |&gt; sum()\n\n[1] 69.99219\n\n#SS_YY\ndev_y^2 |&gt; sum()\n\n[1] 2.389687\n\n#SS_XY\ndev_xy |&gt; sum()\n\n[1] -12.36094\n\n\n\nExample 5.2 (The trees dataset) For another example, consider the trees dataset.\nIn R, a packaged called datasets include a number of available datasets. One of the datasets is called trees.\n\nlibrary(datasets)\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\nThere are 31 total observations in this dataset. Variables measured are the Girth (actually the diameter measured at 54 in. off the ground), the Height, and the Volume of timber from each black cherry tree.\nSuppose we want to predict Volume from Girth.\nAgain, we plot the data with red lines representing \\(\\bar{x}\\) and \\(\\bar{y}\\).\n\nlibrary(datasets)\nlibrary(tidyverse)\n\nxbar = mean(trees$Girth)\nybar = mean(trees$Volume)\n\nggplot(data=trees, aes(x=Girth, y=Volume)) +\n  geom_point() +\n  geom_hline(yintercept = ybar,col=\"red\") +\n  geom_vline(xintercept = xbar, col=\"red\")\n\n\n\nx = trees$Girth\ny = trees$Volume\n\ndev_x = x-xbar\ndev_y = y-ybar\n\ndev_xy = dev_x*dev_y\n\n\ndat1 = tibble(x,y,dev_x^2,dev_y^2,dev_xy)\ndat1 |&gt; print(n=31)\n\n# A tibble: 31 × 5\n       x     y `dev_x^2` `dev_y^2`  dev_xy\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1   8.3  10.3  24.5        395.    98.3  \n 2   8.6  10.3  21.6        395.    92.4  \n 3   8.8  10.2  19.8        399.    88.8  \n 4  10.5  16.4   7.55       190.    37.8  \n 5  10.7  18.8   6.49       129.    29.0  \n 6  10.8  19.7   5.99       110.    25.6  \n 7  11    15.6   5.06       212.    32.8  \n 8  11    18.2   5.06       143.    26.9  \n 9  11.1  22.6   4.62        57.3   16.3  \n10  11.2  19.9   4.20       105.    21.0  \n11  11.3  24.2   3.80        35.7   11.6  \n12  11.4  21     3.42        84.1   17.0  \n13  11.4  21.4   3.42        76.9   16.2  \n14  11.7  21.3   2.40        78.7   13.7  \n15  12    19.1   1.56       123.    13.8  \n16  12.9  22.2   0.121       63.5    2.78 \n17  12.9  33.8   0.121       13.2   -1.26 \n18  13.3  27.4   0.00266      7.68  -0.143\n19  13.7  25.7   0.204       20.0   -2.02 \n20  13.8  24.9   0.304       27.8   -2.91 \n21  14    34.5   0.565       18.7    3.25 \n22  14.2  31.7   0.906        2.34   1.46 \n23  14.5  36.3   1.57        37.6    7.67 \n24  16    38.3   7.57        66.1   22.4  \n25  16.3  42.6   9.31       154.    37.9  \n26  17.3  55.4  16.4        637.   102.   \n27  17.5  55.7  18.1        652.   109.   \n28  17.9  58.3  21.6        791.   131.   \n29  18    51.5  22.6        455.   101.   \n30  18    51    22.6        434.    99.0  \n31  20.6  77    54.0       2193.   344.   \n\n#SS_XX\ndev_x^2 |&gt; sum()\n\n[1] 295.4374\n\n#SS_YY\ndev_y^2 |&gt; sum()\n\n[1] 8106.084\n\n#SS_XY\ndev_xy |&gt; sum()\n\n[1] 1496.644\n\n\nIn this example, most of the observations have \\((x-\\bar{x})(y-\\bar{y})\\) that are positive. This is because these observations have values of \\(x\\) that are below \\(\\bar{x}\\) and values of \\(y\\) that are below \\(\\bar{y}\\), or values of \\(x\\) that are above \\(\\bar{x}\\) and values of \\(y\\) that are above \\(\\bar{y}\\).\nThere are four observations that have a negative value of \\((x-\\bar{x})(y-\\bar{y})\\). Although they are negative, the value of \\(SS_{xy}\\) is positive due to all the observations with positive values of \\((x-\\bar{x})(y-\\bar{y})\\). Therefore, we say if \\(SS_{xy}\\) is positive, then \\(y\\) tends to increase as \\(x\\) increases. Likewise, if \\(SS_{xy}\\) is negative, then \\(y\\) tends to decrease as \\(x\\) increases.\nIf \\(SS_{xy}\\) is zero (or close to zero), then we say \\(y\\) does not tend to change as \\(x\\) increases.\n\n\n5.1.1 Defining the Correlation Coefficient\nWe first note that \\(SS_{xy}\\) cannot be greater in absolute value than the quantity \\[\n\\sqrt{SS_{xx}SS_{yy}}\n\\] We will not prove this here, but it is a direct application of the Cauchy-Schwarz inequality .\nWe define the linear correlation coefficient as \\[\n\\begin{align}\n    r=\\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}\n\\end{align}\n\\tag{5.1}\\]\n\\(r\\) is also called the Pearson correlation coefficient.\nWe note that \\[\n-1\\le r \\le 1\n\\]\nIf \\(r=0\\), then there is no linear relationship between \\(x\\) and \\(y\\).\nIf \\(r\\) is positive, then the slope of the linear relationship is positive. If \\(r\\) is negative, then the slope of the linear relationship is negative.\nThe closer \\(r\\) is to one in absolute value, the stronger the linear relationship is between \\(x\\) and \\(y\\).\n\n\n5.1.2 Some Examples of \\(r\\)\nThe best way to grasp correlation is to see examples. In Figure 5.1, scatterplots of 200 observations are shown with a least squares line.\n\n\n\n\n\n\n\n(a) \\(r=-0.079\\)\n\n\n\n\n\n\n\n(b) \\(r=-0.672\\)\n\n\n\n\n\n\n\n\n\n(c) \\(r=0.723\\)\n\n\n\n\n\n\n\n(d) \\(r=0.524\\)\n\n\n\n\nFigure 5.1: Examples of correlation\n\n\nNote how the value of \\(r\\) relates to how spread out the points are from the line as well as to the slope of the line.\nThe correlation coefficient, \\(r\\), quantifies the strength of the linear relationship between two variables, \\(x\\) and \\(y\\), similar to the way the least squares slope, \\(b_1\\), does. However, unlike the slope, the correlation coefficient is scaleless. This means that the value of \\(r\\) always falls between \\(\\pm 1\\), regardless of the units used for \\(x\\) and \\(y\\).\nThe calculation of \\(r\\) uses the same data that is used to fit the least squares line. Given that both \\(r\\) and \\(b_1\\) offer insight into the utility of the model, it’s not surprising that their computational formulas are related.\nIt’s also important to remember that a high correlation does not imply causality. If a high positive or negative value of \\(r\\) is observed, this does not mean that changes in \\(x\\) cause changes in \\(y\\). The only valid conclusion is that there may be a linear relationship between \\(x\\) and \\(y\\).\n\n\n5.1.3 The Population Correlation Coefficient\nThe correlation \\(r\\) is for the observed data which is usually from a sample. Thus, \\(r\\) is the sample correlation coefficient.\nWe could make a hypothesis about the correlation of the population based on the sample. We will denote the population correlation with \\(\\rho\\). The hypothesis we will want to test is \\[\\begin{align*}\n  H_0:\\rho = 0\\\\\nH_a:\\rho \\ne 0\n\\end{align*}\\]\nRecall the hypothesis test for the slope in Section 4.6.\nIf we test \\[\\begin{align*}\nH_{0}: & \\beta_{1}=0\\\\\nH_{a}: & \\beta_{1}\\ne0\n\\end{align*}\\] then this is equivalent to testing1 \\[\\begin{align*}\nH_{0}: & \\rho=0\\\\\nH_{a}: & \\rho\\ne0\n\\end{align*}\\] since both hypotheses test to see of there is a linear relationship between \\(x\\) and \\(y\\).\nNow note, using Equation 2.5, that \\(b_1\\) can be rewritten as \\[\n\\begin{align}\nb_1 & =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{SS_{xy}}{SS_{xx}}\\\\\n& =\\frac{rSS_{xy}}{rSS_{xx}}\\\\\n& =\\frac{rSS_{xy}}{\\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}SS_{xx}}\\\\\n& =\\frac{r\\sqrt{SS_{xx}SS_{yy}}}{SS_{xx}}\\\\\n& =r\\frac{\\sqrt{\\frac{SS_{xx}}{n-1}\\frac{SS_{yy}}{n-1}}}{\\frac{SS_{xx}}{n-1}}\\\\\n& =r\\frac{s_{X}s_{Y}}{s_{X}^{2}}\\\\\n& =r\\frac{s_{y}}{s_{X}}\n\\end{align}\n\\tag{5.2}\\] where \\(s_{y}\\) and \\(s_{x}\\) are the sample standard deviation of \\(y\\) and \\(x\\), respectively.\nThe test statistic is \\[\n\\begin{align}\nt & =\\frac{r\\sqrt{\\left(n-2\\right)}}{\\sqrt{1-r^{2}}}\n\\end{align}\n\\tag{5.3}\\]\nIf \\(H_0\\) is true, then \\(t\\) will have a Student’s \\(t\\)-distribution with \\(n-2\\) degrees of freedom.\nThe only real difference between the least squares slope \\(b_1\\) and the coefficient of correlation \\(r\\) is the measurement scale2.\nTherefore, the information they provide about the utility of the least squares model is to some extent redundant.\nFurthermore, the slope \\(b_1\\) gives us additional information on the amount of increase (or decrease) in \\(y\\) for every 1-unit increase in \\(x\\).\nFor this reason, the slope is recommended for making inferences about the existence of a positive or negative linear relationship between two variables."
  },
  {
    "objectID": "05_Correlation.html#the-coefficient-of-determination",
    "href": "05_Correlation.html#the-coefficient-of-determination",
    "title": "5  Correlation Coefficient and the Coefficient of Determination",
    "section": "5.2 The Coefficient of Determination",
    "text": "5.2 The Coefficient of Determination\nThe second measure of how well the model fits the data involves measuring the amount of variability in \\(y\\) that is explained by the model using \\(x\\).\nWe start by examining the variability of the variable we want to learn about. We want to learn about the response variable \\(y\\). One way to measure the variability of \\(y\\) is with \\[\nSS_{yy} = \\sum\\left(y_i-\\bar{y}\\right)^2\n\\]\nNote that \\(SS_{yy}\\) does not include the model or \\(x\\). It is just a measure of how \\(y\\) deviates from its mean \\(\\bar{y}\\).\nWe also have the variability of the points about the line. We can measure this with the sum of squares error \\[\nSSE = \\sum \\left(y_i - \\hat{y}_i\\right)^2\n\\]\nNote that SSE does include \\(x\\). This is because the fitted line \\(\\hat{y}\\) is a function of \\(x\\).\nHere are a couple of key points regarding sums of squares:\n\nIf \\(x\\) provides little to no useful information for predicting \\(y\\), then \\(SS_{yy}\\) and \\(SSE\\) will be nearly equal.\nIf \\(x\\) does provide valuable information for predicting \\(y\\), then \\(SSE\\) will be smaller than \\(SS_{yy}\\).\nIn the extreme case where all points lie exactly on the least squares line, \\(SSE = 0\\).\n\nHere’s an example to illustrate:\nSuppose we have data for two variables, hours studied (x) and test scores (y). If studying time doesn’t help predict the test score, the variation in test scores (measured by \\(SS_{yy}\\)) will be similar to the error in the prediction (measured by \\(SSE\\)). However, if studying time is a good predictor, the prediction errors will be much smaller, making \\(SSE\\) significantly smaller than \\(SS_{yy}\\). If the relationship between study time and test scores is perfect, then the error would be zero, resulting in \\(SSE = 0\\).\n\n5.2.1 Proportion of Variation Explained\nWe want to explain as much of the variation of \\(y\\) as possible. So we want to know just how much of that variation is explained by using linear regression model with \\(x\\). We can quantify this variation explained by taking the difference \\[\n\\begin{align}\n    SSR = SS_{yy}-SSE\n\\end{align}\n\\tag{5.4}\\]\nSSR is called the sum of squares regression.\nWe calculate the proportion of the variation of \\(y\\) explained by the regression model using \\(x\\) by calculating3 \\[\n\\begin{align}\n    r^2 = \\frac{SSR}{SS_{yy}}\n\\end{align}\n\\tag{5.5}\\]\n\\(r^2\\) is called the coefficient of determination4\nPractical Interpretation:\nAbout \\(100(r^2)\\%\\) of the sample variation in \\(y\\) (measured by the total sum of squares of deviations of the sample \\(y\\)-values about their mean \\(\\bar{y}\\)) can be explained by (or attributed to) using \\(x\\) to predict \\(y\\) in the straight-line model.\n\nExample 5.3 (Example 5.2 revisited) We can find the coefficient of determination using the summary function with an lm object.\n\nlibrary(datasets)\n\nfit = lm(Volume~Girth, data = trees)\n\nfit |&gt; summary()\n\n\nCall:\nlm(formula = Volume ~ Girth, data = trees)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that 93.53% of the variability in the volume of the trees can be explained by the linear model using girth to predict the volume.\nIf we want to find the correlation coefficient, we can just use the cor function on the dataframe. This will find the correlation coefficient for each pair of variables in the dataframe. Note that there can only be quantitative variables in the dataframe in order this function to work.\n\ntrees |&gt; cor()\n\n           Girth    Height    Volume\nGirth  1.0000000 0.5192801 0.9671194\nHeight 0.5192801 1.0000000 0.5982497\nVolume 0.9671194 0.5982497 1.0000000\n\n\nSo the correlation between Girth and Volume is 0.9671."
  },
  {
    "objectID": "05_Correlation.html#footnotes",
    "href": "05_Correlation.html#footnotes",
    "title": "5  Correlation Coefficient and the Coefficient of Determination",
    "section": "",
    "text": "Note: The two tests are equivalent in simple linear regression only.↩︎\nThe estimated slope is measured in the same units as \\(y\\). However, the correlation coefficient \\(r\\) is independent of scale.↩︎\nIn simple linear regression, it can be shown that this quantity is equal to the square of the simple linear coefficient of correlation \\(r\\).↩︎\nNote that some software will denote the coefficient of determination as \\(R^2\\).↩︎"
  },
  {
    "objectID": "06_Using.html#using-the-model-for-estimation-and-prediction",
    "href": "06_Using.html#using-the-model-for-estimation-and-prediction",
    "title": "6  Using the Simple Linear Model",
    "section": "6.1 Using the Model for Estimation and Prediction",
    "text": "6.1 Using the Model for Estimation and Prediction\nNow that we have fit the model\n\\[\ny = {\\beta}_0 + {\\beta}_1 x + \\varepsilon\n\\] and assessed how good of fit the model is, we can now use the model estimation and prediction.\nRecall that the mean of \\(y_{i}\\) for some value of \\(x_{i}\\) is the population line \\(\\beta_{0}+\\beta_{1}x_{i}\\) evaluated at \\(x_{i}\\).\nSo we can estimate the mean of \\(y_{i}\\) for some value of \\(x_{i}\\) by evaluating the model estimated with the least squares estimators: \\[\\begin{align*}\n\\hat{y}_{i} & =b_0+b_1x_{i}\n\\end{align*}\\]\nWe say \\(\\hat{y}_{i}\\) is a point estimator for the population mean \\(\\beta_{0}+\\beta_{1}x_{i}\\).\n\n6.1.1 The Sampling Distribution of \\(\\hat{y}\\)\nWe will want to make an inference for the population mean response at some value of the predictor variable \\(x_{i}\\).\nWe have a point estimator \\(\\hat{y}_{i}\\). We will now examine the sampling distribution of \\(\\hat{y}_{i}\\) and use it to make a confidence interval for the mean response \\(\\beta_{0}+\\beta_{1}x_{i}\\).\n\n\n6.1.2 Linear Combination of \\(y\\)\nWe will denote the value of \\(x\\) at which we want to estimate the mean response as \\(x_{h}\\). So the value of \\(y\\) at \\(x_{h}\\) will be \\(y_{h}\\)\nWe write \\(\\hat{y}_{h}\\) as \\[\\begin{align*}\n\\hat{y}_{h} & =\\underbrace{b_0}_{(3.2)}+\\underbrace{b_1}_{(3.1)}x_{h}\\\\\n& =\\sum c_{i}y_{i}+\\sum k_{i}y_{h}x_{h}\\\\\n& =\\sum\\left(c_{i}+k_{i}x_{h}\\right)y_{h}\n\\end{align*}\\]\nThus, \\(\\hat{y}_{j}\\) is a linear combination of the observed \\(y_{i}\\) which are normally distributed. Then by Theorem 4.2, \\(\\hat{y}_{j}\\) is normally distributed.\n\n\n6.1.3 The Mean of \\(\\hat{y}_h\\)\nUsing Theorem 4.2, we have the mean as \\[\\begin{align*}\n\\sum\\left(c_{i}+k_{i}x_{h}\\right)E\\left[y_{h}\\right] & =\\left(\\underbrace{\\sum c_{i}}_{=1}+x_{h}\\underbrace{\\sum k_{i}}_{=0}\\right)\\left(\\beta_{0}+\\beta_{1}x_{h}\\right)\\\\\n& =\\beta_{0}+\\beta_{1}x_{h}\n\\end{align*}\\]\n\n\n6.1.4 The Variance of \\(\\hat{y}_h\\)\nUsing Theorem 4.3, we have the variance as \\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\n\\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sum\\left(c_{i}+k_{i}x_{h}\\right)^{2}{Var\\left[y_{h}\\right]}\\\\\n& =\\sum\\left(\\frac{1}{n}-\\bar{x}\\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}x_{h}\\right)^{2}\\sigma^{2}\\\\\n& =\\sigma^{2}\\sum\\left(\\frac{1}{n}+\\frac{\\left(x_{i}-\\bar{x}\\right)\\left(x_{h}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)^{2}\\\\\n& =\\sigma^{2}\\sum\\left(\\frac{1}{n^{2}}+2\\left(\\frac{1}{n}\\right)\\frac{\\left(x_{i}-\\bar{x}\\right)\\left(x_{h}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{i}-\\bar{x}\\right)^{2}\\left(x_{h}-\\bar{x}\\right)^{2}}{\\left(\\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(\\frac{1}{n}+2\\left(\\frac{1}{n}\\right)\\frac{\\left(x_{h}-\\bar{x}\\right)\\sum\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}{\\left(\\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\n\n\n\nSo the sampling distribution of \\(\\hat{y}_{h}\\) is \\[\n\\begin{align}\n\\hat{y}_{h} & \\sim N\\left(\\beta_{0}+\\beta_{1}x_{h},\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\right)\n\\end{align}\n\\tag{6.1}\\]\nWe will need to estimate \\(\\sigma^{2}\\) with \\(s^{2}\\). This will mean that the confidence interval is a \\(t\\) interval.\n\n\n6.1.5 Confidence Interval for the Mean Response\nA \\(\\left(100-\\alpha\\right)100\\%\\) confidence interval for the mean response is \\[\n\\begin{align}\n\\hat{y}_{h} \\pm t_{\\alpha/2}\\sqrt{s^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)}\n\\end{align}\n\\tag{6.2}\\]"
  },
  {
    "objectID": "06_Using.html#predicting-the-response",
    "href": "06_Using.html#predicting-the-response",
    "title": "6  Using the Simple Linear Model",
    "section": "6.2 Predicting the Response",
    "text": "6.2 Predicting the Response\n\n6.2.1 The Predicted Response\nPreviously, we estimated the mean of all \\(y\\)s for some value of \\(x_{h}\\).\nSuppose we want to predict one value of the response variable \\(y\\) for some value of \\(x_{h}\\). We will denote this predicted value as \\(y_{h\\left(pred\\right)}\\).\n\n\n6.2.2 Prediction When the True Line is Known\nOur best point predictor will be the mean (since it is the most likely value). If we knew the the true regression line, then we could predict at \\[\\begin{align*}\ny_{h\\left(pred\\right)} & =\\beta_{0}+\\beta_{1}x_{h}\n\\end{align*}\\]\nThe variance of \\(y_{h\\left(pred\\right)}\\) would be \\[\\begin{align*}\nVar\\left[y_{h}\\right] & =\\sigma^{2}\n\\end{align*}\\]\nThen we can be \\(\\left(1-\\alpha\\right)100\\%\\) confident that the predicted value \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nz_{\\alpha/2}\\sigma\n\\end{align*}\\] units away from the line.\nIf we don’t know \\(\\sigma\\), then we could estimate it with \\(s\\) and we can be \\(\\left(1-\\alpha\\right)100\\%\\) confident that the predicted value \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nt_{\\alpha/2}s\n\\end{align*}\\] units away from the line.\n\n\n6.2.3 Predicting When the True Line is Unknown\nOf course, we do not know the true regression line. We will need to estimate it first.\nUsing the least squares estimators, we will predict at \\[\\begin{align*}\n\\hat{y}_{h} & =b_0+b_1x_{h}\n\\end{align*}\\]\n\n\n6.2.4 The Variance of the Predicted Response\nSince \\(\\hat{y}_{h}\\) is a random variable, it will have a sampling distribution. From Equation 6.1, that sampling distribution has a variance of \\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nThus, the variance of the prediction of \\(y_{h\\left(pred\\right)}\\) will be the sum of the variance of the response variable: \\[\\begin{align*}\n\\sigma^{2}\n\\end{align*}\\] and the variance of the fitted line: \\[\\begin{align*}\n\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nSo the variance of \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nVar\\left[y_{h\\left(pred\\right)}\\right] & =\\sigma^{2}+\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(1+\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nSince \\(y\\) is normally distributed, then we have a \\(\\left(100-\\alpha\\right)100\\%\\) prediction interval for \\(y_{h\\left(pred\\right)}\\) as \\[\n\\begin{align}\n{\\hat{y}_{h} \\pm t_{\\alpha/2}\\sqrt{s^{2}\\left(1+\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)}}\n\\end{align}\n\\]#eq-w2_21\n\nExample 6.1 (Example 5.2 revisited) Let’s examine the trees data from Example 5.2.\nRecall the least squares fit:\n\nlibrary(datasets)\nlibrary(tidyverse)\n\nggplot(data=trees, aes(x=Girth, y=Volume))+\n  geom_point()+\n  geom_smooth(method='lm',formula=y~x,se = F)\n\n\n\nfit = lm(Volume~Girth, data=trees)\nfit\n\n\nCall:\nlm(formula = Volume ~ Girth, data = trees)\n\nCoefficients:\n(Intercept)        Girth  \n    -36.943        5.066  \n\n\nTo find the confidence and prediction intervals, we must construct a new data frame with the value of \\(X_h\\). This value is then used, along with the lm object, to the predict function. If no value of \\(X_h\\) is provided, then predict will provide intervals for all values of \\(x\\) found in the data used in lm.\nRecall the least squares fit:\n\n# get a 95% confidence interval for the mean Volume\n# when girth is 16\nxh=data.frame(Girth=16)\n\nfit |&gt; predict(xh,interval=\"confidence\",level=0.95)\n\n       fit      lwr      upr\n1 44.11024 42.01796 46.20252\n\n# 95% prediction interval for one value of Volume when\n# girth is 16\nfit |&gt; predict(xh,interval=\"prediction\",level=0.95)\n\n       fit     lwr      upr\n1 44.11024 35.1658 53.05469\n\n\nWe can plot the confidence interval for all values of \\(x\\) by using the geom_smooth command in ggplot:\n\nggplot(data=trees, aes(x=Girth, y=Volume)) + \n  geom_point() + \n  geom_smooth(method='lm',formula=y~x)\n\n\n\n\nWe can plot prediction intervals by adding them manually:\n\npred_int = fit |&gt; predict(interval=\"prediction\",level=0.95) |&gt; as.data.frame()\ndat = cbind(trees, pred_int)\n\nggplot(data=dat, aes(x=Girth, y=Volume)) +\n  geom_point() +\n  geom_smooth(method='lm',formula=y~x) +\n  geom_line(aes(y=lwr), color = \"red\", linetype = \"dashed\") +\n  geom_line(aes(y=upr), color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method=lm, se=TRUE)\n\n\n\n\nNote that the prediction interval (the red dashed lines) is wider than the prediction interval. This is because the prediction interval has the extra source of variability.\n\n\n\n6.2.5 Extrapolation and Precision\nWhen using the least squares prediction equation to estimate the mean value of \\(y\\) or to predict a particular value of \\(y\\) for values of \\(x\\) outside the range of your sample data, you may encounter much larger errors than expected. This practice is known as extrapolation.\nEven though the least squares model might fit the data well within the range of sample \\(x\\) values, it can poorly represent the true model for values of \\(x\\) outside this range.\nAs the sample size \\(n\\) increases, the width of the confidence interval decreases. In theory, you can achieve as precise an estimate of the mean value of \\(y\\) as desired for any given \\(x\\) by selecting a large enough sample.\nSimilarly, the prediction interval for a new value of \\(y\\) also becomes narrower as \\(n\\) increases. However, the prediction interval has a lower limit, which is reflected in the formula:\n\\[\n\\hat{y} \\pm z_{\\alpha/2} \\sigma\n\\]\nThis means that no matter how large the sample, the interval can’t shrink below a certain size unless you reduce the standard deviation of the regression model, \\(\\sigma\\). To make more accurate predictions for new values of \\(y\\), you must improve the model—either by using a curvilinear relationship with \\(x\\), adding new independent variables, or both."
  },
  {
    "objectID": "07_Checking.html#residual-diagnostics",
    "href": "07_Checking.html#residual-diagnostics",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "7.1 Residual Diagnostics",
    "text": "7.1 Residual Diagnostics\n\n7.1.1 Model Assumptions\nLet’s review the assumptions for the simple linear regression model:\n\nThe mean of the probability distribution of \\(\\varepsilon\\) is 0.\nThe variance of the probability distribution of \\(\\varepsilon\\) is constant for all settings of the independent variable \\(x\\).\nThe probability distribution of \\(\\varepsilon\\) is normal.\nThe errors associated with any two different observations are independent.\n\nAfter fitting the model, we will need to check these assumptions.\n\n\n7.1.2 Residuals\nWe check the assumptions of the model by examining the residuals: \\[\n\\begin{align}\n{e_{i}  =y_{i}-\\hat{y}_{i}}\n\\end{align}\n\\tag{7.1}\\]\nWe do this since the assumptions, with the exception of the linearity assumption, are based on the error terms \\(\\varepsilon_i\\). We can think of \\(e_i\\) as an observed value of \\(\\varepsilon_i\\).\n\n\n7.1.3 Properties of Residuals\nBelow are some properties of the residuals: \\[\n\\begin{align}\n\\sum e_{i} & =0\\\\\n\\sum x_{i}e_{i} & =0\\\\\n\\sum\\hat{y}_{i}e_{i} & =0 \\\\\n\\sum y_{i} & =\\sum\\hat{y}_{i}\n\\end{align}\n\\tag{7.2}\\]\nClearly, from Equation 7.2, the mean of the residuals is \\[\n\\begin{align}\n    \\bar{e}_i=0\n\\end{align}\n\\tag{7.3}\\]\nThe variance of all \\(n\\) residuals, \\(e_1,\\ldots,e_n\\) is \\[\n\\begin{align}\n\\frac{\\sum\\left(e_{i}-\\bar{e}\\right)^{2}}{n-2} & =\\frac{\\sum e_{i}^{2}}{n-2}\\\\\n& =\\frac{SSE}{n-2}\\\\\n& =MSE\\\\\n& =s^{2}\n\\end{align}\n\\tag{7.4}\\]\n\n\n7.1.4 Semistudentized Residuals\nIt will be helpful to studentize each residuals. As always, we do this by subtracting off the mean, \\(\\bar{e}_{i}\\), and dividing by the standard error of \\(e_{i}\\).\nWe know by Equation 7.3 that \\(\\bar{e}_{i}=0\\).\nIn Equation 7.4, we said the variance of the sample of the \\(e_{i}\\)’s is MSE. For each individual \\(e_{i}\\), the standard error is not quite \\(\\sqrt{MSE}\\). The actual standard error is dependent on the predictor variable(s). We will discuss this more in multiple regression.\nFor now, we will use the approximation \\(\\sqrt{MSE}\\) and calculate \\[\\begin{align*}\ne_{i}^{*} & =\\frac{e_{i}-\\bar{e}}{\\sqrt{MSE}}\\\\\n& =\\frac{e_{i}}{\\sqrt{MSE}}\n\\end{align*}\\] We call \\(e_{i}^{*}\\) the semistudentized residual since the standard error is an approximation."
  },
  {
    "objectID": "07_Checking.html#the-linearity-assumption",
    "href": "07_Checking.html#the-linearity-assumption",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "7.2 The Linearity Assumption",
    "text": "7.2 The Linearity Assumption\n\n7.2.1 Residual Plots\nWe can check the linearity assumption by plotting the residuals vs the predictor variable or plotting the residuals vs the fitted values.\nWe usually examine a scatterplot to determine if a linear relationship between \\(x\\) and \\(y\\) is appropriate. There are times when the scatterplot makes it difficult to see if a nonlinear relationship exists. This may be the case if the observed \\(y\\) are close to the fitted line \\(\\hat{y}_i\\). This usually means the slope is steep.\n\nExample 7.1 (Weight and Height Data) In this example, we will consider the weights (in kg) and heights (in m) of 16 women ages 30-39. The dataset is from kaggle.\n\nlibrary(tidyverse)\n\ndat = read_csv(\"Weight_Height.csv\")\n\nggplot(dat, aes(x=Weight, y=Height))+\n  geom_point()\n\n\n\n#fit the model\nfit = lm(Weight~Height, data=dat)\n\n#plot with regression line\nggplot(dat, aes(x=Weight, y=Height))+\n  geom_point()+\n  geom_smooth(method=\"lm\", formula=y~x, se=F)\n\n\n\n#make dataset with Weight, the fitted values, and residuals\ndat2 = tibble(x = dat$Weight, \n              yhat = fit$fitted.values, \n              e = fit$residuals)\n\n#plot x by residuals\nggplot(dat2, aes(x=x, y=e))+\n  geom_point()+\n  geom_hline(yintercept = 0, col=\"red\")\n\n\n\n#plot fitted values by residuals\nggplot(dat2, aes(x=yhat, y=e))+\n  geom_point()+\n  geom_hline(yintercept = 0, col=\"red\")\n\n\n\n\n\n\n\n7.2.2 Plotting against Predictor Variable or Fitted Values\nPlotting the residuals against \\(x\\) will provide the same information as plotting the residuals against \\(\\hat{y}\\) for the simple linear regression model.\nWhen more predictor variables are considered, then plotting against the \\(x\\) variables and plotting against \\(\\hat{y}\\) may provide different information. It is usually helpful to plot both in that case.\n\n\n7.2.3 Data Transformation for Linearity\nWhen the linearity assumption does not hold (as seen in the residual plots), then a nonlinear model may be considered or a transformation on either \\(x\\) or \\(y\\) can be attempted to make the relationship linear.\n\n\n7.2.4 Transforming \\(x\\)\nTransforming the response variable \\(y\\) may lead to issues with other assumptions such as the constant variance assumption or the normality of \\(\\varepsilon\\) assumption.\nIf our only concern is the linearity assumption, then transforming \\(x\\) will be the best option. This transformation may be a square root transformation \\(\\sqrt{X}\\), a log transformation \\(\\log{X}\\), or some power transformation \\(X^{p}\\) were \\(p\\) is some real number.\nSometimes a transformation of \\(x\\) will not be enough to satisfy the linearity assumption. In that case, the simple linear regression model should be abandoned in favor of a nonlinear model."
  },
  {
    "objectID": "07_Checking.html#homogeneity-of-variance",
    "href": "07_Checking.html#homogeneity-of-variance",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "7.3 Homogeneity of Variance",
    "text": "7.3 Homogeneity of Variance\n\n7.3.1 Residual Plots\nAs we did previously, we can plot the residuals against the predictor variable \\(x\\) or against the fitted values \\(\\hat{y}\\) to help determine whether the variance of the error term \\(\\varepsilon\\) is constant.\nWhen the variance is constant, we say the model has homoscedasticity. When the variance is nonconstant, we say the model has heteroscedasticity.\n\n\n7.3.2 Absolute Residuals and Squared Residuals\nWhen examining a residual plot for non-constant variance, we look for any clear changes in the spread of the residuals. One common clear pattern seen when heteroscedasticity is present is a cone pattern.\nWe are usually not concerned about the sign of the residual when examining for heteroscedasticity. Thus, it is common to plot the absolute residuals or the squared residuals vs the predictor variable or fitted values.\nUsually a least squares line is then fit to the absolute residual or squared residuals plot. If this line has a significant slope, then this gives evidence that the variance is nonconstant.\n\nExample 7.2 (Diastolic Blood Pressure Data) We will model the diastolic blood pressure by the age of 54 healthy adult women. The data are found in Kutner et al1.\n\nlibrary(tidyverse)\n\ndat = read.table(\"bloodpressure.txt\", header=T)\n\nfit = lm(dbp~age, data=dat)\n\n#add the fit line without using geom_smooth\nggplot(dat, aes(x=age, y=dbp))+\n  geom_point()+\n  geom_abline(slope = fit$coefficients[2],\n              intercept = fit$coefficients[1])\n\n\n\n\nBy examining the scatterplot of dbp vs age, we already see evidence of nonconstant variance.\n\ndat$e = fit |&gt; resid()\ndat$yhat = fit |&gt; fitted()\n\n#use the geom_smooth function to add a least squares line\n#for the residuals, the least squares line will always be #horizontal at 0\nggplot(dat, aes(x=age, y=e))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n#plot the squared residuals and add least squares line\nggplot(dat, aes(x=age, y=e^2))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n#plot the squared residuals and add least squares line\nggplot(dat, aes(x=age, y=abs(e)))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n\nIn any of the residual plots we examine, we see the “cone” shape of the residuals which indicates the variance is nonconstant.\n\n\n\n7.3.3 Tests for Heteroscedasticity\nWe can set up a hypothesis test for heteroscedasticity: \\[\\begin{align*}\nH_0:&\\text{ the variance is constant}\\\\\nH_a:&\\text{ the variance is non-constant}\n\\end{align*}\\]\nThe procedures we will use usually test for variance that increases or decreases over the values of \\(x\\). That is, the spread of the points about the line is a cone shape.\n\n\n7.3.4 Levene’s Test and Brown-Forsythe Test\nThe Levene test2 starts by dividing the range of the predictor variable \\(x\\) into \\(k\\) intervals. For each of the intervals, calculate the mean of the residuals in that interval \\(\\bar{e}_{j}\\) where \\(j\\) denotes the \\(j\\)th interval.\nNow, define the absolute deviation from the mean \\[\nd_{ik}=|e_{ik}-\\bar{e}_{j}|\n\\]\nAn ANOVA F-test is then performed on the \\(k\\) groups. The ANOVA F-test will be discussed more later in the course. A small p-value is evidence that the variance is non-constant over the values of \\(x\\).\nThe Brown-Forsythe test3 is a modification of the Levene test in which the median \\(\\tilde{e}_k\\) is used instead of the mean \\(\\bar{e}_l\\). This test is robust against nonnormal errors.\n\n\n7.3.5 Breusch-Pagan Test\nA second test for non-constant variance is the Breusch-Pagan test4. This test assumes the variance for each \\(\\varepsilon_i\\) is related to the values of \\(x\\) in the following way: \\[\n\\ln \\sigma^2_i = \\gamma_0 + \\gamma_1 X_i\n\\] Hence, a linear regression is assumed between \\(\\sigma^2\\) and \\(x\\). We can fit the line by squaring the residuals and regressing on \\(x\\). The third plot in example above shows the squared residuals plotted against \\(x\\) and the fitted line.\nIf the variance is non-constant, then the slope of this line will be non-zero. Thus, a test for the slope is conducted. A small p-value is evidence that the variance is non-constant.\nBecause it is testing the slope, the Brown-Forsythe test assumes the error terms are independent and normally distributed.\n\nExample 7.3 (Example 7.2 revisited) Let’s examine the blood pressure data from Example 7.2 again.\nIf you want the Brown-Forsythe test, then you will need to determine the \\(k\\) groups yourself and then use the bf.test function in onewaytests package.\nIn the lmtest package, the Breusch-Pagan test can be conducted by using the bptest function.\n\nlibrary(tidyverse)\nlibrary(lmtest)\n\ndat = read.table(\"bloodpressure.txt\", header=T)\n\nfit = lm(dbp~age, data=dat)\n\n#Breusch-Pagan test\nbptest(dbp~age, data=dat)\n\n\n    studentized Breusch-Pagan test\n\ndata:  dbp ~ age\nBP = 12.541, df = 1, p-value = 0.0003981\n\n\nSince the p-value is low, then there is sufficient evidence to conclude the variance is not constant through different values of \\(x\\)."
  },
  {
    "objectID": "07_Checking.html#footnotes",
    "href": "07_Checking.html#footnotes",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "",
    "text": "Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied Linear Statistical Models McGraw-Hill/lrwin series operations and decision sciences.↩︎\nLevene, H. (1960) Robust Tests for Equality of Variances. In: Olkin, I., Ed., Contributions to Probability and Statistics, Stanford University Press, Palo Alto, 278-292.↩︎\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for the equality of variances. Journal of the American statistical association, 69(346), 364-367.↩︎\nBreusch, T. S., & Pagan, A. R. (1979). A simple test for heteroscedasticity and random coefficient variation. Econometrica: Journal of the econometric society, 1287-1294.↩︎"
  },
  {
    "objectID": "08_Checking2.html#checking-for-outliers",
    "href": "08_Checking2.html#checking-for-outliers",
    "title": "8  Checking the Normality and Independence Assumptions and Outliers",
    "section": "8.1 Checking for Outliers",
    "text": "8.1 Checking for Outliers\n\n8.1.1 Outliers With Respect to the Predictor Variable and the Response Variable\nWhen checking for outliers, we must think in terms of two dimensions since we have two variables, \\(x\\) and \\(y\\).\nSince we are interested in modeling the response variable \\(y\\) in the model, we will mainly be concerned with outliers with respect to \\(y\\). However, outliers with respect to \\(x\\) may also be of concern if it affects the fitted line.\n\n\n8.1.2 Effect on the Fitted Line\nA demonstration on the effect of outliers on the fitted line can be found at http://www.jpstats.org/Regression/ch_03_04.html#sub1_1\n\n\n\n8.1.3 Detecting Outliers with Semistudentized Residual Plots\nSince the effect on the fitted line is determined mainly by how far the point is from the line, we will identify outliers by examining the residuals, in particular, the semistudentized residuals \\[\\begin{align*}\ne_{i}^{*} & =\\frac{e_{i}-\\bar{e}}{\\sqrt{MSE}}\\\\\n& =\\frac{e_{i}}{\\sqrt{MSE}}\n\\end{align*}\\]\nWe can plot \\(e_{i}^{*}\\) against \\(x\\) or against \\(\\hat{y}\\). A general rule of thumb is any value of \\(e_{i}^{*}\\) below -4 or above 4 should be considered an outlier. Note that this rule is only applicable to large \\(n\\).\nWe will discuss other methods for detecting outliers after we cover multiple regression.\n\nExample 8.1 (Calculating Semistudentized Residuals) Let’s examine 50 observations of \\(x\\) and \\(y\\):\n\nlibrary(tidyverse)\n\ndat = read_csv(\"example_08_01.csv\")\n\ndat |&gt; \n  ggplot(aes(x=x,y=y))+\n    geom_point()+\n    geom_smooth(method = \"lm\")\n\n\n\nfit = lm(y~x, data=dat)\n\n#calculate semistudentized residuals\ndat = dat |&gt; \n  mutate(e.star = fit$residuals / summary(fit)$sigma)\n\ndat |&gt; \n  ggplot(aes(x=x, y=e.star))+\n    geom_point()\n\n\n\n\nWe can see from the semistudentized plot that there is one observation with a value of \\(e^*\\) below -3. Although this is not below the rule of thumb of -4, we note that our sample size \\(n\\) is only moderate and not large. So we may want to investigate an observation with a value of \\(e^*\\) below -3.\n\nOnce we see there is a potential outlier, we must investigate why it is an outlier. If the observation is unusually small or large due to a data recording error, then perhaps the value can be corrected or just deleted from the dataset. If we cannot determine this is the cause of the outlier for certain, then we should not remove the observation. This observation could be unusual just due to chance."
  },
  {
    "objectID": "08_Checking2.html#correlated-error-terms",
    "href": "08_Checking2.html#correlated-error-terms",
    "title": "8  Checking the Normality and Independence Assumptions and Outliers",
    "section": "8.2 Correlated Error Terms",
    "text": "8.2 Correlated Error Terms\n\n8.2.1 Assumption of Independence\nSince we are assuming the random error term \\(\\varepsilon\\) are normal, we want to check to see the uncorrelated errors assumption. If there is no correlation between the residuals, then we can assume independence.\n\n\n8.2.2 Residual Sequence Plots\nThe usual cause of correlation in the residuals is data taken in some type of sequence such as time or space. When the error terms are correlated over time or some other sequence, we say they are serially correlated or autocorrelated.\nWhen the data are taken in some sequence, a sequence plot of the residuals may show a pattern indicating autocorrelation. In a sequence plot, the residuals are plotted against the observation index \\(i\\). If there is no autocorrelation, then the residuals should be “randomly” spread about zero. If there is a pattern, then there is evidence of autocorrelation.\n\n\n8.2.3 Autocorrelation Function Plot\nSometime a residual sequence plot may not show an obvious pattern but autocorrelation may still exist.\nAnother plot that helps examine correlation that may not be visible in the sequence plot is the autocorrelation function plot (ACF).\nIn the ACF plot, correlations are calculated between residuals some \\(k\\) index away. That is, \\[\n\\begin{align}\nr_{k} & =\\widehat{Cor}\\left[e_{i},e_{i+k}\\right]\\\\\n& =\\frac{\\sum_{i=1}^{n-k}\\left(e_{i}-\\bar{e}\\right)\\left(e_{i+k}-\\bar{e}\\right)}{\\sum_{i=1}^{n}\\left(e_{i}-\\bar{e}\\right)^{2}}\n\\end{align}\n\\tag{8.1}\\]\nIn an ACF plot, \\(r_k\\) is plotted for varying values of \\(k\\). If the value of \\(r_k\\) is larger in magnitude than some threshold shown on the plot (usually a 95% confidence interval), then we consider this evidence of autocorrelation.\n\n\n8.2.4 Tests for Autocorrelation\nIn addition to examining serial plots and ACF plots, tests can be conducted for significant autocorrelation. In each of these tests, the null hypothesis is there is no autocorrelation.\n\n\n8.2.5 Durbin-Watson Test\nThe Durbin-Watson1 test is for autocorrelation at \\(k=1\\) in Equation 8.1. That is, it tests for correlation one index (one time point) away.\nThe Durbin-Watson test can be conducted in R with the dwtest function in the lmtest package.\n\n\n8.2.6 Ljung-Box Test\nThe Ljung-Box2 test differs from the Durbin-Watson test in that it tests for overall correlation over all lags up to \\(k\\) in Equation 8.1. For example, if \\(k=4\\) then the Ljung-Box test is for significant autocorrelation over all lags up to \\(k=4\\).\nThe Ljung-Box test can be conducted in R with the Box.test function with the argument type=(\"Ljung\"). This function is in base R.\n\n\n8.2.7 Breusch-Godfrey Test\nThe Breusch-Godfrey34 test is similar to the Ljung-Box test in that it tests for overall correlation over all lags up to \\(k\\). The difference between the two test is not of concern in the regression models we will examine in this course. When using time series models, then the Breusch-Godfrey test is preferred over the Ljung-Box test due to asymptotic justification.\nThe Breusch-Godfrey test can be conducted in R with the bgtest function in the lmtest package.\n\nExample 8.2 (Portland Crime Data) Let’s look at data collected on the mean temperature for each day in Portland, OR, and the number of non-violent crimes reported that day. The crime data was part of a public database gathered from www.portlandoregon.gov. The data are presented in order by day. The variable \\(x\\) in the dataset is the day index number.\n\nlibrary(tidyverse)\nlibrary(lmtest)\nlibrary(forecast)\n\ndat = read_csv(\"PortlandWeatherCrime.csv\")\n\n#the file does not have a name for the index variable\nnames(dat)[1] = \"day\"\n\ndat |&gt; \n  ggplot(aes(x=Mean_Temp, y=Num_Total_Crimes))+\n    geom_point()+\n    geom_smooth(method=\"lm\")\n\n\n\nfit = lm(Num_Total_Crimes~Mean_Temp, data=dat)\nfit |&gt; summary()\n\n\nCall:\nlm(formula = Num_Total_Crimes ~ Mean_Temp, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-168.198  -41.055    0.149   40.455  183.680 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 281.0344     6.4456   43.60   &lt;2e-16 ***\nMean_Temp     4.3061     0.1116   38.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.69 on 1765 degrees of freedom\nMultiple R-squared:  0.4576,    Adjusted R-squared:  0.4573 \nF-statistic:  1489 on 1 and 1765 DF,  p-value: &lt; 2.2e-16\n\ndat$res = fit |&gt; resid()\n\nggplot(dat, aes(x=day, y=res))+\n  geom_point()\n\n\n\n\nWe can see that the residuals have a pattern where the values at the lower levels of the index tend to be below zero whereas the values at the higher levels of the index tend to be above zero. This is evidence of autocorrelation in the residuals.\n\nggAcf(dat$res)\n\n\n\n\nThe values of the ACF at all lags are beyond the blue guideline for significant autocorreleation.\nNote that in the Ljung-Box test and the Breusch-Godfrey test below, we tested up to lag 7. We chose this lag since the data was taken over time and it would make sense for values at seven days apart to be similar. That is, we expect the number of crimes on Mondays to be similar, the number of crimes on Tuesdays to be similar, etc.\n\ndwtest(fit)\n\n\n    Durbin-Watson test\n\ndata:  fit\nDW = 0.66764, p-value &lt; 2.2e-16\nalternative hypothesis: true autocorrelation is greater than 0\n\nBox.test(dat$res, lag=7,type=\"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  dat$res\nX-squared = 3865, df = 7, p-value &lt; 2.2e-16\n\nbgtest(fit, order=7)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 7\n\ndata:  fit\nLM test = 977.84, df = 7, p-value &lt; 2.2e-16\n\n\n\nWhen the assumption of independence is violated, then a difference in the \\(y\\) values could help remove the autocorrelation. This difference is \\[\ny^{\\prime} = y_i - y_{i-k}\n\\] where \\(k\\) is some max lag where autocorrelation is significant. This difference \\(Y^{\\prime}\\) is then regressed on \\(x\\). This difference may not help, in which case a time series model would be necessary."
  },
  {
    "objectID": "08_Checking2.html#normality-of-the-residuals",
    "href": "08_Checking2.html#normality-of-the-residuals",
    "title": "8  Checking the Normality and Independence Assumptions and Outliers",
    "section": "8.3 Normality of the Residuals",
    "text": "8.3 Normality of the Residuals\n\n8.3.1 The Normality Assumption\nIf we do not have normality of the error terms, then the t-tests and t-intervals for \\({\\beta}_0\\) and \\({\\beta}_1\\) would not be valid.\nFurthermore, the confidence interval for the mean response and the prediction interval for the response would not be valid.\nWe can check the normality of error terms by examining the residuals of the fitted line.\n\n\n8.3.2 Graphically Checking Normality\nWe can graphically check the distribution of the residuals. The two most common ways to do this is with a histogram or with a normal probability plot.\nAnother (more general) name for a normal probability plot is a normal quantile-quantile (QQ) plot.\nFor a histogram, we check to see if the shape is approximately close to that of a normal distribution.\nFor a QQ plot, we check to see if the points approximately follow a straight line. Major departures from a straight line indicates nonnormality.\nIt is important to note that we will never see an exact normal distribution is real-world data. Thus, we will always look for approximate normality in the residuals.\nThe inferences discussed previously are still valid for small departure of normality. However, major departures from normality will lead to incorrect p-values in the hypothesis tests and incorrect coverages in the intervals.\n\n\n8.3.3 Examples of QQ-plots\nBelow are some examples of histograms and QQ-plots for some simulated datasets.\n\n\n\n\n\n\n\n(a) Normal - Histogram\n\n\n\n\n\n\n\n(b) Normal - QQ plot\n\n\n\n\n\n\n\n\n\n(c) Right skewed - Histogram\n\n\n\n\n\n\n\n(d) Right skewed - QQ plot\n\n\n\n\n\n\n\n\n\n(e) Heavy right skewed - Histogram\n\n\n\n\n\n\n\n(f) Heavy right skewed - QQ plot\n\n\n\n\n\n\n\n\n\n(g) Left skewed - Histogram\n\n\n\n\n\n\n\n(h) Left skewed - QQ plot\n\n\n\n\n\n\n\n\n\n(i) Heavy tails - Histogram\n\n\n\n\n\n\n\n(j) Heavy tails - QQ plot\n\n\n\n\n\n\n\n\n\n(k) No tails - Histogram\n\n\n\n\n\n\n\n(l) No tails - QQ plot\n\n\n\n\nFigure 8.1: Examples of QQ-Plots\n\n\n\n\n8.3.4 The Shapiro-Wilk Test\nThere are a number of hypothesis test for normality. The most popular test is the Shapiro-Wilk5 test. This test has been found to have the most power among many of the other tests for normality6.\nIn the Shapiro-Wilk test, the null hypothesis is that the data are normally distributed and the alternative is that the data are not normally distributed.\nThis test can be conducted using the shapiro.test function in base R."
  },
  {
    "objectID": "08_Checking2.html#footnotes",
    "href": "08_Checking2.html#footnotes",
    "title": "8  Checking the Normality and Independence Assumptions and Outliers",
    "section": "",
    "text": "Durbin, J., & Watson, G. S. (1951). Testing for Serial Correlation in Least Squares Regression. II. Biometrika. Vol 38. (pp. 159-177).↩︎\nLjung, G. M., & Box, G. E. (1978). On a measure of lack of fit in time series models. Biometrika, 65(2), 297-303.↩︎\nBreusch, T. S. (1978). Testing for Autocorrelation in Dynamic Linear Models. Australian Economic Papers. 17: 334–355.↩︎\nGodfrey, L. G. (1978). Testing Against General Autoregressive and Moving Average Error Models when the Regressors Include Lagged Dependent Variables. Econometrica. 46: 1293–1301↩︎\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52(3-4), 591-611.↩︎\nRazali, N. M., & Wah, Y. B. (2011). Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of statistical modeling and analytics, 2(1), 21-33.↩︎"
  },
  {
    "objectID": "09_Tidymodels.html#specifying-the-model",
    "href": "09_Tidymodels.html#specifying-the-model",
    "title": "9  Simple Linear Regression with Tidymodels",
    "section": "9.1 Specifying the Model",
    "text": "9.1 Specifying the Model\nIn tidymodels, the first step in model building is to define the type of model you want to fit. The parsnip package is the tool within tidymodels that handles model specification. Unlike the traditional approach where you directly use a modeling function like lm(), parsnip separates the model definition from the model fitting, making the process more modular and adaptable to different engines (i.e., different computational backends).\n\n9.1.1 Model Specification with parsnip\nThe primary function to specify a model in parsnip is linear_reg(), which indicates that you are building a linear regression model. However, linear_reg() does not fit the model by itself. Instead, it allows you to define the general structure of the model, which is then paired with a computational engine. An engine is the specific function or package that will perform the calculations. For linear regression, the default engine is \"lm\", which corresponds to R’s base lm() function.\nLet’s break down how the model specification works:\n\n# Specify a linear regression model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\nlinear_reg(): This specifies that we are interested in a linear regression model. The model type is abstracted, allowing us to focus on the statistical method (linear regression) rather than the computational details.\nset_engine(\"lm\"): This command specifies that we will use the \"lm\" engine to estimate the linear regression parameters. parsnip supports other engines, such as glmnet for penalized regression models and stan for Bayesian regression models, which allows for great flexibility in model building.\n\n\n\n9.1.2 Hyperparameters and Engine Customization\nYou can also specify hyperparameters, which are settings that influence the model fitting process. For simple linear regression with the \"lm\" engine, there are no specific hyperparameters to set, but with more complex models (such as regularized regression), you might need to tune parameters like the penalty term or the mixture parameter for Lasso or Ridge regression (discussed in a later chapter).\nAn example of specifying a model with hyperparameters (for demonstration purposes):\n# Example for models that require hyperparameters\nridge_model = linear_reg(penalty = 0.1, mixture = 0) |&gt;\n  set_engine(\"glmnet\")\nAlthough these hyperparameters are not necessary for simple linear regression, parsnip enables seamless transition to more complex models, and this flexibility is a key feature of the tidymodels framework.\n\n\n9.1.3 Choosing the Right Model and Engine\nOne of the strengths of the parsnip package is that it decouples the conceptual model (linear regression) from the computational engine (e.g., \"lm\", \"glmnet\", \"stan\"). This allows the user to switch engines without changing the model structure.\nFor example, suppose you wanted to fit the same linear regression model using a Bayesian approach via the stan engine:\n# Specify a Bayesian linear regression model\nbayesian_lm_model = linear_reg() |&gt;\n  set_engine(\"stan\")\nIn this case, the model structure remains the same, but the estimation procedure differs (frequentist vs. Bayesian). This level of abstraction enhances the flexibility of your workflow, especially as you explore different modeling approaches.\n\n\n9.1.4 Benefits of Model Specification with parsnip\nThe model specification process in parsnip offers several key benefits. Its modularity allows for the separation of model specification from model fitting, enabling you to reuse the same model definition across different datasets or engines. This framework provides flexibility, making it easy to switch between various engines, such as lm(), glmnet(), or stan, without altering the core model structure.\nAdditionally, parsnip enhances scalability by simplifying the process of extending models. For example, moving from simple linear regression to more complex models like Ridge or Lasso regression only requires adjusting the engine and hyperparameters, without the need to rewrite your code.\nIn this section, we have seen how to specify a linear regression model using parsnip and set the engine to \"lm\". Next, we will define the relationship between the predictor and response variables and organize our workflow before fitting the model."
  },
  {
    "objectID": "09_Tidymodels.html#defining-the-workflow",
    "href": "09_Tidymodels.html#defining-the-workflow",
    "title": "9  Simple Linear Regression with Tidymodels",
    "section": "9.2 Defining the Workflow",
    "text": "9.2 Defining the Workflow\nIn the tidymodels ecosystem, workflows serve as a central organizing structure that ties together the components of a model-building process. A workflow in tidymodels allows you to combine different elements, such as the model specification, preprocessing steps, and the formula that defines the relationship between the predictor(s) and response. This section will explain the importance of workflows and demonstrate how to create one for simple linear regression.\n\n9.2.1 What is a Workflow?\nA workflow can be thought of as a blueprint that organizes how data flows through different steps of the modeling process. It encapsulates model specification and data preprocessing tasks, which can include transformations such as scaling, normalization, or encoding categorical variables. In traditional modeling approaches, these steps are often written separately, which can lead to code that is harder to manage and prone to errors, especially as the complexity of the model increases.\nWith workflows, the entire process becomes more organized and reproducible. The key benefit is that you don’t need to repeatedly specify how the data should be prepared or how the model should be fit. Once a workflow is defined, it can be reused or modified easily to fit different models or datasets.\n\n\n9.2.2 Creating a Workflow for Simple Linear Regression\nFor a simple linear regression model, our workflow consists of two main components: the formula that defines the relationship between the response and predictor variables, and the model specification itself. Since simple linear regression does not require extensive preprocessing (like handling categorical variables or missing data), the workflow is relatively straightforward.\nLet’s begin by constructing the workflow for predicting miles per gallon (mpg) using horsepower (hp) from the mtcars dataset. Recall that the model specification (lm_model) was defined in the previous section.\n\n# Define a workflow\nlm_workflow = workflow() |&gt;\n  add_model(lm_model) |&gt;\n  add_formula(mpg ~ hp)\n\nIn this code:\n\nworkflow() initializes an empty workflow.\nadd_model() adds the linear regression model (lm_model) that was specified earlier using the parsnip package.\nadd_formula() defines the model’s formula, which indicates that the response variable mpg is modeled as a function of the predictor hp.\n\nThe formula-based approach used in this workflow is intuitive for most users familiar with R’s base modeling functions, such as lm() and glm(). However, workflows also support more advanced techniques, such as specifying custom preprocessing steps with the recipes package, which we will explore later.\n\n\n9.2.3 The Formula Interface\nThe formula interface in tidymodels is a key aspect of defining relationships between variables. The tilde (~) symbol is used to separate the response variable (on the left-hand side) from the predictor(s) (on the right-hand side). In the example mpg ~ hp, the model will predict mpg using hp as the predictor. This is a simple linear regression model, where the model assumes a linear relationship between these two variables.\nIf there were multiple predictors, the formula would include them on the right-hand side, separated by a plus sign (+), such as mpg ~ hp + wt + qsec, indicating a multiple linear regression model. The formula syntax allows for flexible model building, including interaction terms (* or :) and transformations (e.g., log() or polynomial terms).\n\n\n9.2.4 Preprocessing in Workflows\nOne of the primary advantages of using workflows is the ability to integrate preprocessing steps seamlessly into the model-building process. While simple linear regression may not require extensive preprocessing, workflows can easily handle more complex tasks, such as:\n\nStandardizing predictors: Scaling variables to have mean 0 and standard deviation 1.\nHandling missing data: Imputing missing values.\nEncoding categorical variables: Converting factors into dummy or one-hot encoded variables.\n\nFor example, suppose that instead of horsepower (hp), we had a categorical variable representing car transmission type (am), and we wanted to include it as a predictor. In this case, we could use the recipes package to preprocess the data by encoding the categorical variable. The workflow can be extended as follows:\n# Example of adding preprocessing using recipes\nrecipe = recipe(mpg ~ hp + am, data = mtcars) |&gt;\n  step_dummy(all_nominal_predictors())\n\nlm_workflow = workflow() |&gt;\n  add_model(lm_model) |&gt;\n  add_recipe(recipe)\nIn this example, recipe() initializes a preprocessing recipe that converts the categorical predictor am into a set of dummy variables. step_dummy() is one of many preprocessing steps available in the recipes package.\n\n\n9.2.5 Why Use Workflows?\nWorkflows provide several key benefits when building models with tidymodels:\n\nReproducibility: By encapsulating the entire modeling process into a workflow, the model can be easily reproduced, reducing the chances of human error. Each step is clearly defined and can be executed in a consistent manner.\nModularity: You can modify or swap out different components of the workflow—such as changing the model from linear regression to a different type of regression—without rewriting significant portions of your code.\nSeparation of Concerns: Workflows help you organize your code by clearly separating data preprocessing, model specification, and model fitting. This makes it easier to debug, extend, or modify your models later on.\nPreprocessing Integration: Workflows enable the integration of complex preprocessing pipelines, ensuring that data transformations and model fitting occur seamlessly within a single structure. This is especially useful for more advanced models, where data preprocessing steps are often critical to model performance.\n\nIn this section, we introduced the concept of workflows in tidymodels, which serves as a crucial tool for organizing and managing the different components of the modeling process. For simple linear regression, the workflow was relatively simple, involving a formula and a model specification. However, workflows can be extended to handle more complex models and preprocessing tasks as we will see in later chapters. By using workflows, you ensure that your model-building process is organized, reproducible, and adaptable to future changes."
  },
  {
    "objectID": "09_Tidymodels.html#fitting-the-model",
    "href": "09_Tidymodels.html#fitting-the-model",
    "title": "9  Simple Linear Regression with Tidymodels",
    "section": "9.3 Fitting the Model",
    "text": "9.3 Fitting the Model\nOnce the model and workflow have been defined, the next step is to fit the model to the data. In tidymodels, this is done using the fit() function, which estimates the model parameters (such as the intercept and slope in simple linear regression) based on the data provided. The fitting process involves using the workflow to apply the model specification and formula to the dataset.\n\n9.3.1 The fit() Function\nThe fit() function is a central function in the modeling process that takes the workflow and the data as inputs. It applies the formula and estimates the model’s parameters by minimizing the SSE (for linear regression models) or another appropriate criterion depending on the model type.\nFor our simple linear regression model, we fit the model using the mtcars dataset, where the response variable is mpg (miles per gallon) and the predictor is hp (horsepower).\n\n# Fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = mtcars)\n\nIn this command:\n\nlm_workflow: The workflow containing the model and the formula for simple linear regression.\ndata = mtcars: The dataset used to fit the model. The model will use this data to estimate the parameters \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope).\n\nThe fit() function then processes the data, applies the specified formula (mpg ~ hp), and estimates the model parameters using the specified engine (lm in this case). In the background, tidymodels calls the R base lm() function to fit the model, but the workflow structure abstracts these technical details, making it easier to switch between engines without altering the core syntax.\n\n\n9.3.2 Model Output\nAfter fitting the model, it’s essential to inspect the model’s output to understand the estimated relationship between the predictor and response variables. The fitted workflow object (lm_fit) contains a number of important details, including the estimated coefficients for the linear model, the residuals, and performance metrics.\nLet’s start by examining the output of the fitted model:\n\n# View the model fit summary\nlm_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nmpg ~ hp\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)           hp  \n   30.09886     -0.06823  \n\n\nThe output of lm_fit provides an overview of the fitted model, including the coefficients for the intercept and slope. This information is crucial for interpreting the relationship between the predictor and the response variable. Specifically, the estimated intercept (\\(b_0\\)) represents the expected value of mpg when hp is zero, and the slope (\\(b_1\\)) represents the expected change in mpg for each one-unit increase in hp.\n\n\n9.3.3 Extracting Model Coefficients\nTo better understand the results, you can extract the estimated coefficients from the model using the tidy() function from the broom package, which is part of the tidymodels ecosystem. This function provides a clean, easy-to-read summary of the model’s coefficients, along with other important statistics such as standard errors, t-values, and p-values.\n\n# Extract coefficients\ntidy(lm_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  30.1       1.63       18.4  6.64e-18\n2 hp           -0.0682    0.0101     -6.74 1.79e- 7\n\n\nThis table shows the estimated coefficients for the intercept and slope:\n\nIntercept: The estimated intercept (\\(b_0\\)) is 30.10, meaning that when hp is zero, the predicted mpg is approximately 30.10. In this example, the interpretation is not practical.\nSlope: The estimated slope (\\(b_1\\)) is -0.068, indicating that for each additional unit of horsepower, the mpg decreases by about 0.068 miles per gallon, on average. The negative sign confirms an inverse relationship between mpg and hp.\n\nAdditionally, the table provides the standard errors, t-statistics, and p-values, which are useful for assessing the statistical significance of the estimated coefficients. In this case, both the intercept and the slope have very small p-values (less than 0.001), suggesting that they are statistically significant at conventional significance levels (e.g., 0.05).\n\n\n9.3.4 Model Predictions\nAfter fitting the model, it’s often useful to make predictions based on new or existing data. The predict() function allows you to generate predictions from the fitted model. In this case, we’ll generate predictions using the original dataset (mtcars), though in practice, you would typically use the model to predict on new, unseen data.\n\n# Make predictions on the original data\npredictions = lm_fit |&gt;\n  predict(new_data = mtcars)\n\n# Add predictions to the original data\nmtcars_with_preds = mtcars |&gt;\n  select(mpg, hp) |&gt; \n  mutate(predicted_mpg = predictions$.pred)\n\n# View the predictions\nhead(mtcars_with_preds)\n\n                   mpg  hp predicted_mpg\nMazda RX4         21.0 110      22.59375\nMazda RX4 Wag     21.0 110      22.59375\nDatsun 710        22.8  93      23.75363\nHornet 4 Drive    21.4 110      22.59375\nHornet Sportabout 18.7 175      18.15891\nValiant           18.1 105      22.93489\n\n\nIn this example, we use the predict() function to generate predicted values of mpg for each observation in the mtcars dataset. These predictions are stored in a new column called predicted_mpg. Comparing the predicted values to the actual values allows us to assess how well the model fits the data.\n\n\n9.3.5 Performance Metrics\nOnce predictions have been made, it’s important to evaluate the model’s performance. One common metric for regression models is the Root Mean Squared Error (RMSE), which measures the average difference between the predicted and actual values. Lower RMSE values indicate better model performance.\nYou can calculate the RMSE and other performance metrics using the yardstick package, another component of the tidymodels framework.\n\n# Calculate RMSE\nmetrics = lm_fit |&gt;\n  predict(new_data = mtcars) |&gt;\n  bind_cols(mtcars) |&gt;\n  metrics(truth = mpg, estimate = .pred)\n\n# View performance metrics\nmetrics\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       3.74 \n2 rsq     standard       0.602\n3 mae     standard       2.91 \n\n\nThis will output the RMSE, along with other useful metrics like the Mean Absolute Error (MAE) and R-squared.\n\nrmse: The Root Mean Squared Error (RMSE) is approximately 3.74, meaning that, on average, the model’s predictions are off by about 3.74 miles per gallon.\nrsq: The R-squared value is 0.60, indicating that the model explains about 60% of the variation in mpg.\n\nIn this section, we demonstrated how to fit a simple linear regression model using the fit() function in tidymodels. We also explored how to extract and interpret the model coefficients, make predictions, and evaluate model performance using metrics like RMSE and R-squared. By abstracting the fitting process into a workflow, tidymodels makes it easy to manage and evaluate models in a structured and reproducible way. In the next section, we will delve into model diagnostics to assess how well the assumptions of linear regression are met in this fitted model."
  },
  {
    "objectID": "09_Tidymodels.html#model-diagnostics",
    "href": "09_Tidymodels.html#model-diagnostics",
    "title": "9  Simple Linear Regression with Tidymodels",
    "section": "9.4 Model Diagnostics",
    "text": "9.4 Model Diagnostics\nAfter fitting a linear regression model, it’s essential to check the underlying assumptions to ensure that the model is appropriate for the data and that the results can be trusted. These assumptions include linearity, constant variance (homoscedasticity) of the residuals, independence of errors, and normally distributed errors. Violations of these assumptions can lead to biased estimates or misleading inference. In this section, we will focus on diagnosing these assumptions using residual plots and other techniques, leveraging the tidymodels framework.\n\n9.4.1 Residuals and Fitted Values\nTo start our diagnostics, we will generate the residuals and fitted values using the augment() function from the broom package, which adds additional information such as residuals and fitted values to the original dataset. To use augment we must first extract the fitted model from the overall fitted workflow with the extract_fit_engine function.\n\n# Augment the model with residuals and fitted values\npredictions = extract_fit_engine(lm_fit) |&gt; \n  augment()\n\n# View the first few rows of augmented data\nhead(predictions)\n\n# A tibble: 6 × 8\n    ..y    hp .fitted .resid   .hat .sigma  .cooksd .std.resid\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1  21     110    22.6 -1.59  0.0405   3.92 0.00374      -0.421\n2  21     110    22.6 -1.59  0.0405   3.92 0.00374      -0.421\n3  22.8    93    23.8 -0.954 0.0510   3.92 0.00173      -0.253\n4  21.4   110    22.6 -1.19  0.0405   3.92 0.00210      -0.315\n5  18.7   175    18.2  0.541 0.0368   3.93 0.000389      0.143\n6  18.1   105    22.9 -4.83  0.0432   3.82 0.0369       -1.28 \n\n\n\n\n9.4.2 Plotting Residuals vs. Fitted Values\nA key diagnostic plot for linear regression is the residuals versus fitted values plot. This plot helps assess the assumption of linearity and constant variance (homoscedasticity). In a well-behaved model, the residuals should exhibit no clear pattern and should have constant spread across all levels of the fitted values.\nLet’s create the residuals vs. fitted values plot:\n\n# Plot residuals vs. fitted values\nggplot(predictions, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs. Fitted Values\") +\n  theme_minimal()\n\n\n\n\nIn this plot:\n\nThe x-axis represents the fitted values (predicted mpg from the model).\nThe y-axis represents the residuals (differences between observed and predicted mpg).\nThe dashed red line at zero represents where the residuals would lie if the model were perfect.\n\n\n\n9.4.3 Checking Homoscedasticity (Constant Variance)\nAnother assumption of linear regression is that the residuals should have constant variance, also known as homoscedasticity. If the spread of residuals increases or decreases with the fitted values, the assumption of constant variance is violated, which can lead to inefficient estimates and biased standard errors.\nIn the residuals vs. fitted values plot, this would manifest as a “funnel” shape, where the residuals get larger (or smaller) as the fitted values increase. If you observe this pattern, it suggests heteroscedasticity, and you may need to consider a transformation of the response variable or use robust standard errors to account for the non-constant variance.\n\n\n9.4.4 Normality of Residuals\nLinear regression assumes that the residuals are normally distributed. A common way to check this assumption is to create a Q-Q (quantile-quantile) plot, which compares the quantiles of the residuals to the quantiles of a normal distribution. If the residuals are normally distributed, the points should fall roughly along a 45-degree line.\nLet’s create a Q-Q plot using ggplot2:\n\n# Q-Q plot of residuals\nggplot(predictions, aes(sample = .resid)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal()\n\n\n\n\nIn the Q-Q plot:\n\nIf the residuals are normally distributed, the points will fall along the 45-degree reference line.\nDeviations from this line, especially at the tails, suggest that the residuals may not follow a normal distribution.\n\nIf the normality assumption is violated (e.g., heavy tails or skewness), it may be necessary to apply a transformation to the response variable or explore alternative models that do not assume normality, such as generalized linear models.\n\n\n9.4.5 Checking for Independence of Errors\nAnother important assumption is that the errors (or residuals) should be independent of one another. This is especially relevant in time series data, where errors may be autocorrelated (i.e., residuals at one time point may be related to residuals at nearby time points). We can used the residuals in an ACF plot to determine if there is significant autocorrelation at a lag.\n\nlibrary(forecast)\n\nggAcf(predictions$.resid)\n\n\n\n\n\n\n9.4.6 Detecting Outliers\nTo identify potential outliers, you can plot the standardized residuals, which are residuals divided by their estimated standard deviation. Residuals with an absolute value greater than 2 or 3 are often considered potential outliers.\n\n# Plot standardized residuals\nggplot(predictions, aes(x = .fitted, y = .std.resid)) +\n  geom_point() +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Fitted Values\", y = \"Standardized Residuals\", title = \"Standardized Residuals vs. Fitted Values\") +\n  theme_minimal()\n\n\n\n\nIn this plot:\n\nPoints outside the dashed lines (at standardized residuals of -2 and 2) are potential outliers. These points should be investigated further to understand why they deviate from the model’s predictions."
  },
  {
    "objectID": "09_Tidymodels.html#summary",
    "href": "09_Tidymodels.html#summary",
    "title": "9  Simple Linear Regression with Tidymodels",
    "section": "9.5 Summary",
    "text": "9.5 Summary\nIn this chapter, we demonstrated how to use the tidymodels framework to fit a simple linear regression model. We covered the key steps, including specifying the model with parsnip, building a workflow, and fitting the model. Additionally, we explored how to extract model results and perform basic diagnostics.\nThe tidymodels ecosystem offers a flexible and organized approach to model building, and in future chapters, we will expand this framework to handle more complex regression models and advanced techniques such as cross-validation and model tuning."
  },
  {
    "objectID": "10_Intro_Multiple.html#types-of-models",
    "href": "10_Intro_Multiple.html#types-of-models",
    "title": "10  An Intro to Multiple Regression",
    "section": "10.1 Types of Models",
    "text": "10.1 Types of Models\nWhen we discuss the simple linear regression model \\[\n\\begin{align*}\n    y = \\beta_0 + \\beta_1 x +\\varepsilon\n\\end{align*}\n\\] stated previously that it is “simple” because there is only one predictor variable.\nThe model is “linear in the parameters” because every parameter is only to the first power and is not multiplied or divided by another parameter.\nThe model is also “linear in the predictor variable” because \\(x\\) appears only with an exponent of one (instead of \\(x^2\\), \\(x^{1/2}\\), etc.).\nA “linear model” means it is linear in the parameters (not necessarily linear in the predictor variables). A model that is linear in both the parameters and the predictor variables is called a first-order model."
  },
  {
    "objectID": "10_Intro_Multiple.html#multiple-predictor-variables",
    "href": "10_Intro_Multiple.html#multiple-predictor-variables",
    "title": "10  An Intro to Multiple Regression",
    "section": "10.2 Multiple Predictor Variables",
    "text": "10.2 Multiple Predictor Variables\nOften when modeling some response variable \\(y\\), one predictor variable may not be adequate. Thus, more multiple predictor variable can be used to model \\(y\\).\nAs in simple linear regression, we will assume models that are linear in the parameters.\nWe will present the multiple linear regression model that is linear in the parameters but not necessarily linear in the predictor variables. This type of model is called the general linear regression model.\n\n10.2.1 The Multiple Regression Model\nThe general model is \\[\n\\begin{align}\ny_{i}= & \\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\cdots+\\beta_{p-1}x_{i,p-1}+\\varepsilon_{i}\\\\\n= & \\beta_{0}+\\sum_{k=1}^{p-1}\\beta_{k}x_{ik}+\\varepsilon_{i}\\\\\n& \\varepsilon\\overset{iid}{\\sim}N\\left(0,\\sigma^{2}\\right)\n\\end{align}\n\\tag{10.1}\\] where \\[\n\\begin{align*}\n& \\beta_{0},\\beta_{1},\\ldots,\\beta_{p-1}\\text{ are parameters}\\\\\n& x_{i1},\\ldots,x_{i,p-1}\\text{ are known constants}\\\\\n& i=1,\\ldots,n\n\\end{align*}\n\\]\nThe predictor variable \\(x_{k}\\) can be raised to some power or transformed in some other way. Also, the predictor variable can be the product of two variables. When this is the case, we say the term is an interaction term. We will discuss these more later.\n\n\n10.2.2 Assumptions About the Predictor Variables\nIn multiple regression, we are interested in how the predictor variables relate to the response variable. In particular, we want to know:\n\nHow important are the difference predictor variables in modeling \\(y\\)?\nWhat is the effect of a given predictor variable on predicting \\(y\\)?\nAre any of the predictor variables unnecessary in modeling \\(y\\) and therefore be dropped from the model?\nAre there any predictor variables not included in the model that should be included?\n\nTheses questions are relatively simple to answer if the predictor variables are uncorrelated among themselves.\nUnfortunately, in real world application, especially for observational studies, the predictor variables tend to be correlated among themselves. When this is the case, we say that multicollinearity exists.\nJust due to chance, there will always be some correlation among the predictor variables. In general, we will assume the correlation among the predictor variables is low. If the correlation is high, then this may present problems in the analyses. As we discuss the inferences and assumptions going further, we will discuss the problem of multicollinearity in more detail."
  },
  {
    "objectID": "10_Intro_Multiple.html#estimating-the-multiple-regression-model",
    "href": "10_Intro_Multiple.html#estimating-the-multiple-regression-model",
    "title": "10  An Intro to Multiple Regression",
    "section": "10.3 Estimating the Multiple Regression Model",
    "text": "10.3 Estimating the Multiple Regression Model\n\n10.3.1 Minimizing the SSE\nAs we did in the simple linear regression case, we want to fit model Equation 10.1 to the observed data.\nThe fitted line in the multiple regression case is \\[\n\\begin{align}\n\\hat{y}_i = b_0 + b_1 x_{i1} + b_2 x_{i2}+\\cdots +b_{p-1}x_{i,p-1}\n\\end{align}\n\\tag{10.2}\\] The estimates \\(b_0,b_1,\\ldots,b_{p-1}\\) are found by minimizing the squared distances between the observed values \\(y_i\\) and the fitted values \\(\\hat{y}_i\\). The sum of the squared distances is now \\[\n\\begin{align}\nQ=\\sum \\left(y_i-\\left(b_0 + b_1 x_{i1} + b_2 x_{i2}+\\cdots +b_{p-1}x_{i,p-1}\\right)\\right)^2\\\n\\end{align}\n\\tag{10.3}\\] in the multiple regression case.\nNote that model Equation 10.1 is no longer a line. It is a plane when \\(p=3\\) and a hyperplane when \\(p&gt;3\\).\n\n\n10.3.2 Case with Two Predictor Variables\nWhen there are two predictor variables (\\(p=3\\)), we will take three partial derivatives of Equation 10.3 with respect to \\(b_0\\), \\(b_1\\), and \\(b_2\\). This leads us to \\[\n\\begin{align}\n\\frac{\\partial Q}{\\partial b_{0}} & =-2\\sum\\left(y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{2i}\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{1}} & =-2\\sum x_{i1}\\left(y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{2i}\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{2}} & =-2\\sum x_{i2}\\left(y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{2i}\\right)\n\\end{align}\n\\tag{10.4}\\]\n\n\n10.3.3 The Normal Equations\nSetting the partial derivative equal to zero and rearranging the terms lead us to the normal equations \\[\n\\begin{align}\n\\sum y_{i} & =nb_{0}\\sum x_{i1}+b_{2}\\sum x_{i2}\\\\\n\\sum x_{i1}y_{i} & =b_{0}\\sum x_{i1}+b_{1}\\sum x_{i1}^{2}+b_{2}\\sum x_{i1}x_{i2}\\\\\n\\sum x_{i2}y_{i} & =b_{0}\\sum x_{i2}+b_{1}\\sum x_{i1}x_{i2}+b_{2}\\sum x_{i2}^{2}\n\\end{align}\n\\tag{10.5}\\]\n\n\n10.3.4 The Least Squares Estimators\nSolving the normal equations for \\(b_0\\), \\(b_1\\), and \\(b_2\\) gives use the least squares estimators \\[\n\\begin{align}\nb_{1} & =\\frac{\\left(\\sum x_{i2}^{2}\\right)\\left(\\sum x_{i1}y_{i}\\right)-\\left(\\sum x_{i1}x_{i2}\\right)\\left(\\sum x_{i2}y_{i}\\right)}{\\left(\\sum x_{i1}^{2}\\right)\\left(\\sum x_{i2}^{2}\\right)-\\left(\\sum x_{i1}x_{i2}\\right)^{2}}\\\\\nb_{2} & =\\frac{\\left(\\sum x_{i1}^{2}\\right)\\left(\\sum x_{i2}y_{i}\\right)-\\left(\\sum x_{i1}x_{i2}\\right)\\left(\\sum x_{i1}y_{i}\\right)}{\\left(\\sum x_{i1}^{2}\\right)\\left(\\sum x_{i2}^{2}\\right)-\\left(\\sum x_{i1}x_{i2}\\right)^{2}}\\\\\nb_{0} & =\\bar{Y}-b_{1}\\bar{X}_{1}-b_{2}\\bar{X}_{2}\n\\end{align}\n\\tag{10.6}\\]\nWe see that the expression for the least squares estimators become cumbersome even for \\(p=3\\). As more variables are added to the model, the equations become even more cumbersome.\nWe can simplify notation by utilizing matrices to represent the model. We will present some basic notation and operations for matrices and then present the model using matrices."
  },
  {
    "objectID": "10_Intro_Multiple.html#a-primer-on-matrices",
    "href": "10_Intro_Multiple.html#a-primer-on-matrices",
    "title": "10  An Intro to Multiple Regression",
    "section": "10.4 A Primer on Matrices",
    "text": "10.4 A Primer on Matrices\n\n10.4.1 Matrices\nA matrix is a rectangular array of elements arranged in rows and columns.\nAn example of a matrix is: \\[\\begin{align*}\n\\left[\\begin{array}{ccc}\n8.3 & 70 & 10.3\\\\\n8.6 & 65 & 10.3\\\\\n8.8 & 63 & 10.2\\\\\n10.5 & 72 & 16.4\\\\\n\\end{array}\\right]\n\\end{align*}\\]\nThis matrix represents some of the data from the dataset. The values in the first column represents Girth, the second column represents Height, and the third column represents Volume.\nEach row corresponds to a tree. The first row represents the values for the first tree. It has 8.3 for Girth, 70 for Height, and 10.3 for Volume.\nSo this matrix gives the values of three variables for four trees.\n\n\n10.4.2 Notation\nEach value of the matrix is called an element of that matrix. We denote the elements as \\(a_{ij}\\) for the element in the \\(i\\)th row and the \\(j\\)th column. Note that the first subscript identifies the row number and the second the column number.\nSo for the matrix above, the elements can be denotes as \\[\n\\begin{align*}\n\\left[\\begin{array}{ccc}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\\\\\na_{41} & a_{42} & a_{43}\n\\end{array}\\right]\n\\end{align*}\n\\]\nA matrix may be denoted by a symbol such as \\(\\bf{A}\\), \\(\\bf{X}\\), or \\(\\bf{Z}\\). The matrix could also be a greek symbol such as \\(\\bf{\\Omega}\\). The symbol is in boldface to identify that it refers to a matrix.\nThus, we might define for the above matrix; \\[\n\\begin{align*}\n\\bf{A} =\\left[\\begin{array}{ccc}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\\\\\na_{41} & a_{42} & a_{43}\n\\end{array}\\right]\n\\end{align*}\n\\]\nAnother notation we could use is: \\[\n\\textbf{A}=\\left[a_{ij}\\right]\\qquad i=1,\\ldots,4; j=1,2,3\n\\]\nThis notation avoids the need for writing out all elements of the matrix by stating only the general element.\nSometimes we will specify the matrix with the dimension below the matrix symbol. For example, a \\(r\\) x \\(c\\) matrix can be expressed as\n\n\n10.4.3 Matrix Dimensions\nThe dimension of the matrix above is 4 x 3, since there are four rows and three columns.\nRecall that the trees dataset has 31 observations. So a matrix representing the full dataset would be 31 x 3.\nNote that in giving the dimension of a matrix, we always specify the number of rows first and then the number of columns.\nSo a \\(r\\) x \\(c\\) matrix can be expressed as \\[\n\\begin{align*}\n\\underset{r\\times c}{{\\bf A}} & =\\left[\\begin{array}{cccc}\na_{11} & a_{12} & \\cdots & a_{1c}\\\\\na_{21} & a_{22} & \\cdots & a_{2c}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\na_{r1} & a_{r2} & \\cdots & a_{rc}\n\\end{array}\\right]\n\\end{align*}\n\\] or in the compact form \\[\n\\begin{align*}\n\\underset{r\\times c}{{\\bf A}} & =\\left[a_{ij}\\right]\\qquad i=1,\\ldots,r;j=1,\\ldots,c\n\\end{align*}\n\\] Again, the dimensions may or may not be given under the matrix symbol.\n\n\n10.4.4 Square Matrices\nA matrix is said to be square if the number of rows equals the number of columns. For example, the matrices \\[\n\\begin{align*}\n\\left[\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21} & a_{22}\n\\end{array}\\right]\n\\end{align*}\n\\] and\n\\[\n\\begin{align*}\n\\left[\\begin{array}{ccc}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\\\\\n\\end{array}\\right]\n\\end{align*}\n\\] are both square matrices.\n\n\n10.4.5 Vectors\nA matrix containing only one column is called a column vector or simply a vector.\nTwo examples are: \\[\n\\begin{align*}\n\\textbf{A}=\\left[\\begin{array}{c}\n1\\\\\n20\\\\\n7\n\\end{array}\\right] & \\qquad\\textbf{B}=\\left[\\begin{array}{c}\nb_{1}\\\\\nb_{2}\\\\\nb_{3}\\\\\nb_{4}\\\\\nb_{5}\n\\end{array}\\right]\n\\end{align*}\n\\]\nNote that the elements only have one subscript in \\(\\bf{B}\\) since there is only one column. The subscript indicates only the row.\nA matrix containing only one row is called a row vector.\nTwo examples are: \\[\n\\begin{align*}\n\\textbf{B}^{\\prime}=\\left[\\begin{array}{ccc}\n15 & 25 & 50\\end{array}\\right] & \\qquad\\boldsymbol{\\delta}^{\\prime}=\\left[\\begin{array}{cc}\n\\delta_{1} & \\delta_{2}\\end{array}\\right]\n\\end{align*}\n\\]\nWe use the prime (\\({}^\\prime\\)) symbol for row vectors for reasons to be seen next.\n\n\n10.4.6 Transpose\nThe transpose of a matrix \\(\\bf{A}\\) is another matrix, denoted by \\(\\textbf{A}^{\\prime}\\), that is obtained by interchanging corresponding columns and rows of the matrix \\(\\bf{A}\\).\nFor example, if: \\[\n\\begin{align*}\n\\underset{3\\times2}{\\textbf{A}}=\\left[\\begin{array}{cc}\n1 & 7\\\\\n12 & 4\\\\\n5 & 9\n\\end{array}\\right]\n\\end{align*}\n\\] then the transpose \\(\\bf{A}^\\prime\\) is: \\[\n\\begin{align*}\n\\underset{2\\times3}{\\textbf{A}^{\\prime}}=\\left[\\begin{array}{ccc}\n1 & 12 & 5\\\\\n7 & 4 & 9\n\\end{array}\\right]\n\\end{align*}\n\\]\nNote that the first column of \\(\\bf{A}\\) is the first row of \\(\\bf{A}^\\prime\\), and similarly the second column of \\(\\bf{A}\\) is the second row of \\(\\bf{A}^\\prime\\).\nNote that the dimension of \\(\\bf{A}\\) becomes reversed for the dimension of \\(\\bf{A}^\\prime\\).\nNote that the transpose of a column vector is a row vector, and vice versa.\nThis is the reason why we used the symbol \\(\\bf{B}^\\prime\\) earlier to identify a row vector, since it may be thought of as the transpose of a column vector \\(\\bf{B}\\).\n\n\n10.4.7 Symmetric Matrices\nA matrix is said to be symmetric if \\(\\bf{A}=\\bf{A}^\\prime\\).\nA symmetric matrix \\(\\bf{A}\\) has elements \\(a_{ij}=a_{ji}\\). Clearly, a symmetric matrix must be a square matrix.\n\n\n10.4.8 Diagonal Matrices}\nA square matrix is said to be diagonal if all of the off-diagonal elements are zero.\nFor example \\[\n\\begin{align*}\n{\\bf A} & =\\left[\\begin{array}{cccc}\na_{11} & 0 & 0 & 0\\\\\n0 & a_{22} & 0 & 0\\\\\n0 & 0 & a_{33} & 0\\\\\n0 & 0 & 0 & a_{44}\n\\end{array}\\right]\n\\end{align*}\n\\] is a diagonal matrix.\n\n\n10.4.9 Indentity Matrix\nThe identity matrix is a diagonal matrix with ones for all the diagonal elements. The identity matrix is denoted with with \\(\\bf{I}\\).\nFor Example \\[\n\\begin{align*}\n{\\bf I} & =\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n\\end{align*}\n\\] is a 4 x 4 identity matrix.\n\n\n10.4.10 Matrices and Vectors of Ones and Zeros\nA matrix of with ones for all the elements is denoted as \\[\n\\begin{align*}\n{\\bf J} & =\\left[\\begin{array}{cccc}\n1 & 1 & \\cdots & 1\\\\\n1 & 1 & \\cdots & 1\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & 1 & \\cdots & 1\n\\end{array}\\right]\n\\end{align*}\n\\]\nA vector with ones for all the elements is denoted as \\[\n\\begin{align*}\n{\\bf 1} & =\\left[\\begin{array}{c}\n1\\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{array}\\right]\n\\end{align*}\n\\]\nLikewise a vector of zeros is denoted as \\[\n\\begin{align*}\n{\\bf 0} & =\\left[\\begin{array}{c}\n0\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\n10.4.11 Matrix Addition and Subtraction\nAdding or subtracting two matrices requires that they have the same dimension.\nThe sum, or difference, of two matrices is another matrix whose elements each consist of the sum, or difference, of the corresponding elements of the two matrices.\nSuppose: \\[\n\\begin{align*}\n\\underset{3\\times2}{\\textbf{A}}=\\left[\\begin{array}{cc}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{array}\\right] & \\qquad\\underset{3\\times2}{\\textbf{B}}=\\left[\\begin{array}{cc}\n1 & 2\\\\\n2 & 3\\\\\n3 & 4\n\\end{array}\\right]\n\\end{align*}\n\\] then \\[\n\\begin{align*}\n\\underset{3\\times2}{\\textbf{A}+\\textbf{B}=} & \\left[\\begin{array}{cc}\n1+1 & 4+2\\\\\n2+2 & 5+3\\\\\n3+3 & 6+4\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & 6\\\\\n4 & 8\\\\\n6 & 10\n\\end{array}\\right]\n\\end{align*}\n\\]\nSimilarly: \\[\n\\begin{align*}\n\\underset{3\\times2}{\\textbf{A}-\\textbf{B}=} & \\left[\\begin{array}{cc}\n1-1 & 4-2\\\\\n2-2 & 5-3\\\\\n3-3 & 6-4\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n0 & 2\\\\\n0 & 2\\\\\n0 & 2\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\n10.4.12 Matrix Multiplication\nThe addition and subtraction rules discussed above are fairly straight forward and similar to addition and subtraction of (non-matrix) numbers.\nMultiplication of matrices are not as straight forward as multiplication of (non-matrix) numbers.\n\nMultiplication of a Matrix by a Scalar\nA scalar is an ordinary number or a symbol representing a number.\nIn multiplication of a matrix by a scalar, every element of the matrix is multiplied by the scalar.\nFor example, suppose the matrix \\(\\textbf{A}\\) is given by \\[\n\\begin{align*}\n\\textbf{A}=\\left[\\begin{array}{cc}\n1 & 3\\\\\n5 & 7\n\\end{array}\\right]\n\\end{align*}\n\\]\nThen \\(2\\textbf{A}\\), where 2 is the scalar, equals \\[\n\\begin{align*}\n2\\textbf{A}=2\\left[\\begin{array}{cc}\n1 & 3\\\\\n5 & 7\n\\end{array}\\right] & =\\left[\\begin{array}{cc}\n2 & 6\\\\\n10 & 14\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nMultiplication of a Matrix by a Matrix\nConsider the two matrices: \\[\n\\begin{align*}\n\\underset{2\\times2}{\\textbf{A}}=\\left[\\begin{array}{cc}\n1 & 2\\\\\n3 & 4\n\\end{array}\\right] & \\qquad\\underset{2\\times2}{\\textbf{B}}=\\left[\\begin{array}{cc}\n5 & 6\\\\\n7 & 8\n\\end{array}\\right]\n\\end{align*}\n\\]\nMultiplying \\(\\bf{A}\\) by \\(\\bf{B}\\) is found by a multiplying the elements of each row vector by the elements of each each column vector and then summing the products.\nFor example, to find the element in the first row and the first column of the product \\(\\textbf{AB}\\), we work with the first row of \\(\\textbf{A}\\) and the first column of \\(\\textbf{B}\\): \\[\n\\begin{align*}\n\\begin{array}{cc}\n& \\textbf{A}\\\\\n& \\left[\\begin{array}{cc}\n{\\color{red}1} & {\\color{red}2}\\\\\n3 & 4\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\\begin{array}{c}\n\\textbf{B}\\\\\n\\left[\\begin{array}{cc}\n{\\color{red}5} & 6\\\\\n{\\color{red}7} & 8\n\\end{array}\\right]\\\\\n\\begin{array}{cc}\n& \\end{array}\n\\end{array} & =\\begin{array}{cc}\n& \\textbf{AB}\\\\\n&  \\left[\\begin{array}{cc}\n\\color{red}{\\left(1\\right)\\left(5\\right)+\\left(2\\right)\\left(7\\right)} &\\\\\n\\\\\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\\\\\n& = \\begin{array}{cc}\n& \\textbf{AB}\\\\\n&  \\left[\\begin{array}{cc}\n\\color{red}{19} &\\\\\n\\\\\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\n\\end{align*}\n\\]\nTo find the element in the first row and second column of \\(\\textbf{AB}\\): \\[\n\\begin{align*}\n\\begin{array}{cc}\n& \\textbf{A}\\\\\n& \\left[\\begin{array}{cc}\n{\\color{red}1} & {\\color{red}2}\\\\\n3 & 4\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\\begin{array}{c}\n\\textbf{B}\\\\\n\\left[\\begin{array}{cc}\n5 & \\color{red}{6}\\\\\n7 & \\color{red}{8}\n\\end{array}\\right]\\\\\n\\begin{array}{cc}\n& \\end{array}\n\\end{array} & =\\begin{array}{cc}\n& \\textbf{AB}\\\\\n&  \\left[\\begin{array}{cc}\n33&\n\\color{red}{\\left(1\\right)\\left(6\\right)+\\left(2\\right)\\left(8\\right)} \\\\\n\\\\\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\\\\\n& = \\begin{array}{cc}\n& \\textbf{AB}\\\\\n&  \\left[\\begin{array}{cc}\n33 & \\color{red}{22}\\\\\n\\\\\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\n\\end{align*}\n\\]\nContinuing this process we get \\[\n\\begin{align*}\n\\underset{2\\times2}{\\textbf{AB}} & =\\left[\\begin{array}{cc}\n\\left(1\\right)\\left(5\\right)+\\left(2\\right)\\left(7\\right) & \\left(1\\right)\\left(6\\right)+\\left(2\\right)\\left(8\\right)\\\\\n\\left(3\\right)\\left(5\\right)+\\left(4\\right)\\left(7\\right) & \\left(3\\right)\\left(6\\right)+\\left(4\\right)\\left(8\\right)\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n19 & 22\\\\\n43 & 50\n\\end{array}\\right]\n\\end{align*}\n\\]\nNote that the order in matrix multiplication is important. In general, \\(\\textbf{AB} \\ne \\textbf{BA}\\). In fact, even though the product \\(\\textbf{AB}\\) may be defined, the product \\(\\textbf{BA}\\) may not be defined at all.\nIn general, the product \\(\\textbf{AB}\\) is defined only when the number of columns in \\(\\textbf{A}\\) equals the number of rows in \\(\\textbf{B}\\).\nFor example: \\[\n\\begin{align*}\n\\underset{{\\color{red}2}\\times{\\color{blue}3}}{\\textbf{A}} & \\quad\\underset{{\\color{blue}3}\\times{\\color{red}1}}{\\textbf{B}}=\\underset{{\\color{red}2}\\times{\\color{red}1}}{\\textbf{AB}}\n\\end{align*}\n\\] is defined since the number of columns of \\(\\textbf{A}\\) (3) is equal to the number of rows of \\(\\textbf{B}\\) (3).\nHowever, note that \\[\n\\begin{align*}\n\\underset{{\\color{blue}3}\\times{\\color{red}1}}{\\textbf{B}}\\quad\\underset{{\\color{red}2}\\times{\\color{blue}3}}{\\textbf{A}}\n\\end{align*}\n\\] is not defined since the number of columns of \\(\\textbf{B}\\) (1) is not equal to the number of rows of \\(\\textbf{A}\\) (2).\nWhen obtaining the product \\(\\textbf{AB}\\), we say that \\(\\textbf{A}\\) is postmultiplied by \\(\\textbf{B}\\) or \\(\\textbf{B}\\) is premultiplied by \\(\\textbf{A}\\).\n\n\nInverse of a Matrix\nFor ordinary (non-matrix) numbers, the inverse of a number is its reciprocal. Thus, the inverse of 2 is \\(\\frac{1}{2}\\)\nA number multiplied by its inverse always equals 1: \\[\n\\begin{align*}\n&2\\cdot\\frac{1}{2}=\\frac{1}{2}\\cdot2=1\n\\end{align*}\n\\]\nIn matrix algebra, the inverse of a matrix \\(\\textbf{A}\\) is another matrix, denoted by \\(\\textbf{A}^{-1}\\), such that: \\[\n\\textbf{A}^{-1}\\textbf{A}=\\textbf{A}\\textbf{A}^{-1}=\\textbf{I}\n\\] where \\(\\textbf{I}\\) is the identity matrix.\nThus, the identity matrix \\(\\textbf{I}\\) plays the same role as the number 1 in ordinary algebra.\nAn inverse of a matrix is defined only for square matrices.\nEven so, many square matrices do not have inverses.\nIf a square matrix does have an inverse, the inverse is unique.\nIf a the inverse of a matrix does not exist, then we say the matrix is singular. If the inverse does exist, then we say the matrix is nonsingular.\n\n\n\n10.4.13 Basic Matrix Results\nBelow are some basic results for matrices presented without proof. They will be useful as we use matrices in regression. \\[\n\\begin{align}\n\\textbf{A}+\\textbf{B} & =\\textbf{B}+\\textbf{A} &\\\\\n\\left(\\textbf{A}+\\textbf{B}\\right)+\\textbf{C} & =\\textbf{A}+\\left(\\textbf{B}+\\textbf{C}\\right) &\\\\\n\\left(\\textbf{A}\\textbf{B}\\right)\\textbf{C} & =\\textbf{A}\\left(\\textbf{B}\\textbf{C}\\right)&\\\\\n\\textbf{C}\\left(\\textbf{A}+\\textbf{B}\\right) & =\\textbf{C}\\textbf{A}+\\textbf{C}\\textbf{B}&\\\\\nk\\left(\\textbf{A}+\\textbf{B}\\right) & =k\\textbf{A}+k\\textbf{B}&\\\\\n\\left(\\textbf{A}^{\\prime}\\right)^{\\prime} & =\\textbf{A}&\\\\\n\\left(\\textbf{A}+\\textbf{B}\\right)^{\\prime} & =\\textbf{A}^{\\prime}+\\textbf{B}^{\\prime}&\\\\\n\\left(\\textbf{A}\\textbf{B}\\right)^{\\prime} & =\\textbf{B}^{\\prime}\\textbf{A}^{\\prime}&\\\\\n\\left(\\textbf{A}\\textbf{B}\\textbf{C}\\right)^{\\prime} & =\\textbf{C}^{\\prime}\\textbf{B}^{\\prime}\\textbf{A}^{\\prime}&\\\\\n\\left(\\textbf{A}^{-1}\\right)^{-1} & =\\textbf{A}&\\\\\n\\left(\\textbf{A}^{\\prime}\\right)^{-1} & =\\left(\\textbf{A}^{-1}\\right)^{\\prime}&\n\\end{align}\n\\]\n\n\n10.4.14 Matrix Differentiation\nThere are a number of results when using matrix calculus which are beyond the scope of this course. We will present a few results for matrix differentiation that will be useful in multiple regression.\nIt is important to note that matrix calculus can be confusing due to notational conventions that are used in various fields. There are two main conventions (although the two are sometimes mixed by some authors) that are based how to take a derivative with respect to a vector. One convention is the numerator layout and the other is the denominator layout. Below, we will present the results using numerator layout.\nIn all the results that follow, let \\(d\\) be a scalar, \\({\\bf A}\\) be a \\(n\\times1\\) vector with elements \\([a_{i}]\\), \\({\\bf B}\\) be a \\(m\\times1\\) vector with elements \\([b_{i}]\\), and \\({\\bf C}\\) be a \\(p\\times q\\) matrix with elements \\([c_{ij}]\\).\n\nVector by a Scalar\n\\[\n\\begin{align*}\n\\frac{\\partial{\\bf A}}{\\partial d} & =\\left[\\begin{array}{c}\n\\frac{\\partial a_{1}}{\\partial d}\\\\\n\\frac{\\partial a_{2}}{\\partial d}\\\\\n\\vdots\\\\\n\\frac{\\partial a_{n}}{\\partial d}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nScalar by a Vector\n\\[\n\\begin{align*}\n\\frac{\\partial d}{\\partial{\\bf A}}  =\\left[\\begin{array}{cccc}\n\\frac{\\partial d}{\\partial a_{1}} & \\frac{\\partial d}{\\partial a_{2}} & \\cdots & \\frac{\\partial d}{\\partial a_{n}}\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nVector by a Vector\n\\[\n\\begin{align*}\n\\frac{\\partial{\\bf A}}{\\partial{\\bf B}} & =\\left[\\begin{array}{cccc}\n\\frac{\\partial a_{1}}{\\partial b_{1}} & \\frac{\\partial a_{1}}{\\partial b_{2}} & \\cdots & \\frac{\\partial a_{1}}{\\partial b_{m}}\\\\\n\\frac{\\partial a_{2}}{\\partial b_{1}} & \\frac{\\partial a_{2}}{\\partial b_{2}} & \\cdots & \\frac{\\partial a_{2}}{\\partial b_{m}}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial a_{n}}{\\partial b_{1}} & \\frac{\\partial a_{n}}{\\partial b_{2}} & \\cdots & \\frac{\\partial a_{n}}{\\partial b_{m}}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nMatrix by a Scalar\n\\[\n\\begin{align*}\n\\frac{\\partial{\\bf C}}{\\partial d} & =\\left[\\begin{array}{cccc}\n\\frac{\\partial c_{11}}{\\partial d} & \\frac{\\partial c_{12}}{\\partial d} & \\cdots & \\frac{\\partial c_{1q}}{\\partial d}\\\\\n\\frac{\\partial c_{21}}{\\partial d} & \\frac{\\partial c_{22}}{\\partial d} & \\cdots & \\frac{\\partial c_{2q}}{\\partial d}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial c_{p1}}{\\partial d} & \\frac{\\partial c_{p2}}{\\partial d} & \\cdots & \\frac{\\partial c_{pq}}{\\partial d}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nScalar by a Matrix\n\\[\n\\begin{align*}\n\\frac{\\partial d}{\\partial{\\bf C}} & =\\left[\\begin{array}{cccc}\n\\frac{\\partial d}{\\partial c_{11}} & \\frac{\\partial d}{\\partial c_{21}} & \\cdots & \\frac{\\partial d}{\\partial c_{p1}}\\\\\n\\frac{\\partial d}{\\partial c_{12}} & \\frac{\\partial d}{\\partial c_{22}} & \\cdots & \\frac{\\partial d}{\\partial c_{p2}}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial d}{\\partial c_{1q}} & \\frac{\\partial d}{\\partial c_{2q}} & \\cdots & \\frac{\\partial d}{\\partial c_{pq}}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nCommon Derivatives Involving Matrices\n\\[4\n\\begin{align*}\n& \\frac{\\partial{\\bf A}^{\\prime}{\\bf A}}{\\partial{\\bf A}}=2{\\bf A}^{\\prime}\\\\\n& \\frac{\\partial{\\bf A}^{\\prime}{\\bf B}}{\\partial{\\bf B}}=\\frac{\\partial{\\bf B}^{\\prime}{\\bf A}}{\\partial{\\bf B}}={\\bf A}^{\\prime} &  & \\text{(provided }m=n)\\\\\n& \\frac{\\partial{\\bf \\left({\\bf A}^{\\prime}{\\bf B}\\right)^{2}}}{\\partial{\\bf A}}=2{\\bf A}^{\\prime}{\\bf B}{\\bf B}^{\\prime} &  & \\text{(provided }m=n)\\\\\n& \\frac{\\partial{\\bf C}{\\bf A}}{\\partial{\\bf A}}={\\bf C} &  & \\text{(provided }q=n)\\\\\n& \\frac{\\partial{\\bf A}^{\\prime}{\\bf C}}{\\partial{\\bf A}}={\\bf C}^{\\prime} &  & \\text{(provided }p=n)\\\\\n& \\frac{\\partial{\\bf A}^{\\prime}{\\bf C}{\\bf A}}{\\partial{\\bf A}}={\\bf A}^{\\prime}\\left({\\bf C}+{\\bf C}^{\\prime}\\right) &  & \\text{(provided }n=p=q)\n\\end{align*}\n\\]\n\n\n\n10.4.15 Random Matrices\nA random matrix contains elements that are random variables.\nThus, the vector of the response vector \\[\n\\begin{align*}\n{\\bf Y} & =\\left[\\begin{array}{c}\nY_{1}\\\\\nY_{2}\\\\\n\\vdots\\\\\nY_{n}\n\\end{array}\\right]\n\\end{align*}\n\\] is a random vector since the \\(Y_i\\) elements are random variables.\n\nExpected Value\nThe expected value of \\({\\bf Y}\\) is a matrix (or vector) that has elements that are the expected values of the elements of \\({\\bf Y}\\). Thus, \\[\n\\begin{align*}\n{\\bf E}\\left[{\\bf Y}\\right] & =\\left[\\begin{array}{c}\nE\\left[Y_{1}\\right]\\\\\nE\\left[Y_{2}\\right]\\\\\n\\vdots\\\\\nE\\left[Y_{n}\\right]\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nVariance-Covariance Matrix\nWhen working with random vectors, we will be interested in the variance of the individual elements \\[\n\\begin{align*}\nVar\\left[Y_{i}\\right]\n\\end{align*}\n\\] along with the covariance between pairs of elements \\[\n\\begin{align*}\nCov\\left[Y_{i},Y_{j}\\right] & \\text{ }i\\ne j.\n\\end{align*}\n\\]\nAll of these variances and covariances are given in the variance-covariance matrix or simply covariance matrix: \\[\n\\begin{align*}\n{\\bf Cov}\\left[{\\bf Y}\\right] & =\\left[\\begin{array}{cccc}\nVar\\left[Y_{1}\\right] & Cov\\left[Y_{1},Y_{2}\\right] & \\cdots & Cov\\left[Y_{1},Y_{n}\\right]\\\\\nCov\\left[Y_{2},Y_{1}\\right] & Var\\left[Y_{2}\\right] & \\cdots & Cov\\left[Y_{2},Y_{n}\\right]\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nCov\\left[Y_{n},Y_{1}\\right] & Cov\\left[Y_{n},Y_{2}\\right] & \\cdots & Var\\left[Y_{n}\\right]\n\\end{array}\\right]\n\\end{align*}\n\\]\nNote that \\({\\bf Cov}\\left[{\\bf Y}\\right]\\) is a symmetric matrix since \\(Cov\\left[Y_{i},Y_{j}\\right]=Cov\\left[Y_{j},Y_{i}\\right]\\)."
  },
  {
    "objectID": "11_Regression_Matrix.html#the-matrices-for-the-different-components",
    "href": "11_Regression_Matrix.html#the-matrices-for-the-different-components",
    "title": "11  The Regression Model in Matrix Terms",
    "section": "11.1 The Matrices for the Different Components",
    "text": "11.1 The Matrices for the Different Components\nRecall the regression model is \\[\\begin{align*}\ny_{i}= & \\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\cdots+\\beta_{p-1}x_{i,p-1}+\\varepsilon_{i}\\\\\n& \\varepsilon\\overset{iid}{\\sim}N\\left(0,\\sigma^{2}\\right)\n\\end{align*}\\] for \\(i=1,\\ldots,n\\).\nThis implies: \\[\\begin{align*}\ny_{1} & =\\beta_{0}+\\beta_{1}x_{11}+\\beta_{2}x_{12}+\\cdots+\\beta_{p-1}x_{1,p-1}+\\varepsilon_{1}\\\\\ny_{2} & =\\beta_{0}+\\beta_{1}x_{21}+\\beta_{2}x_{22}+\\cdots+\\beta_{p-1}x_{2,p-1}+\\varepsilon_{2}\\\\\n& \\vdots\\\\\ny_{n} & =\\beta_{0}+\\beta_{1}x_{n1}+\\beta_{2}x_{n2}+\\cdots+\\beta_{p-1}x_{n,p-1}+\\varepsilon_{n}\n\\end{align*}\\]\n\n11.1.1 Response Vector\nWe define the response vector as \\[\\begin{align*}\n\\underset{{n\\times1}}{\\textbf{Y}}=\\left[\\begin{array}{c}\ny_{1}\\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n11.1.2 Random Error Vector\nWe define the vector of random errors as \\[\\begin{align*}\n\\underset{{n\\times1}}{\\boldsymbol{\\varepsilon}}=\\left[\\begin{array}{c}\n\\varepsilon_{1}\\\\\n\\varepsilon_{2}\\\\\n\\vdots\\\\\n\\varepsilon_{n}\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n11.1.3 Vector of Coefficients\nWe define the vector of Coefficients as \\[\\begin{align*}\n\\underset{{p\\times1}}{\\boldsymbol{\\beta}}=\\left[\\begin{array}{c}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{p-1}\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n11.1.4 The Design Matrix\nWe define the matrix of the predictor variables as \\[\\begin{align*}\n\\underset{{n\\times p}}{\\textbf{X}}=\\left[\\begin{array}{ccccc}\n1 & x_{11} & x_{12} & \\cdots & x_{1,p-1}\\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2,p-1}\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{n,p-1}\n\\end{array}\\right]\n\\end{align*}\\]\nNote that the first column of \\(\\bf{X}\\) is a vector of ones. This column will represent the coefficient of the \\(y\\)-intercept in the model."
  },
  {
    "objectID": "11_Regression_Matrix.html#the-model",
    "href": "11_Regression_Matrix.html#the-model",
    "title": "11  The Regression Model in Matrix Terms",
    "section": "11.2 The Model",
    "text": "11.2 The Model\nWe can now write the model as\n\\[\\begin{align*}\n\\underset{{n\\times1}}{\\textbf{Y}} & =\\underset{{n\\times p}}{\\textbf{X}}\\underset{{p\\times 1}}{\\boldsymbol{\\beta}}+\\underset{{n\\times1}}{\\boldsymbol{\\varepsilon}}\n\\end{align*}\\] since: \\[\\begin{align*}\n\\left[\\begin{array}{c}\ny_{1}\\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{array}\\right] & =\\left[\\begin{array}{ccccc}\n1 & x_{11} & x_{12} & \\cdots & x_{1,p-1}\\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2,p-1}\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{n,p-1}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{p-1}\n\\end{array}\\right]+\\left[\\begin{array}{c}\n\\varepsilon_{1}\\\\\n\\varepsilon_{2}\\\\\n\\vdots\\\\\n\\varepsilon_{n}\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{c}\n\\beta_{0}+\\beta_{1}x_{11}+\\beta_2x_{12}+\\cdots+\\beta_{p-1}x_{1,p-1}\\\\\n\\beta_{0}+\\beta_{1}x_{21}+\\beta_2x_{22}+\\cdots+\\beta_{p-1}x_{2,p-1}\\\\\n\\vdots\\\\\n\\beta_{0}+\\beta_{1}x_{n1}+\\beta_2x_{n2}+\\cdots+\\beta_{p-1}x_{n,p-1}\n\\end{array}\\right]+\\left[\\begin{array}{c}\n\\varepsilon_{1}\\\\\n\\varepsilon_{2}\\\\\n\\vdots\\\\\n\\varepsilon_{n}\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{c}\n\\beta_{0}+\\beta_{1}x_{11}+\\beta_2x_{12}+\\cdots+\\beta_{p-1}x_{1,p-1}+\\varepsilon_{1}\\\\\n\\beta_{0}+\\beta_{1}x_{21}+\\beta_2x_{22}+\\cdots+\\beta_{p-1}x_{2,p-1}+\\varepsilon_{2}\\\\\n\\vdots\\\\\n\\beta_{0}+\\beta_{1}x_{n1}+\\beta_2x_{n2}+\\cdots+\\beta_{p-1}x_{n,p-1}+\\varepsilon_{n}\n\\end{array}\\right]\n\\end{align*}\\]\n\n11.2.1 Multivariate Normal Distribution\nThe assumption on the normal error model for the random error term is \\[\\begin{align*}\n\\varepsilon\\overset{iid}{\\sim} & N\\left(0,\\sigma^{2}\\right).\n\\end{align*}\\] In matrix notation, this can be expressed with the normal distribution.\nNote that the univariate normal distribution has a probability density function expressed as \\[\\begin{align*}\nf\\left(x\\right) & =\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2\\sigma^{2}}\\left(x-\\mu\\right)^{2}\\right]\n\\end{align*}\\] where \\(\\mu\\) is the mean of the distribution and \\(\\sigma\\) is the standard deviation.\nThe multivariate normal distribution is expressed as \\[\\begin{align*}\nf\\left({\\bf Y}\\right) & =\\frac{1}{\\left(2\\pi\\right)^{n/2}\\left|\\boldsymbol{\\Sigma}\\right|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left({\\bf Y}-\\boldsymbol{\\mu}\\right)^{\\prime}\\boldsymbol{\\Sigma}^{-1}\\left({\\bf Y}-\\boldsymbol{\\mu}\\right)\\right]\n\\end{align*}\\] where \\({\\bf Y}\\) is a \\(n\\times1\\) vector, \\(\\boldsymbol{\\mu}\\) is a \\(n\\times1\\) vector of means, and \\(\\boldsymbol{\\Sigma}\\) is the \\(n\\times n\\) covariance matrix.\nWe denote the multivariate normal distribution of a random vector \\({\\bf Y}\\) as \\[\\begin{align*}\n{\\bf Y} & \\sim N_{n}\\left(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right).\n\\end{align*}\\]\nFor the normal error model, the mean vector of the random vector \\(\\boldsymbol{\\varepsilon}\\) is a vector of zeros (\\({\\bf 0}\\)) and the covariance matrix is \\[\\begin{align*}\n{\\bf Cov}\\left(\\boldsymbol{\\varepsilon}\\right) & =\\left[\\begin{array}{cccc}\n\\sigma^{2} & 0 & \\cdots & 0\\\\\n0 & \\sigma^{2} & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\cdots & \\sigma^{2}\n\\end{array}\\right]\\\\\n& =\\sigma^{2}{\\bf I}.\n\\end{align*}\\]\n\n\n11.2.2 The Model in Matrix Notation\nWe now represent the normal errors multiple regression model as \\[\n\\begin{align}\n{\\textbf{Y}}= & {\\textbf{X}}{\\boldsymbol{\\beta}}+{\\boldsymbol{\\varepsilon}}\\\\\n& \\boldsymbol{\\varepsilon} \\sim N_{n}\\left({\\bf 0},\\sigma^{2}{\\bf I}\\right)\n\\end{align}\n\\tag{11.1}\\]\nNote that \\[\\begin{align*}\n\\textbf{E}\\left(\\textbf{Y}\\right) & =\\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\\]"
  },
  {
    "objectID": "11_Regression_Matrix.html#least-squares-and-inferences-using-matrices",
    "href": "11_Regression_Matrix.html#least-squares-and-inferences-using-matrices",
    "title": "11  The Regression Model in Matrix Terms",
    "section": "11.3 Least Squares and Inferences Using Matrices",
    "text": "11.3 Least Squares and Inferences Using Matrices\n\n11.3.1 Least Squares Estimators\nFor the model in Equation 11.1 we want to minimize the squared distances between the observed \\(y_i\\) and the fitted \\(\\hat{y}_i\\).\nWe will express the sum of the squared distances in Equation 10.3 in matrix terms as \\[\n\\begin{align}\nQ & =\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)^{\\prime}\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)\n\\end{align}\n\\tag{11.2}\\] where \\({\\bf Y}\\) is the response vector, \\({\\bf X}\\) is the design matrix, and \\({\\bf b}\\) is the vector of estimators \\[\\begin{align*}\n{\\bf b} & =\\left[\\begin{array}{c}\nb_{0}\\\\\nb_{1}\\\\\n\\vdots\\\\\nb_{p-1}\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n11.3.2 Minimizing the Sums of Squares\nWe will minimize \\(Q\\) in Equation 11.2 by taking the derivative with respect to \\({\\bf b}\\) and setting it equal to the vector of zeros. The derivative is done using the properties listed in Chapter 10 and using the chain rule: \\[\\begin{align*}\n\\frac{\\partial\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)^{\\prime}\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)}{\\partial{\\bf b}} & =-2{\\bf X}^{\\prime}\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)\n\\end{align*}\\]\nSetting this derivative equal to the vector of zeros gives us \\[\\begin{align*}\n-2{\\bf X}^{\\prime}\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)={\\bf 0} & \\Longrightarrow{\\bf X}^{\\prime}\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)={\\bf 0}\n\\end{align*}\\]\n\n\n11.3.3 The Normal Equations\nWe can distribute \\({\\bf X}^{\\prime}\\) through the parentheses \\[\n\\begin{align}\n{\\bf X}^{\\prime}\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)={\\bf 0} & \\Longrightarrow{\\bf X}^{\\prime}{\\bf Y}-{\\bf X}^{\\prime}{\\bf X}{\\bf b}={\\bf 0}\\\\\n& \\Longrightarrow{\\bf X}^{\\prime}{\\bf X}{\\bf b}={\\bf X}^{\\prime}{\\bf Y}\n\\end{align}\n\\tag{11.3}\\] Equation Equation 11.3 represents the normal equations in matrix format.\n\n\n11.3.4 The Estimators\nSolving the normal equations for \\({\\bf b}\\) gives us the estimators: \\[\n\\begin{align}\n{\\bf X}^{\\prime}{\\bf X}{\\bf b}={\\bf X}^{\\prime}{\\bf Y} & \\Longrightarrow\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}^{\\prime}{\\bf X}{\\bf b}=\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}^{\\prime}{\\bf Y}\\\\\n& \\Longrightarrow{\\bf b}=\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}^{\\prime}{\\bf Y}\n\\end{align}\n\\tag{11.4}\\]\n\n\n11.3.5 Fitted Values\nThe fitted values are \\[\n\\begin{align}\n\\hat{{\\bf Y}} & ={\\bf X}{\\bf b}\n\\end{align}\n\\tag{11.5}\\]\nSubstituting Equation 11.4 for \\({\\bf b}\\) gives us \\[\n\\begin{align}\n\\hat{{\\bf Y}}={\\bf X}\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}^{\\prime}{\\bf Y}\n\\end{align}\n\\tag{11.6}\\]\nThe \\(n\\times n\\) matrix \\({\\bf X}\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}^{\\prime}\\) pops up a number of times in multiple regression analysis. We call this matrix the hat matrix and denote it as \\[\n\\begin{align}\n{\\bf H} & ={\\bf X}\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}^{\\prime}\n\\end{align}\n\\tag{11.7}\\]\nThus, the fitted values can be expressed as \\[\n\\begin{align}\n\\hat{{\\bf Y}} & ={\\bf H}{\\bf Y}\n\\end{align}\n\\tag{11.8}\\]\n\n\n11.3.6 Residuals\nThe residuals can be expressed in matrix terms as \\[\n\\begin{align}\n{\\bf e} & ={\\bf Y}-\\hat{{\\bf Y}}\\\\\n& ={\\bf Y}-{\\bf X}{\\bf b}\n\\end{align}\n\\tag{11.9}\\]\nUsing Equation 11.8, the residuals can be expressed in terms of the hat matrix as well \\[\n\\begin{align}\n{\\bf e} & ={\\bf Y}-{\\bf H}{\\bf Y}\\\\\n& =\\left({\\bf I}-{\\bf H}\\right){\\bf Y}\n\\end{align}\n\\tag{11.10}\\]\n\n\n11.3.7 Estimator for the Variance\nWe have seen in simple regression that \\(s^{2}\\) is an estimate of the variance \\(\\sigma^{2}\\). Recall that we call \\(s^{2}\\) the MSE which is just the SSE divided by the degrees of freedom (\\(n-2\\) in simple linear regression).\nIn matrix terms, we can express SSE as \\[\n\\begin{align}\nSSE & ={\\bf e}^{\\prime}{\\bf e}\\\\\n& ={\\bf Y}^{\\prime}\\left({\\bf I}-{\\bf H}\\right)^{\\prime}\\left({\\bf I}-{\\bf H}\\right){\\bf Y}\\\\\n& ={\\bf Y}^{\\prime}\\left({\\bf I}-{\\bf H}\\right){\\bf Y}\n\\end{align}\n\\tag{11.11}\\]\nThe term \\(\\left({\\bf I}-{\\bf H}\\right)^{\\prime}\\left({\\bf I}-{\\bf H}\\right)\\) simplifies to \\(\\left({\\bf I}-{\\bf H}\\right)\\) since \\(\\left({\\bf I}-{\\bf H}\\right)\\) is idempotent1\nThe MSE is \\[\\begin{align*}\nMSE & =\\frac{SSE}{n-p}\n\\end{align*}\\]\nHere the degrees of freedom are now \\(n-p\\) since there are \\(p\\) parameters \\(\\left(\\beta_{0},\\beta_{1},\\ldots,\\beta_{p-1}\\right)\\) that need to be estimated first in order to get \\(SSE\\). As before, MSE is an estimator for \\(\\sigma^{2}\\)."
  },
  {
    "objectID": "11_Regression_Matrix.html#footnotes",
    "href": "11_Regression_Matrix.html#footnotes",
    "title": "11  The Regression Model in Matrix Terms",
    "section": "",
    "text": "Idempotent matrix is a square matrix which when multiplied by itself, gives back the same matrix.↩︎"
  },
  {
    "objectID": "12_ANOVA.html#model-assumptions",
    "href": "12_ANOVA.html#model-assumptions",
    "title": "12  Model Assumptions and the ANOVA F-test",
    "section": "12.1 Model Assumptions",
    "text": "12.1 Model Assumptions\nThe assumptions for the multiple regression model are the same as the simple linear model. That is,\n\nLinearity - We assume the model is linear in the parameters but not necessarily linear in the predictor variables. Thus, some of the predictor variables may need to be transformed.\nconstant variance\nnormality\nindependence\n\nThe inferences on the multiple regression model will depend on these assumptions holding. We will discuss how to check these later."
  },
  {
    "objectID": "12_ANOVA.html#a-first-order-model-with-quantitative-predictors",
    "href": "12_ANOVA.html#a-first-order-model-with-quantitative-predictors",
    "title": "12  Model Assumptions and the ANOVA F-test",
    "section": "12.2 A First-Order Model with Quantitative Predictors",
    "text": "12.2 A First-Order Model with Quantitative Predictors\nRecall that a first-order model means that none of the predictor variables are functions of any other predictor variables.\nWhen the independent variables are quantitative, the \\(\\beta\\) parameters in the first-order model have similar interpretations as the simple regression model. The difference is that when we interpret the \\(\\beta\\) that multiplies one of the variables (e.g., \\(x_1\\)), we must be certain to hold the values of the remaining independent variables (e.g., \\(x_2\\), \\(x_3\\)) fixed."
  },
  {
    "objectID": "12_ANOVA.html#testing-the-utility-of-a-model-the-analysis-of-variance-f-test",
    "href": "12_ANOVA.html#testing-the-utility-of-a-model-the-analysis-of-variance-f-test",
    "title": "12  Model Assumptions and the ANOVA F-test",
    "section": "12.3 Testing the Utility of a Model: The Analysis of Variance \\(F\\)-Test",
    "text": "12.3 Testing the Utility of a Model: The Analysis of Variance \\(F\\)-Test\nThe objective of step 5 in a multiple regression analysis is to conduct a test of the utility of the model—that is, a test to determine whether the model is adequate for predicting \\(y\\).\nLater, we will examine how to conduct \\(t\\)-tests on each \\(\\beta\\) parameter in a model, where \\[\n\\begin{align*}\n    H_0: \\beta_j = 0, \\qquad j=1, 2, \\ldots, p-1\n\\end{align*}\n\\]\nHowever, this approach is generally not a good way to determine whether the overall model is contributing information for the prediction of \\(y\\). If we were to conduct a series of \\(t\\)-tests to determine whether the independent variables are contributing to the predictive relationship, we would be very likely to make one or more errors in deciding which terms to retain in the model and which to exclude.\nSuppose you fit a first-order model with 10 quantitative independent variables, \\(x_1, x_2,..., x_{10}\\), and decide to conduct \\(t\\)-tests on all 10 individual \\(\\beta\\)’s in the model, each at \\(\\alpha = .05\\).\nFor any one test, the probability of making a type I error is 0.05: \\[\n\\begin{align*}\n    P(\\text{Reject } H_0|\\beta_j=0) &= {0.05}\\\\\n    & {= 1 - 0.95}\n\\end{align*}\n\\] If we were to do ten of these tests (one for each predictor variable), then the probability that at least one is a type I error is \\[\n\\begin{align*}\n    P(\\text{Reject at least one } H_0|\\beta_1=\\beta_2=\\cdots=\\beta_{p-1}=0)\n    & = 1-[(1-\\alpha)^{10}]\\\\\n    & {= 1-(0.95)^{10}}\\\\\n    &{ = 0.401}\n\\end{align*}\n\\]\nEven if all the \\(\\beta\\) parameters (except \\(\\beta_0\\)) in the model are equal to 0, approximately 40% of the time you will incorrectly reject the null hypothesis at least once and conclude that some \\(\\beta\\) parameter is nonzero. In other words, the overall Type I error is about .40, not .05.\nTo illustrate this inflated type I error, let’s look at the following example.\n\nExample 12.1 We will simulate a sample of size 200 with a response variable \\(y\\) and ten predictor variables \\(x_1, x_2, \\ldots, x_{10}\\). The random error term \\(\\varepsilon\\) will be a standard normal random variable.\nThe true model will have each coefficient \\(\\beta\\) set to 0, with the exception of \\(\\beta_0\\) which will be 20. We will conduct a \\(t\\)-test for each coefficient (except \\(\\beta_0\\)). Since the true coefficient is 0, we expect to see p-values greater than 0.05 for each coefficient.\nWe will fit the model and test the coefficients 1000 times. We will count the number of times at least one of the coefficients was less than 0.05 (which would lead to a Type I error).\n\nlibrary(tidyverse)\n\nset.seed(3430)\n\nn = 200\n\ntype1error = numeric(1000)\n\nfor(i in 1:1000){\n  \n  x = runif(n*10) |&gt; matrix(ncol = 10)\n  \n  eps = rnorm(n)\n  \n  y = 20 + 0*x[,1] + 0*x[,2] + 0*x[,3] + 0*x[,4] +\n    0*x[,5] + 0*x[,6] + 0*x[,7] + 0*x[,8] + 0*x[,9] +\n    0*x[,10] + eps\n  \n  dat = tibble(y, x1 = x[,1], x2 = x[,2], x3 = x[,3],\n               x4 = x[,4], x5 = x[,5], x6 = x[,6], x7 = x[,7],\n               x8 = x[,8], x9 = x[,9], x10 = x[,10],)\n  \n  fit = lm(y~., data = dat)\n  \n  type1error[i] = any(summary(fit)$coefficients[-1,4] &lt; 0.05)\n}\n\nmean(type1error)\n\n[1] 0.414\n\n\nWe see that 41.4% of the time, we had at least one coefficient have a p-value smaller than 0.05. This corresponds to the probability of at least one Type I error of around 0.401.\n\nIn multiple regression models for which a large number of independent variables are being considered, conducting a series of \\(t\\)-tests may cause the experimenter to include a large number of insignificant variables and exclude some useful ones. If we want to test the utility of a multiple regression model, we will need a global test (one that encompasses all the \\(\\beta\\) parameters).\n\n12.3.1 Partitioning the Sum of Squares\nRecall when we discussed the coefficient of determination that we used \\(SS_{yy}\\) to denote the variability of the response variable \\(y\\) from its mean \\(\\bar y\\) (without regard to the model involving \\(x\\)).\nAnother name for \\(SS_{yy}\\) is the sum of squares total (SSTO). We call it this since it gives us a measure of total variability in \\(y\\).\nIn multiple regression, SSTO is still the same as \\(SS_{yy}\\). In matrix notation this can be expressed as \\[\n\\begin{align}\nSSTO & ={\\bf Y}^{\\prime}{\\bf Y}-\\left(\\frac{1}{n}\\right){\\bf Y}^{\\prime}{\\bf J}{\\bf Y}\n\\end{align}\n\\tag{12.1}\\]\n\nSum of Square Error (SSE)\nAlso recall the variability of \\(y\\) about the regression line (in simple linear regression) was expressed by SSE. We can think of this as the variability of \\(y\\) remaining after exampling some of the variability with the regression model.\nIn multiple regression, SSE is still the sum of the square distances between the response \\(y\\) and the fitted model \\(\\hat{y}\\). Now, the fitted model is the fitted hyperplane instead of a line.\nThe SSE can be expressed in matrix terms as \\[\n\\begin{align}\nSSE & =\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)^{\\prime}\\left({\\bf Y}-{\\bf X}{\\bf b}\\right)\\\\\n& ={\\bf Y}^{\\prime}{\\bf Y}-{\\bf b}^{\\prime}{\\bf X}^{\\prime}{\\bf Y}\n\\end{align}\n\\tag{12.2}\\]\n\n\nSum of Square Regression (SSR)\nIf SSTO is the total variability of \\(y\\) (without of regard to the predictor variables), and SSE is the variability of \\(y\\) left over after explaining the variability of \\(y\\) with the model (including the predictor variables), we might want to know the variability of \\(y\\) explained by the model.\nWe call the variability explained by the regression model the sum of squares regression (SSR).\nWe will show below that SSR can be expressed as \\[\n\\begin{align}\nSSR & =\\sum\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}\n\\end{align}\n\\tag{12.3}\\] which can be expressed in matrix terms as \\[\n\\begin{align}\nSSR & ={\\bf b}^{\\prime}{\\bf X}^{\\prime}{\\bf Y}-\\left(\\frac{1}{n}\\right){\\bf Y}^{\\prime}{\\bf J}{\\bf Y}\n\\end{align}\n\\tag{12.4}\\]\n\n\nComponents of SSTO\nTo see how SSTO, SSR, and SSE relate to each other, consider how SSTO is a sum of squares of \\(y\\) from its mean \\(\\bar{y}\\): \\[\n\\begin{align*}\ny_{i}-\\bar{y}\n\\end{align*}\n\\]\nWe can add and subtract the fitted value \\(\\hat{y}_{i}\\) to get \\[\n\\begin{align*}\ny_{i}-\\bar{y} & =y_{i}-\\hat{y}_{i}+\\hat{y}_{i}-\\bar{y}\\\\\n& =\\left(y_{i}-\\hat{y}_{i}\\right)+\\left(\\hat{y}_{i}-\\bar{y}\\right)\n\\end{align*}\n\\]\nSquaring both sides gives us \\[\n\\begin{align*}\n\\left(y_{i}-\\bar{y}\\right)^{2} & =\\left[\\left(y_{i}-\\hat{y}_{i}\\right)+\\left(\\hat{y}_{i}-\\bar{y}\\right)\\right]^{2}\\\\\n& =\\left(y_{i}-\\hat{y}_{i}\\right)^{2}+\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}+2\\left(y_{i}-\\hat{y}_{i}\\right)\\left(\\hat{y}_{i}-\\bar{y}\\right)\n\\end{align*}\n\\]\nSumming both sides gives us \\[\n\\begin{align*}\n\\sum\\left(y_{i}-\\bar{y}\\right)^{2} & =\\sum\\left(y_{i}-\\hat{y}_{i}\\right)^{2}+\\sum\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}+2\\sum\\left(y_{i}-\\hat{y}_{i}\\right)\\left(\\hat{y}_{i}-\\bar{y}\\right)\\\\\n& =\\sum\\left(y_{i}-\\hat{y}_{i}\\right)^{2}+\\sum\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}+2\\sum\\hat{y}_{i}e_{i}-2\\bar{y}\\sum e_{i}\n\\end{align*}\n\\]\nNote that \\(\\sum\\hat{y}_{i}e_{i}=0\\)  and \\(\\sum e_{i}=0\\). \nTherefore, we have \\[\n\\begin{align*}\n\\sum\\left(y_{i}-\\bar{y}\\right)^{2} & =\\sum\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}+\\sum\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\\\\nSSTO & =SSR+SSE\n\\end{align*}\n\\tag{12.5}\\]\nWe call this the decomposition of SSTO.\n\n\nDegrees of Freedom\nThe degrees of freedom can be decomposed as well. Note that the degrees of freedom for SSTO is \\[\n\\begin{align*}\ndf_{SSTO} & =n-1\n\\end{align*}\n\\] since the mean of \\(y\\) is needed to be estimated with \\(\\bar{y}\\).\nThe degrees of freedom for SSE is \\[\n\\begin{align*}\ndf_{SSE} & =n-p\n\\end{align*}\n\\] since the \\(p\\) coefficients \\(\\beta_{0},\\ldots,\\beta_{p-1}\\) need to be estimated with \\(\\hat{\\beta}_{0},\\ldots,\\hat{\\beta}_{p-1}.\\)\nFor SSR, the degrees of freedom is \\[\n\\begin{align*}\ndf_{SSR} & =p-1\n\\end{align*}\n\\] since there are \\(p\\) estimated coefficients \\(\\hat{\\beta}_{0},\\ldots,\\hat{\\beta}_{p-1}\\) but need to estimate the mean of \\(y\\) with \\(\\bar{y}\\).\nDecomposing the degrees of freedom give us \\[\n\\begin{align}\nn-1 & =p-1+n-p\\\\\ndf_{SSTO} & =df_{SSR}+df_{SSE}\n\\end{align}\n\\tag{12.6}\\]\n\n\n\n12.3.2 The Analysis of Variance (ANOVA) Table\nThe sums of squares and degrees of freedom are commonly displayed in an analysis of variance (ANOVA) table:\n\n\n\nSource\ndf\nSS\nMS\nF\np-value\n\n\n\n\nRegression\n\\(df_{SSR}\\)\n\\(SSR\\)\n\n\n\n\n\nError\n\\(df_{SSE}\\)\n\\(SSE\\)\n\n\n\n\n\nTotal\n\\(df_{SSTO}\\)\n\\(SSTO\\)\n\n\n\n\n\n\n\nMean Squares\nRecall that if we divide SSE by its degrees of freedom, we obtain the mean square error: \\[\n\\begin{align}\nMSE & =\\frac{SSE}{n-p}\n\\end{align}\n\\tag{12.7}\\]\nLikewise, if we divide SSR by its degrees of freedom, we obtain the mean square regression: \\[\n\\begin{align}\nMSR & =\\frac{SSR}{p-1}\n\\end{align}\n\\tag{12.8}\\]\nThese values are also included in the ANOVA table:\n\n\n\nSource\ndf\nSS\nMS\nF\np-value\n\n\n\n\nRegression\n\\(df_{SSR}\\)\n\\(SSR\\)\n\\(MSR\\)\n\n\n\n\nError\n\\(df_{SSE}\\)\n\\(SSE\\)\n\\(MSE\\)\n\n\n\n\nTotal\n\\(df_{SSTO}\\)\n\\(SSTO\\)\n\n\n\n\n\n\nNote that although the sum of squares and degrees of freedom decompose, the mean squares do not. That is \\[\n\\begin{align*}\n\\frac{SSTO}{n-1} & \\ne MSR+MSE\n\\end{align*}\n\\]\nIn fact, the mean square for the total (\\(SSTO/n-1\\)) does not usually show up on the ANOVA table.\n\n\n\n12.3.3 The ANOVA F-test\nWe tested the slope in simple regression to see if there is a significant linear relationship between \\(X\\) and \\(Y\\).\nIn multiple regression, we will want to see if there is any significant linear relationship between any of the \\(x\\)s and \\(y\\). Thus, we want to test the hypotheses \\[\n\\begin{align*}\nH_{0}: & \\beta_{1}=\\beta_{2}=\\cdots=\\beta_{p-1}=0\\\\\nH_{a}: & \\text{at least one } \\beta \\text{ is not equal to zero}\n\\end{align*}\n\\]\nTo construct a test statistic, we first note that \\[\n\\begin{align*}\n\\frac{SSE}{\\sigma^{2}} & \\sim\\chi^{2}\\left(n-p\\right)\n\\end{align*}\n\\] Also, if \\(H_{0}\\) is true, then \\[\n\\begin{align*}\n\\frac{SSR}{\\sigma^{2}} & \\sim\\chi^{2}\\left(p-1\\right)\n\\end{align*}\n\\]\nThe ratio of two independent chi-square random variables divided by their degrees of freedom give a statistic that follows a F-distribution.\nSince \\(SSE/\\sigma^{2}\\) and \\(SSR/\\sigma^{2}\\) are independent (proof not give here), then under \\(H_{0}\\), we can construct a test statistic as \\[\n\\begin{align}\nF^{*} & =\\left(\\frac{\\frac{SSR}{\\sigma^{2}}}{p-1}\\right)\\div\\left(\\frac{\\frac{SSE}{\\sigma^{2}}}{n-p}\\right)\\\\\n& =\\left(\\frac{SSR}{p-1}\\right)\\div\\left(\\frac{SSE}{n-p}\\right)\\\\\n& =\\frac{MSR}{MSE}\n\\end{align}\n\\tag{12.9}\\]\nLarge values of \\(F^{*}\\) indicate evidence for \\(H_{a}\\).\nThe test statistic and p-value are the last two components of the ANOVA table:\n\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\nF\np-value\n\n\n\n\nRegression\n\\(df_{SSR}\\)\n\\(SSR\\)\n\\(MSR\\)\n\\(F^{*}\\)\n\\(P\\left(Z\\ge Z^{*}\\right)\\)\n\n\nError\n\\(df_{SSE}\\)\n\\(SSE\\)\n\\(MSE\\)\n\n\n\n\nTotal\n\\(df_{SSTO}\\)\n\\(SSTO\\)\n\n\n\n\n\n\n\nExample 12.2 We will fit a multiple regression model to the trees dataset. The tidymodels framework will be used.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndata(trees)\n\n#prepare data\ndat_recipe = recipe(Volume ~ Girth + Height, data = trees) \n\n#setup model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#setup the workflow\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = trees)\n\n#to get the coefficients\nlm_fit |&gt; tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -58.0       8.64      -6.71 2.75e- 7\n2 Girth          4.71      0.264     17.8  8.22e-17\n3 Height         0.339     0.130      2.61 1.45e- 2\n\n\nThe fitted model is \\[\n\\hat{y} = -57.988 + 4.708x_1 + 0.339x_2\n\\] For every one inch increase in Girth, the average Volume increases by 4.708 cubit ft, keeping Height fixed.\nFor every one ft increase in Height, the average Volume increases by 0.339 cubit ft, keeping Girth fixed.\n\n#to get the global F-test p-value\nlm_fit |&gt; glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.948         0.944  3.88      255. 1.07e-18     2  -84.5  177.  183.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n#To get the ANOVA table\n#Note that SSR is split into individual predictors\nlm_fit |&gt; extract_fit_engine() |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: ..y\n          Df Sum Sq Mean Sq  F value  Pr(&gt;F)    \nGirth      1 7581.8  7581.8 503.1503 &lt; 2e-16 ***\nHeight     1  102.4   102.4   6.7943 0.01449 *  \nResiduals 28  421.9    15.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince the p-value of \\(1.071\\times 10^{-18}\\) is less than 0.05, there is sufficient evidence to conclude that at least one of the coefficients is not zero."
  },
  {
    "objectID": "13_Inferences.html#inferences-for-the-parameters",
    "href": "13_Inferences.html#inferences-for-the-parameters",
    "title": "13  Model Inferences and Second-Order Models",
    "section": "13.1 Inferences for the Parameters",
    "text": "13.1 Inferences for the Parameters\nAs they were in the simple regression case, the least squares estimators \\({\\bf b}\\) are unbiased \\[\n\\begin{align}\n{\\bf E}\\left[{\\bf b}\\right] & =\\boldsymbol{\\beta}\n\\end{align}\n\\tag{13.1}\\]\nThe \\(p\\times p\\) covariance matrix is \\[\n\\begin{align}\n{\\bf Cov}\\left[{\\bf b}\\right] & =\\sigma^{2}\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}\n\\end{align}\n\\tag{13.2}\\]\nWe can estimate the covariance matrix by using MSE as an estimate for \\(\\sigma^{2}\\). We will denote this estimated covariance matrix as \\[\n\\begin{align}\n{\\bf s}^{2}\\left[{\\bf b}\\right] & =MSE\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}\n\\end{align}\n\\tag{13.3}\\]\nFrom \\({\\bf s}^{2}\\left[{\\bf b}\\right]\\), we can obtain \\(s^{2}\\left[b_{0}\\right]\\), \\(s^{2}\\left[b_{1}\\right]\\), or the variance for any other coefficient to use in the confidence intervals and hypothesis tests.\n\n13.1.1 Confidence Intervals for the Coefficients\nA \\(\\left(1-\\alpha\\right)100\\%\\) confidence interval for the parameter \\(\\beta_{k}\\) is \\[\n\\begin{align}\nb_{k} & \\pm t_{\\alpha/2,n-p}s\\left[b_{k}\\right]\n\\end{align}\n\\tag{13.4}\\]\n\n\n13.1.2 Tests for the Coefficients\nWe can test the hypothesis \\[\n\\begin{align*}\nH_{0}: & \\beta_{k}=0\\\\\nH_{a}: & \\beta_{k}\\ne0\n\\end{align*}\n\\] with the test statistic \\[\n\\begin{align}\nt^{*} & =\\frac{b_{k}}{s\\left[b_{k}\\right]}\n\\end{align}\n\\tag{13.5}\\] where \\(t^{*}\\sim t\\left(n-p\\right)\\).\n\nExample 13.1 (bodyfat data from Kutner1) In this dataset the bodyfat, tricep skinfold thickness, thigh circumference, and midarm circumference are found for 20 healthy females 25-34 years old. We want to model bodyfat based on the other three variables.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndat = read_table(\"BodyFat.txt\")\n\n#prepare data\ndat_recipe = recipe(bfat ~ tri + thigh + midarm, \n                    data = dat) \n\n#setup model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#setup the workflow\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = dat)\n\n#to get the coefficients and CIs\nlm_fit |&gt; tidy(conf.int = TRUE, conf.level = 0.95)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   117.       99.8       1.17   0.258   -94.4     329.  \n2 tri             4.33      3.02      1.44   0.170    -2.06     10.7 \n3 thigh          -2.86      2.58     -1.11   0.285    -8.33      2.62\n4 midarm         -2.19      1.60     -1.37   0.190    -5.57      1.20\n\n\nNote that each coefficient is insignificant when tested individually. Let’s use the global F-test to see if the model is useful.\n\nlm_fit |&gt; glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.801         0.764  2.48      21.5 0.00000734     3  -44.3  98.6  104.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe global F-test is significant. The reason for the contradiction between the t-tests and the F-test will be discussed when we examine the correlation between predictor variables.\n\n\nRecommendation for Checking the Utility of a Multiple Regression Model\n\nFirst, conduct a test of overall model adequacy using the F-test.\nIf the model is deemed adequate (i.e., if you reject \\(H_0\\)), then proceed to step 2. Otherwise, you should hypothesize and fit another model. The new model may include more independent variables or higher-order terms.\nConduct \\(t\\)-tests on those \\(\\beta\\) parameters in which you are particularly interested (i.e., the “most important” \\(\\beta\\)’s). It is safe practice to limit the number of \\(\\beta\\)’s that are tested. Conducting a series of \\(t\\)-tests leads to a high overall Type I error rate.\n\nExtreme care should be exercised when conducting \\(t\\)-tests on the individual \\(\\beta\\) parameters in a first-order linear model for the purpose of determining which independent variables are useful for predicting \\(y\\) and which are not.\nIf you fail to reject \\(H_0: \\beta_j = 0\\), several conclusions are possible:\n\nThere is no relationship between \\(y\\) and \\(x_j\\).\nA straight-line relationship between \\(y\\) and \\(x_j\\) exists (holding the other \\(x\\)’s in the model fixed), but a Type II error occurred.\nA relationship between \\(y\\) and \\(x_j\\) (holding the other \\(x\\)’s in the model fixed) exists, but is more complex than a straight-line relationship (e.g., a curvilinear relationship may be appropriate). The most you can say about a \\(\\beta\\) parameter test is that there is either sufficient (if you reject \\(H_0: \\beta_j=0\\)) or insufficient (if you do not reject \\(H_0: \\beta_j=0\\)) evidence of a linear (straight-line) relationship between \\(y\\) and \\(x_j\\)."
  },
  {
    "objectID": "13_Inferences.html#coefficient-of-multiple-determination",
    "href": "13_Inferences.html#coefficient-of-multiple-determination",
    "title": "13  Model Inferences and Second-Order Models",
    "section": "13.2 Coefficient of Multiple Determination",
    "text": "13.2 Coefficient of Multiple Determination\nAs was the case with the simple regression model, the coefficient of determination for the multiple regression model (or the coefficient of multiple determination) is \\[\n\\begin{align}\nR^{2} & ={\\frac{SSR}{SSTO}}\\\\\n& {=1-\\frac{SSE}{SSTO}}\n\\end{align}\n\\tag{13.6}\\]\nThe interpretation is still the same: it gives the proportion of the variation in \\(y\\) explained by the model using the predictor variables.\n\n13.2.1 Adjusted Coefficient of Determination\nIt is of importance to note that \\(R^{2}\\) cannot decrease when adding another \\(x\\) to the model. It can either increase (if the new \\(x\\) explains more of the variability of \\(y\\)) or stay the same (if the new \\(x\\) does not explain more of the variability of \\(y\\)). This can be seen in Equation 13.6 by noting that SSE cannot become larger by including more \\(X\\) variables and SSTO stays the same regardless of which \\(X\\) variables are used.\nBecause of this, \\(R^{2}\\) cannot be used for comparing the fit of models with different subsets of the \\(x\\) variables.\nA modified version of the \\(R^{2}\\) could be used that adjusts for the number of \\(X\\) variables. It is called the adjusted coefficient of determination denoted as \\(R_{a}^{2}\\).\nIn \\(R_{a}^{2}\\), SSE and SSTO are divided by their respective degrees of freedom: \\[\n\\begin{align}\nR_{a}^{2} & =1-\\frac{\\frac{SSE}{n-p}}{\\frac{SSTO}{n-1}}\\\\\n& =1-\\left(\\frac{n-1}{n-p}\\right)\\frac{SSE}{SSTO}\n\\end{align}\n\\tag{13.7}\\]\nThe value of \\(R_{a}^{2}\\) can decrease when another \\(x\\) is included in the model because any decrease in SSE may be more than offset by the loss of a degree of freedom of SSE (\\(n-p\\))."
  },
  {
    "objectID": "13_Inferences.html#estimation-and-prediction-of-the-response",
    "href": "13_Inferences.html#estimation-and-prediction-of-the-response",
    "title": "13  Model Inferences and Second-Order Models",
    "section": "13.3 Estimation and Prediction of the Response",
    "text": "13.3 Estimation and Prediction of the Response\n\n13.3.1 Estimating the Mean Response\nWe estimate the mean response as we did in the simple linear case except now we will estimate at a vector of values: \\[\n\\begin{align*}\n{\\bf X}_{h} & =\\left[\\begin{array}{c}\n1\\\\\nx_{h1}\\\\\nx_{h2}\\\\\n\\vdots\\\\\nx_{h,p-1}\n\\end{array}\\right]\n\\end{align*}\n\\]\nSo the estimated mean response will be the regression function evaluated at \\({\\bf X}_{h}\\): \\[\n\\begin{align}\n\\hat{Y}_{h} & ={\\bf X}_{h}{\\bf b}\n\\end{align}\n\\tag{13.8}\\] The variance of the estimated mean response is \\[\n\\begin{align}\nVar\\left[\\hat{Y}_{h}\\right] & =\\sigma^{2}{\\bf X}_{h}^{\\prime}\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}_{h}\n\\end{align}\n\\tag{13.9}\\]\nWe can estimate the variance as \\[\n\\begin{align}\ns^{2}\\left[\\hat{Y}_{h}\\right] & =MSE{\\bf X}_{h}^{\\prime}\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}_{h}\n\\end{align}\n\\tag{13.10}\\]\nWe can then obtain a \\(\\left(1-\\alpha\\right)100\\%\\) confidence interval for the mean response at \\({\\bf X}_{h}\\) as \\[\n\\begin{align}\n\\hat{Y}_{h} & \\pm t_{\\alpha/2}s\\left[\\hat{Y}_{h}\\right]\n\\end{align}\n\\tag{13.11}\\] where \\(t_{\\alpha/2}\\) has \\(n-p\\) degrees of freedom.\n\n\n13.3.2 Predicting the Response\nWe can predict a new response \\(Y_{h\\left(new\\right)}\\) at some \\({\\bf X}_{h}\\) with a \\(\\left(1-\\alpha\\right)100\\%\\) prediction interval \\[\n\\begin{align}\n\\hat{Y}_{h} & \\pm t_{\\alpha/2}s\\left[Y_{h\\left(pred\\right)}\\right]\n\\end{align}\n\\tag{13.12}\\] where \\[\n\\begin{align}\ns^{2}\\left[Y_{h\\left(pred\\right)}\\right] & =MSE\\left(1+{\\bf X}_{h}^{\\prime}\\left({\\bf X}^{\\prime}{\\bf X}\\right)^{-1}{\\bf X}_{h}\\right)\n\\end{align}\n\\tag{13.13}\\]\n\nExample 13.2 (Example 13.1 revisited) For the bodyfat data, suppose we want to predict and estimate at tri=25, thigh=51.2, and midarm=24.9. We will use the predict function.\n\nnew_dat = tibble(tri=25, thigh=51.2, midarm = 24.9)\n\n#confidence interval for mean response\nlm_fit |&gt; predict(new_dat, type = \"conf_int\", level = 0.95)\n\n# A tibble: 1 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1        17.5        31.9\n\n#prediction interval\nlm_fit |&gt; predict(new_dat, type = \"pred_int\", level = 0.95)\n\n# A tibble: 1 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1        15.8        33.6"
  },
  {
    "objectID": "13_Inferences.html#second-order-models",
    "href": "13_Inferences.html#second-order-models",
    "title": "13  Model Inferences and Second-Order Models",
    "section": "13.4 Second-Order Models",
    "text": "13.4 Second-Order Models\n\n13.4.1 Simulating data in R\nIn R, there are a number of functions involving probability distributions. Each available distribution will have some functions that can be utilized. For the normal distribution, we can access the functions\n\ndnorm - gives the density (pdf) of the distribution\npnorm - gives the cumulative distribution function (cdf)\nqnorm - gives the quantile of the distribution\nrnorm - gives a random sample from the distribution\n\nDifferent distributions will have the same four functions with the starting letter above. For example, if we want to use the binomial distribution, we can use the functions dbinom, pbinom, qbinom, and rbinom.\nIf we use the following code:\n\nset.seed(34)\n\nx = rnorm(100)\n\nggplot(data = data.frame(x))+\n  aes(x=x)+\n  geom_histogram(bins = 10)\n\n\n\n\nNote that the histogram is shown for one random sample. If you were to run the code again, it would give you a different sample and a different histogram.\nIf we would like to simulate from a normal distribution with mean 10 and standard deviation of 5, we would use the code rnorm(100, mean = 10, sd = 5)\n\n\n13.4.2 Simulating a regression model\nSuppose we had the regression model \\[\n\\begin{align*}\n    y =& 1+2x_1 - x_2 + \\varepsilon\\\\\n    &\\varepsilon \\sim N(0, 0.1)\n\\end{align*}\n\\]\nWe can use the rnorm function to obtain a sample of the error terms.\n\nn = 100\neps = rnorm(100, mean = 0, sd = 0.1)\n\nWe now will obtain some values of the \\(x\\)s. Let’s make the values of \\(x_1\\) be values from 0 to 3. The values of \\(x_2\\) will be only the values 0, 1, 2. Thus, both predictors are quantitative but \\(x_2\\) is discrete.\nWe can simulate from the model by:\n\nn = 300\neps = rnorm(n, mean = 0, sd = 0.1)\n\nx1values = seq(0, 3, length = n/3)\nx2values = c(0, 1, 2)\n\nvalues = expand.grid(x1values, x2values)\n\nx1 = values[,1]\nx2 = values[,2]\n\ny = 1 + 2*x1- x2 + eps\n\ndf = data.frame(y = y, x1 = x1, x2 = factor(x2))\nggplot(df)+\n  aes(x = x1, y = y, color = x2)+\n  geom_point()\n\n\n\n\n\n\n13.4.3 An Interaction Model with Quantitative Predictors\nWhen \\(E(y)\\) is graphed against any one variable (say, \\(x_1\\)) for fixed values of the other variables, the result is a set of parallel straight lines (like the plot above).\nWhen this situation occurs (as it always does for a first-order model), we say that the relationship between \\(E(y)\\) and any one independent variable does not depend on the values of the other independent variables in the model.\nHowever, if the relationship between \\(E(y)\\) and \\(x_1\\) does, in fact, depend on the values of the remaining \\(x\\)’s held fixed, then the first- order model is not appropriate for predicting \\(y\\). In this case, we need another model that will take into account this dependence. Such a model includes the cross products of two or more \\(x\\)’s.\nFor example, suppose that the mean value \\(E(y)\\) of a response \\(y\\) is related to two quantitative independent variables, \\(x_1\\) and \\(x_2\\), by the model \\[\n\\begin{align*}\n    E(y) = 1+2x_1-x_2+x_1x_2\n\\end{align*}\n\\]\nLet’s simulate from this model and construct a scatterplot of \\(y\\) versus \\(x_1\\) again:\n\nn = 300\neps = rnorm(n, mean = 0, sd = 0.1)\n\nx1values = seq(0, 3, length = n/3)\nx2values = c(0, 1, 2)\n\nvalues = expand.grid(x1values, x2values)\n\nx1 = values[,1]\nx2 = values[,2]\n\ny = 1 + 2*x1- x2 + x1*x2+ eps\n\ndf = data.frame(y = y, x1 = x1, x2 = factor(x2))\nggplot(df)+\n  aes(x = x1, y = y, color = x2)+\n  geom_point()\n\n\n\n\nNote that the graph shows three nonparallel lines. You can verify that the slopes of the lines differ by substituting each of the values \\(x_2 = 0, 1\\), and 2 into the equation.\nFor \\(x_2 = 0\\): \\[\n\\begin{align*}\n    E(y) &= 1+2x_1 - (0) + x_1(0)\\\\\n    & = 1+2x_1\n\\end{align*}\n\\]\nFor \\(x_2 =1\\): \\[\n\\begin{align*}\n    E(y) &= 1+2x_1 - (1) + x_1(1)\\\\\n    & = 3x_1\n\\end{align*}\n\\]\nFor \\(x_2 = 2\\): \\[\n\\begin{align*}\n    E(y) &= 1+2x_1 - (2) + x_1(2)\\\\\n    & = -1 + 4x_1\n\\end{align*}\n\\]\nNote that the slope of each line is represented by \\[\n\\begin{align*}\n    \\beta_1 + \\beta_2 x_2 =  2+x_2\n\\end{align*}\n\\] Thus, the effect on \\(E(y)\\) of a change in \\(x_1\\) (i.e., the slope) now depends on the value of \\(x_2\\). When this situation occurs, we say that \\(x_1\\) and \\(x_2\\) interact.\nThe cross-product term, \\(x_1x_2\\), is called an interaction term, and the model called an interaction model with two quantitative variables.\nNote an important point about conducting \\(t\\)-tests on the \\(\\beta\\) parameters in the interaction model.\nThe “most important” \\(\\beta\\) parameter in this model is the interaction term, \\(\\beta_3\\).\nConsequently, we will want to test \\(H_0: \\beta_3 = 0\\) after we have determined that the overall model is useful for predicting \\(y\\). Once interaction is detected tests on the first-order terms \\(x_1\\) and \\(x_2\\) should not be conducted since they are meaningless tests; the presence of interaction implies that both \\(x\\)’s are important.\n\n\n13.4.4 A Quadratic (Second-Order) Model with a Quantitative Predictor\nAll of the models discussed thus far proposed straight-line relationships between \\(E(y)\\) and each of the independent variables in the model.\nWe now consider a model that allows for curvature in the relationship.\nThis model is a second-order model because it will include an \\(x^2\\) term2.\nHere, we consider a model that includes only one independent variable \\(x\\). The form of this model, called the quadratic model, is \\[\n\\begin{align*}\n    y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\varepsilon\n\\end{align*}\n\\]"
  },
  {
    "objectID": "13_Inferences.html#a-test-for-comparing-nested-models",
    "href": "13_Inferences.html#a-test-for-comparing-nested-models",
    "title": "13  Model Inferences and Second-Order Models",
    "section": "13.5 A Test for Comparing Nested Models",
    "text": "13.5 A Test for Comparing Nested Models\nIn regression analysis, we often want to determine (with a high degree of confidence) which one among a set of candidate models best fits the data.\nWe present such a method for nested models.\nTwo models are nested if one model contains all the terms of the second model and at least one additional term. The more complex of the two models is called the complete (or full) model. The simpler of the two models is called the reduced (or restricted) model.\nTo illustrate, suppose you have collected data on a response, \\(y\\), and two quantitative independent variables, \\(x_1\\) and \\(x_2\\), and you are considering the use of either a straight-line interaction model or a curvilinear model to relate \\(E(y)\\) to \\(x_1\\) and \\(x_2\\).\nWill the curvilinear model provide better predictions of \\(y\\) than the straight-line model?\nTo answer this question, examine the two models, and note that the curvilinear model contains all the terms in the straight-line interaction model plus two additional terms—those involving \\(\\beta_4\\) and \\(\\beta_5\\): \\[\n\\begin{align*}\n    E(y) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 &\\quad \\text{Straight-line with interaction}\\\\\n    E(y) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2+ \\beta_4 x^2_1 +\\beta_5 x_2^2&\\quad \\text{Curvillinear model}\n\\end{align*}\n\\]\nConsequently, these are nested models. Since the straight-line model is the simpler of the two, we say that the straight-line model is nested within the more complex curvilinear model.\nAlso, the straight-line model is called the reduced model while the curvilinear model is called the complete (or full) model.\nAsking whether the curvilinear (or complete) model contributes more information for the prediction of \\(y\\) than the straight-line (or reduced) model is equivalent to asking whether at least one of the parameters, \\(\\beta_4\\) or \\(\\beta_5\\), differs from 0. Therefore, to test whether the quadratic terms should be included in the model, we test the null hypothesis \\[\n\\begin{align*}\n    H_0: \\beta_4 = \\beta_5 = 0\n\\end{align*}\n\\] (i.e., the quadratic terms do not contribute information for the prediction of \\(y\\)) against the alternative hypothesis \\[\n\\begin{align*}\n    H_a: \\text{At least one of the parameters, }\\beta_4 \\text{ or } \\beta_5, \\text{differ from 0}\n\\end{align*}\n\\] (i.e., at least one of the quadratic terms contributes information for the prediction of \\(y\\)).\nThe procedure for conducting this test is intuitive. First, we use the method of least squares to fit the reduced model and calculate the corresponding sum of squares for error, \\[\nSSE_R\n\\] Next, we fit the complete model and calculate its sum of squares for error, \\[\nSSE_C\n\\]\nThen, we compare \\(SSE_R\\) to \\(SSE_C\\) by calculating the difference, \\[\nSSE_R - SSE_C\n\\] If the quadratic terms contribute to the model, then \\(SSE_C\\) should be much smaller than \\(SSE_R\\), and the difference will be large. The larger the difference, the greater the weight of evidence that the complete model provides better predictions of \\(y\\) than does the reduced model.\nThe sum of squares for error will always decrease when new terms are added to the model. The question is whether this decrease is large enough to conclude that it is due to more than just an increase in the number of model terms and to chance.\nTo test the null hypothesis that the curvature coefficients simultaneously equal 0, we use an F statistic. For our example, this F statistic is: \\[\n\\begin{align*}\n    F = \\frac{\\frac{(SSE_R - SSE_C)}{\\text{Number of parameters beging tested}}}{\\frac{SSE_C}{n-p}}\n\\end{align*}\n\\]\nWhen the candidate models in model building are nested models, the \\(F\\)-Test is the appropriate procedure to apply to compare the models. However, if the models are not nested, this \\(F\\)-Test is not applicable. In this situation, the analyst must base the choice of the best model on statistics such as \\(R^2_a\\). It is important to remember that decisions based on these and other numerical descriptive measures of model adequacy cannot be supported with a measure of reliability and are often very subjective in nature.\n\nExample 13.3 (Example 13.1 revisited - Nested F-test) Suppose, for some reason, we were to hypothesize that model needed the term midarm^2. Let’s conduct a nested F-test to see if this extra term is significant.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n#prepare data for the complete model\ndat_recipe = recipe(bfat ~ tri + thigh + midarm , \n                    data = dat) |&gt; \n  step_poly(midarm, degree = 2)\n\n#setup model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#setup the workflow for complete model\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#fit the complete model\nlm_fit_C = lm_workflow |&gt;\n  fit(data = dat)\n\n#setup the workflow for reduced model\nlm_workflow_R = lm_workflow |&gt; \n  update_recipe(\n    dat_recipe |&gt; step_rm(midarm_poly_2)\n  )\n\n#fit the reduced model\nlm_fit_R = lm_workflow_R |&gt;\n  fit(data = dat)\n\n#extract the models and conduct nested F-test\nmodel_C = lm_fit_C |&gt; extract_fit_engine()\nmodel_R = lm_fit_R |&gt; extract_fit_engine()\n\nanova(model_R, model_C)\n\nAnalysis of Variance Table\n\nModel 1: ..y ~ tri + thigh + midarm_poly_1\nModel 2: ..y ~ tri + thigh + midarm_poly_1 + midarm_poly_2\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     16 98.405                           \n2     15 90.397  1     8.008 1.3288 0.2671\n\n\nSince the p-value is greater than 0.05, there is not enough evidence to conclude that the quadratic term coefficient is different than 0."
  },
  {
    "objectID": "13_Inferences.html#footnotes",
    "href": "13_Inferences.html#footnotes",
    "title": "13  Model Inferences and Second-Order Models",
    "section": "",
    "text": "Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied Linear Statistical Models McGraw-Hill/lrwin series operations and decision sciences.↩︎\nNote: the interaction model is also second-order since it has two \\(x\\)s multiplied.↩︎"
  },
  {
    "objectID": "14_Multicollinearity.html#multicollinearity",
    "href": "14_Multicollinearity.html#multicollinearity",
    "title": "14  Multicollinearity and Principal Component Regression",
    "section": "14.1 Multicollinearity",
    "text": "14.1 Multicollinearity\nOften, two or more of the independent variables used in the model for \\(E(y)\\) will contribute redundant information. That is, the independent variables will be correlated with each other.\nFor example, suppose we want to construct a model to predict the gasoline mileage rating, \\(y\\), of a truck as a function of its load, \\(x_1\\), and the horsepower, \\(x_2\\), of its engine.\nIn general, you would expect heavier loads to require greater horsepower and to result in lower mileage ratings.\nThus, although both \\(x_1\\) and \\(x_2\\) contribute information for the prediction of mileage rating, some of the information is overlapping, because \\(x_1\\) and \\(x_2\\) are correlated.\nWhen the independent variables are correlated, we say that multicollinearity exists.\nThe ability to obtain a good fit or to make inferences on the mean response or to predict the response are not affected by multicollinearity. However, inferences for the coefficients (the \\(\\beta\\)s) and for the model variance (\\(\\sigma^2\\)) are affected by large correlation among the predictor variables.\nAnother effect of multicollinearity is the interpretation of the estimated coefficients. In multiple regression, we interpret the coefficient as the average change in \\(y\\) when \\(x\\) is increased by one unit when all other predictor variables are held constant.\nIf \\(x\\) is highly correlated with one or more of the other predictor variables, then it may not be feasible to think of varying \\(x\\) when the others are constant.\n\n14.1.1 Variance Inflation Factors\nWe can see evidence of multicollinearity by examining the scatterplot matrix since this will give us a plot of each pair of predictor variables. If there are pairs that appear to be highly correlated (ggpairs in R will give the correlation value as well) then multicollinearity will be present.\nWe could also examine \\(R_{a}^{2}\\) for models with and without certain pairs of variables. If \\(R_{a}^{2}\\) decreases when a particular \\(x\\) variable is added but it appears to have a strong linear relationship with \\(y\\) in the scatterplot matrix, then this is evidence of multicollinearity.\nA more convenient way to examine multicollinearity is through the use of the variance inflation factors (VIF).\nEach predictor variable will have a VIF. Suppose we are interested in the VIF for \\(x_{1}\\). We start by regression \\(x_{1}\\) on all the other predictor variables. Thus, we fit the model \\[\n\\begin{align*}\nx_{i1} & =\\alpha_{0}+\\alpha_{2}x_{i2}+\\alpha_{3}x_{i3}+\\cdots+\\alpha_{p-1}x_{i,p-1}+\\epsilon\n\\end{align*}\n\\] where the \\(\\alpha\\)’s are the coefficients and \\(\\epsilon\\) is the random error term.\nNow find the coefficient of multiple determination for this model which we will denote as \\(R_{1}^{2}\\). The VIF for \\(x_{1}\\) is then \\[\n\\begin{align*}\nVIF_{1} & =\\frac{1}{1-R_{1}^{2}}.\n\\end{align*}\n\\] We can do this for any \\(i\\)th predictor variable so that the VIF for that variable is \\[\n\\begin{align}\nVIF_{i} & =\\frac{1}{1-R_{i}^{2}}\n\\end{align}\n\\tag{14.1}\\] where \\(R_{i}^{2}\\) is the coefficient of multiple determination for the regression fit of \\(x_{i}\\) on all the other predictor variables.\nA rule of thumb is that a VIF greater than 10 is evidence that multicollinearity is high when that variable is added to the model. Some use a cutoff of 5 instead of 10.\n\nExample 14.1 (UN98 data) One approach to seeing which variables are correlated with each other is to remove a variable with a large VIF and see which variables had the largest change in their VIF.\nWe will illustrate this process with the dataset from the library. We will not use the and variables for this example.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(car)\nlibrary(GGally)\n\n#explore correlation and scatterplots between pairs of variables\nUN98 |&gt; \n  select(-region, -GDPperCapita) |&gt; \n  ggpairs()\n\n\n\n#prepare data\ndat_recipe = recipe(infantMortality ~ ., \n                    data = UN98) |&gt; \n  step_rm(region, GDPperCapita)\n\n#setup model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#setup the workflow\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = UN98)\n\nlm_fit |&gt; tidy()\n\n# A tibble: 11 × 5\n   term                   estimate std.error statistic p.value\n   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)            111.        48.9       2.27  0.0309 \n 2 tfr                     12.0        3.87      3.11  0.00423\n 3 contraception           -0.0709     0.137    -0.517 0.609  \n 4 educationMale            5.98       3.11      1.92  0.0647 \n 5 educationFemale         -6.41       2.76     -2.32  0.0278 \n 6 lifeMale                -0.405      1.11     -0.365 0.718  \n 7 lifeFemale              -0.757      1.32     -0.575 0.570  \n 8 economicActivityMale    -0.383      0.264    -1.45  0.158  \n 9 economicActivityFemale   0.172      0.128     1.34  0.190  \n10 illiteracyMale          -0.544      0.392    -1.39  0.176  \n11 illiteracyFemale         0.321      0.313     1.03  0.314  \n\nlm_fit |&gt; glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.944         0.924  9.40      47.5 6.75e-15    10  -136.  296.  316.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n#get the VIFs\nlm_fit |&gt; extract_fit_engine() |&gt; vif()\n\n                   tfr          contraception          educationMale \n             14.319844               3.331552              23.833590 \n       educationFemale               lifeMale             lifeFemale \n             27.561149              42.279211              74.707620 \n  economicActivityMale economicActivityFemale         illiteracyMale \n              2.059083               2.049210              17.476821 \n      illiteracyFemale \n             24.355614 \n\n\nWe see that there are a number of variables with high VIF. The highest is lifeFemale. If we remove this variable, what happens to the VIFs of the other variables?\n\n#take out the lifeFemale variable\nlm_workflow2= lm_workflow |&gt; \n  update_recipe(\n    dat_recipe |&gt; step_rm(lifeFemale)\n  )\n\n#fit the model\nlm_fit2 = lm_workflow2 |&gt;\n  fit(data = UN98)\n\n#get the VIFs\nlm_fit2 |&gt; extract_fit_engine() |&gt; vif()\n\n                   tfr          contraception          educationMale \n              7.650182               3.151511              23.768388 \n       educationFemale               lifeMale   economicActivityMale \n             27.547206               6.053211               2.040815 \neconomicActivityFemale         illiteracyMale       illiteracyFemale \n              1.989025              15.271120              20.160209 \n\n\nWe see that removing lifeFemale leads to a couple of the variable to have a substantial decrease in their VIfs. The variables tfr and lifeMale both decrease by more than five points when lifeFemale is removed. From the scatterplot matrix above, we see that tfr and lifeMale have the highest correlation with lifeFemale. If we are deciding whether to keep lifeFemale in the model, then we can do so with a practical reason. Are any of the variables that are highly correlated with lifeFemale easier to obtain? If so, we should keep that variable and remove the other.\nLet’s now look at the next highest VIF: educationFemale.\n\n#take out the educationFemale variable\nlm_workflow3= lm_workflow |&gt; \n  update_recipe(\n    dat_recipe |&gt; step_rm(educationFemale)\n  )\n\n#fit the model\nlm_fit3 = lm_workflow3 |&gt;\n  fit(data = UN98)\n\n#get the VIFs\nlm_fit3 |&gt; extract_fit_engine() |&gt; vif()\n\n                   tfr          contraception          educationMale \n             14.173794               3.303755               3.425892 \n              lifeMale             lifeFemale   economicActivityMale \n             42.269363              74.669828               1.898826 \neconomicActivityFemale         illiteracyMale       illiteracyFemale \n              2.000282              15.192202              18.021121 \n\n\nWe see that educationMale has dropped substantially when educationFemale was removed. Again, deciding which variable to remove from the model is a practical one.\nWe can continue this process of identifying which variables are highly correlate with other variables.\n\n\n\n14.1.2 Effects on Inferences\nRecall from Equation 13.3 that we can obtain the variance of the least squares estimators with the diagonal of \\({\\bf s}^{2}\\left[{\\bf b}\\right]\\).\nIt can be shown that this variance can be expressed in terms of VIF. So the variance of \\(b_{j}\\) can be expressed as \\[\n\\begin{align}\ns^{2}\\left[b_{j}\\right] & =MSE\\frac{VIF_{j}}{\\left(n-1\\right)\\widehat{Var}\\left[x_{j}\\right]}\n\\end{align}\n\\tag{14.2}\\] where \\(\\widehat{Var}\\left[x_{j}\\right]\\) is the sample variance of \\(x_j\\).\nSo we see that if \\(VIF_{j}\\) is large (meaning there is multicollinearity when \\(x_{j}\\) is included in the model) then the standard error will be larger.\nNoting that the test statistic for testing \\(\\beta_{j}=0\\) in Equation 13.5 is \\[\n\\begin{align*}\nt^{*} & =\\frac{b_{j}}{s\\left[b_{j}\\right]}\n\\end{align*}\n\\]\nSo an inflated standard error \\(s\\left[b_{j}\\right]\\) will lead to a smaller \\(t\\) and thus a larger p-value. This will cause us to conclude there is not enough evidence for the alternative hypothesis when in fact \\(\\beta_{j}\\ne0\\).\n\n\n14.1.3 Effects on CI and PI for the Response\nAs stated above, multicollinearity does not affect the confidence interval for the mean response or the prediction interval.\nWe will illustrate this with the bodyfat data below.\n\nExample 14.2 (bodyfat data)  \n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(car)\nlibrary(GGally)\n\ndat = read_table(\"BodyFat.txt\")\n\n\n#examine the scatterplot matrix\nggpairs(dat)\n\n\n\n\nWe see from this scatterplot matrix, that the predictor variables have some high correlation between them. Thus, we already see that there will be a problem with multicollinearity.\n\n#prepare data\ndat_recipe = recipe(bfat ~ tri + thigh + midarm, \n                    data = dat) \n\n#setup model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#setup the workflow\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = dat)\n\nfit_full = lm_fit |&gt;  extract_fit_engine()\n\nfit_full |&gt; vif()\n\n     tri    thigh   midarm \n708.8429 564.3434 104.6060 \n\n\nFrom VIFs, we see that there is clear multicollinearity between all three variables.\nThe highest VIF is tri. From the scatterplot matrix above, we see tri is most correlated with thigh. Let’s remove tri and see what happens.\n\nlm_workflow_no_tri= lm_workflow |&gt; \n  update_recipe(\n    dat_recipe |&gt; step_rm(tri)\n  )\n\n\n#fit the model\nlm_fit_no_tri = lm_workflow_no_tri |&gt;\n  fit(data = dat)\n\nfit_no_tri= lm_fit_no_tri |&gt;  extract_fit_engine()\n\nfit_no_tri |&gt; vif()\n\n  thigh  midarm \n1.00722 1.00722 \n\n\nWe see that once tri is removed, the remaining variables have small VIFs.\nWe are not suggesting that tri should definitely be removed. It may still be the best predictor of bfat. There are other ways to determine if only having tri is preferred over a model with just the other two variables. We will discuss those methods later. For now, we do see that multicollinearity will be an issue for these three variables.\nTo see the effect on the estimated coefficients, let’s look at the standard errors for the model with all three variables and the model without tri.\n\nfit_full |&gt; tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   117.       99.8       1.17   0.258\n2 tri             4.33      3.02      1.44   0.170\n3 thigh          -2.86      2.58     -1.11   0.285\n4 midarm         -2.19      1.60     -1.37   0.190\n\nfit_no_tri |&gt; tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept) -26.0        7.00     -3.72  0.00172    \n2 thigh         0.851      0.112     7.57  0.000000772\n3 midarm        0.0960     0.161     0.595 0.560      \n\n\nNote how much larger the standard errors are for the full model (with multicollinearity) than the model without tri (no multicollinearity).\nLarger standard errors will lead to smaller \\(t\\) statistics and thus larger p-values. So multicollinearity will make variables look like they are insignificant but they really are significant.\nLet’s now look at the RMSE (sigma in the glance() function output) which is used in the confidence interval of the mean response and prediction interval formulas.\n\nfit_full |&gt; glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.801         0.764  2.48      21.5 0.00000734     3  -44.3  98.6  104.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nfit_no_tri |&gt; glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.776         0.749  2.56      29.4 0.00000303     2  -45.5  99.1  103.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNote that RMSE is not much different between the two models. It is not affected by multicollinearity. So if all we want to use our model for are estimation of the mean and predictions, then multicollinearity is not an issue. If, however, we want to also determine which variables are important in the estimation and prediction, then multicollinearity is an issue."
  },
  {
    "objectID": "14_Multicollinearity.html#standardizing-predictor-variables",
    "href": "14_Multicollinearity.html#standardizing-predictor-variables",
    "title": "14  Multicollinearity and Principal Component Regression",
    "section": "14.2 Standardizing Predictor Variables",
    "text": "14.2 Standardizing Predictor Variables\nIn linear regression, predictor variables are often transformed to ensure that they are on a comparable scale. This is especially important when predictors have vastly different units or magnitudes, as it can influence the stability and interpretability of the model.\nA common technique for transforming predictors is standardization, where each predictor variable is rescaled to have a mean of 0 and a standard deviation of 1. This section will explore the concept of standardization, how it differs from normalization, and the implications for model performance, especially in the context of multicollinearity.\n\n14.2.1 Standardization vs. Normalization\nWhile the terms “standardization” and “normalization” are sometimes used interchangeably, they describe distinct mathematical operations. Standardization transforms a variable \\(X\\) to have a mean of 0 and a standard deviation of 1, using the following formula:\n\\[\nz_i = \\frac{x_i - \\bar{x}}{s_x}\n\\]\nwhere \\(z_i\\) is the standardized value, \\(x_i\\) is the original value, \\(\\bar{x}\\) is the mean of the variable, and \\(s_x\\) is its standard deviation. This process ensures that the transformed variable has a standard normal distribution with mean 0 and standard deviation 1.\nNormalization, on the other hand, typically refers to scaling the data to a specific range, such as [0, 1]. This is done using the formula:\n\\[\nx' = \\frac{x_i - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n\\]\nNormalization is most useful when the range of the variables needs to be constrained for certain algorithms, such as in neural networks. In contrast, standardization is generally preferred in linear models, where the focus is on centering and rescaling predictors to ensure interpretability of coefficients.\n\n\n14.2.2 Why tidymodels Uses the Term “Normalize”\nIn the tidymodels framework in R, the term “normalize” is used to describe what is technically a standardization process. For example, when creating a preprocessing recipe with the recipe() function, the step called step_normalize() computes the mean and standard deviation of each predictor and scales it accordingly. Although the terminology might be confusing, this usage reflects a common convention in some statistical software where both standardization and normalization are loosely referred to as normalization.\n\n\n14.2.3 Standardization and Multicollinearity\nBy scaling all predictors to a common variance, the impact of large magnitude differences between variables is reduced, leading to more stable coefficient estimates. Standardization alone does not reduce correlations between variables. However, it provides numerical stability when using methods other than least squares, making it easier to detect and address multicollinearity issues. When multicollinearity is severe, other techniques, such as principal component regression, may be necessary."
  },
  {
    "objectID": "14_Multicollinearity.html#principal-component-regression",
    "href": "14_Multicollinearity.html#principal-component-regression",
    "title": "14  Multicollinearity and Principal Component Regression",
    "section": "14.3 Principal Component Regression",
    "text": "14.3 Principal Component Regression\nOne effective method for addressing multicollinearity is Principal Component Regression (PCR), which combines Principal Component Analysis (PCA) and linear regression. The key idea behind PCR is to transform the original predictor variables into a smaller set of uncorrelated variables, called principal components (PCs), which capture the most variance in the data.\n\n14.3.1 Introduction to Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique that identifies the directions (called principal components) along which the variation in the data is maximized. These directions are orthogonal to each other and ranked by the amount of variance they capture. The first principal component explains the largest amount of variance in the data, followed by the second principal component, and so on. Each principal component is a linear combination of the original predictor variables.\nIn practical terms, PCA helps to reduce the dimensionality of the data, making it more manageable without losing too much information. This is particularly useful in cases where the number of predictors is large, and some of them are highly correlated.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nPrincipal components (PCs) are linear transformations of the original predictor variables that aim to capture the maximum variance in the data. Here is a mathematical summary of how these components are computed:\n\nStandardizing the Data Given a dataset with \\(p\\) predictors \\(x_1, x_2, \\dots, x_p\\) and \\(n\\) observations, we first standardize each predictor variable to have mean 0 and standard deviation 1:\n\n\\[\nz_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j}\n\\]\nwhere: - \\(z_{ij}\\) is the standardized value of the \\(j\\)-th predictor for the \\(i\\)-th observation. - \\(\\bar{x}_j\\) is the mean of the \\(j\\)-th predictor. - \\(s_j\\) is the standard deviation of the \\(j\\)-th predictor.\nThis step ensures that all variables are on the same scale, which is necessary for PCA.\n\nComputing the Covariance Matrix Once the data is standardized, we compute the covariance matrix \\(\\mathbf{S}\\) of the standardized predictors:\n\n\\[\n\\mathbf{S} = \\frac{1}{n-1} \\mathbf{Z}^T \\mathbf{Z}\n\\]\nwhere \\(\\mathbf{Z}\\) is the matrix of standardized data with \\(n\\) rows (observations) and \\(p\\) columns (predictors).\n\nFinding Eigenvalues and Eigenvectors We solve the following eigenvalue problem for the covariance matrix \\(\\mathbf{S}\\):\n\n\\[\n\\mathbf{S} \\mathbf{v}_j = \\lambda_j \\mathbf{v}_j\n\\]\nwhere: - \\(\\lambda_j\\) is the \\(j\\)-th eigenvalue of the covariance matrix. - \\(\\mathbf{v}_j\\) is the corresponding eigenvector (also called the loading vector) associated with \\(\\lambda_j\\).\nThe eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\) represent the amount of variance explained by each principal component. The eigenvectors define the direction of the new coordinate axes (principal components).\n\nConstructing the Principal Components The \\(j\\)-th principal component \\(PC_j\\) is a linear combination of the original standardized variables:\n\n\\[\nPC_j = v_{j1} z_1 + v_{j2} z_2 + \\cdots + v_{jp} z_p\n\\]\nwhere \\(v_{jk}\\) is the \\(k\\)-th element of the eigenvector \\(\\mathbf{v}_j\\).\nEach principal component is uncorrelated with the others and explains a decreasing amount of variance. Specifically:\n\nPC1 (the first principal component) explains the largest possible variance.\nPC2 explains the largest remaining variance subject to being orthogonal to PC1.\nThis process continues for all \\(p\\) components.\n\n\nExplained Variance The proportion of the total variance explained by the \\(j\\)-th principal component is:\n\n\\[\n\\text{Explained Variance of } PC_j = \\frac{\\lambda_j}{\\sum_{k=1}^p \\lambda_k}\n\\]\nThe cumulative variance explained by the first \\(m\\) components is:\n\\[\n\\text{Cumulative Explained Variance} = \\frac{\\sum_{j=1}^m \\lambda_j}{\\sum_{k=1}^p \\lambda_k}\n\\]\n\n\n\nIn the context of regression, instead of regressing the response variable on the original set of predictors, we use the principal components as the new predictors. By selecting a subset of the principal components, we can retain most of the variability in the predictors while reducing multicollinearity.\n\nExample 14.3 (bodyfat data again) The following steps will illustrate how to apply PCA to the predictors and then use the principal components in a regression model.\n\nrecipe = recipe(bfat ~ ., data = dat) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 3)\n\n#determine the parameters for any of the steps in the recipe\n#in this case, we need the mean and std dev of each variable\n#along with the values for principal components\nprepped = recipe |&gt; prep()\n\n#apply the steps (with the prepared parameters found in prep)\n#to the data\npca_data = prepped |&gt; bake(dat)\n\n# Print the principal components\npca_data\n\n# A tibble: 20 × 4\n    bfat      PC1     PC2       PC3\n   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1  11.9 -1.63     1.10    0.0463  \n 2  22.8 -0.193    0.264   0.0375  \n 3  18.7  1.73     2.19   -0.0245  \n 4  20.1  1.33     0.547  -0.00257 \n 5  12.9 -1.62     1.62   -0.0363  \n 6  21.7 -0.00515 -1.20    0.00331 \n 7  27.1  1.72    -0.683  -0.0242  \n 8  25.4  0.755    0.628   0.0327  \n 9  21.3 -1.02    -0.947   0.0301  \n10  19.3  0.0379  -0.891  -0.0448  \n11  25.4  1.68     0.0702 -0.0153  \n12  27.2  1.43    -0.349   0.000371\n13  11.7 -1.92    -0.677  -0.0247  \n14  17.8 -1.52     0.883  -0.0221  \n15  12.8 -3.10    -0.734  -0.0177  \n16  23.9  1.21     0.296   0.0176  \n17  22.6  0.645   -0.843  -0.0184  \n18  25.4  1.28    -1.42    0.0179  \n19  14.8 -0.767    0.148   0.0302  \n20  21.1 -0.0464  -0.0141  0.0148  \n\n\nThe following code provides the principal components derived from the predictors in the BodyFat dataset. Each principal component is a linear combination of the original predictors (tri, thigh, midarm), and they explain the maximum variance in the data.\nTo visualize how PCA works and to understand the contribution of each principal component, let’s examine the explained variance.\n\n# Extract the PCA results\npca_results = prepped |&gt; tidy(number = 2, type=\"variance\")\npca_results\n\n# A tibble: 12 × 4\n   terms                            value component id       \n   &lt;chr&gt;                            &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance                      2.07             1 pca_tZVif\n 2 variance                      0.933            2 pca_tZVif\n 3 variance                      0.000727         3 pca_tZVif\n 4 cumulative variance           2.07             1 pca_tZVif\n 5 cumulative variance           3.00             2 pca_tZVif\n 6 cumulative variance           3                3 pca_tZVif\n 7 percent variance             68.9              1 pca_tZVif\n 8 percent variance             31.1              2 pca_tZVif\n 9 percent variance              0.0242           3 pca_tZVif\n10 cumulative percent variance  68.9              1 pca_tZVif\n11 cumulative percent variance 100.               2 pca_tZVif\n12 cumulative percent variance 100                3 pca_tZVif\n\n# Visualize the explained variance\npca_results |&gt;\n  filter(terms == \"percent variance\") |&gt;\n  ggplot(aes(x = component, y = value)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Explained Variance by Principal Component\", \n       x = \"Principal Components\", \n       y = \"Proportion of Variance Explained\")\n\n\n\n\nIn this plot, we observe the proportion of variance explained by each principal component. The first principal component typically explains the most variance, followed by the second, and so on. Depending on the cumulative proportion of variance explained, we can select an appropriate number of components for our regression model.\nLet’s now look at the scatterplot matrix of the PCA data. Note how the correlations between the PCs are zero.\n\nggpairs(pca_data)\n\n\n\n\nPrincipal Component Regression is just using these PCs as the predictor variables. We see from above that the third PC does not explain much variability. Thus, we can just use the first two PCs and save a degree of freedom since we will be using one less coefficient in the model.\n\ndat_recipe = recipe(bfat ~ ., data = dat) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 2)\n\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\nlm_fit = lm_workflow |&gt;\n  fit(data = dat)\n\nlm_fit |&gt; tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    20.2      0.566     35.7  1.98e-17\n2 PC1             2.94     0.404      7.27 1.30e- 6\n3 PC2            -1.65     0.601     -2.75 1.38e- 2\n\n\nNote how small the standard errors are for the coefficients now that we no longer have multicollinearity. Let’s verify that there is no multicollinearity by examining the VIFs.\n\nfit = lm_fit |&gt;  extract_fit_engine()\n\nfit |&gt; vif()\n\nPC1 PC2 \n  1   1 \n\n\nBoth VIFs are exactly one indicating no multicollinearity.\n\nPCR gives us the ability to still fit the regression model even in the presence of extreme multicollinearity. Another benefit of PCR is that you can also reduce dimensionality by only using the PCs that explain most of the variability. Note that you still need all the original predictor variables since the PCs are linear combinations of these variables. Thus, this is not a method for removing predictor variables. So if our goal is to determine if a variable (that may be difficult or expensive to obtain) can be dropped, then PCR is not the tool we want to use. In addition, in PCR we lose interpretability of the coefficients."
  },
  {
    "objectID": "15_Indicator.html#two-types-of-independent-variables",
    "href": "15_Indicator.html#two-types-of-independent-variables",
    "title": "15  Indicator Variables",
    "section": "15.1 Two Types of Independent Variables",
    "text": "15.1 Two Types of Independent Variables\nThe independent variables that appear in a linear model can be one of two types: quantitative and qualitative (or categorical)\nThe different values of an independent variable used in regression are called it’s levels.\nFor a quantitative variable, the levels correspond to the numerical values it assumes. For example, if the number of defects in a product ranges from 0 to 3, the independent variable assumes four levels: 0, 1, 2, and 3.\nThe levels of a qualitative variable are not numerical. They can be defined only by describing them. Thus, they are the categories the variable can be.\n\n15.1.1 Qualitative Predictors\nThus far, we have only considered predictor variables that are quantitative.\nQualitative predictor variables can also be used in regression models.\nLet’s look at an example involving handspans and heights of students.\n\nExample 15.1 (Handspan data) The dataset examined here consists of left and right hand spans of 1102 students along with the height, sex, handedness, and dominate eye. The response variable is right hand span with height as a predictor variable. Below is a scatterplot of the data.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndat = read_csv(\"SurveyMeasurements.csv\")\n\ndat |&gt; ggplot(aes(x = `height (inches)`, y = `right handspan (cm)`))+\n  geom_point()\n\n\n\n\nLet’s now take into account sex. We can color code males and females in the plot and see if there is a discernible difference. We will also fit separate simple linear regression models to each sex.\n\ndat |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = `height (inches)`, y = `right handspan (cm)`,\n             color = sex))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n\n\n\nWe can see that sex plays a role in the regression line. For the male subjects, the fitted line tends to be higher than for the female subjects."
  },
  {
    "objectID": "15_Indicator.html#indicator-variables-for-two-classes",
    "href": "15_Indicator.html#indicator-variables-for-two-classes",
    "title": "15  Indicator Variables",
    "section": "15.2 Indicator Variables for Two Classes",
    "text": "15.2 Indicator Variables for Two Classes\nWe will include the qualitative variables in our model by using indicator variables.\nAn indicator variable (or dummy variable) takes on the values of 0 and 1. For example, a model for the handspan data is \\[\n\\begin{align*}\ny_{i} & =\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\varepsilon\n\\end{align*}\n\\] where \\[\n\\begin{align*}\ny & =\\text{ handspan}\\\\\nx_{1} & =\\text{ height}\\\\\nx_{2} & =\\begin{cases}\n1 & \\text{ if sex=M}\\\\\n0 & \\text{ otherwise}\n\\end{cases}\n\\end{align*}\n\\]\n\n15.2.1 Interpretation\nIf the subject is male, then the expectation becomes \\[\n\\begin{align*}\nE\\left[y_{i}\\right] & =\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}\\left(1\\right)\\\\\n& {=\\left(\\beta_{0}+\\beta_{2}\\right)+\\beta_{1}x_{i1}}\n\\end{align*}\n\\] If the subject is female, the expectation is \\[\n\\begin{align*}\nE\\left[y_{i}\\right] & =\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}\\left(0\\right)\\\\\n& {=\\beta_{0}+\\beta_{1}x_{i1}}\n\\end{align*}\n\\]\nThus, the two groups will have the same slope (\\(\\beta_{1}\\)), but will have different intercepts. That means the means of the two groups are different by \\(\\beta_{2}\\) for all values of \\(x_{1}\\).\nWe could also have different slopes for the two groups by incorporating an interaction term between the predictors.\n\nExample 15.2 (Handspan data again) Indicator variables can be specified with the step_dummy function. You can tell it which variable to make into indicator variables. If we want all character variables to be turned into indicator variables, we can specify all_nominal_predictors().\n\n#prepare data\ndat_recipe = recipe(`right handspan (cm)` ~ `height (inches)` + sex, \n                    data = dat) |&gt; \n  step_naomit() |&gt; \n  step_dummy(all_nominal_predictors())\n\n#setup model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#setup the workflow\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = dat)\n\nlm_fit |&gt; tidy()\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          6.22     1.18        5.26 1.75e- 7\n2 `height (inches)`    0.195    0.0182     10.7  1.93e-25\n3 sex_m                1.74     0.155      11.3  5.82e-28\n\n\nNote that the variable sex has an m next to it. This is letting you know which category (level) is encoded with 1. The way this is determined in step_dummy is based on which level first in alphabetical order. For the sex variable, f is first. So it is the reference level. The reference level will be the 0 in the encoding.\nIf we want to change the reference level so another category, we can use the step_relevel function first.\n\n#prepare data\ndat_recipe = recipe(`right handspan (cm)` ~ `height (inches)` + sex, \n                    data = dat) |&gt; \n  step_naomit() |&gt; \n  step_relevel(sex, ref_level = \"m\") |&gt; \n  step_dummy(all_nominal_predictors())\n\n#setup model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#setup the workflow\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = dat)\n\nlm_fit |&gt; tidy()\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          7.96     1.29        6.19 8.73e-10\n2 `height (inches)`    0.195    0.0182     10.7  1.93e-25\n3 sex_f               -1.74     0.155     -11.3  5.82e-28\n\n\nNote that the coefficient for height did not change and the coefficient for sex just signs but had the same magnitude."
  },
  {
    "objectID": "15_Indicator.html#qualitative-predictors-with-more-than-two-classes",
    "href": "15_Indicator.html#qualitative-predictors-with-more-than-two-classes",
    "title": "15  Indicator Variables",
    "section": "15.3 Qualitative Predictors with More Than Two Classes",
    "text": "15.3 Qualitative Predictors with More Than Two Classes\nWe can use indicator variables for qualitative predictors that have more than two classes (categories).\nFor example, suppose we wanted to model the sales price of a home bases on the quantitative predictors lot size (\\(x_1\\)), local taxes (\\(x_2\\)), and age (\\(x_3\\)).\nWe may also want to include the qualitative predictor for air conditioning type. The possible classes are “no air conditioning”, “window units”, “heat pumps”, and “central air conditioning”.\nWe will now set up the indicator variables in the following way: \\[\n\\begin{align*}\nx_{4} & =\\begin{cases}\n1 & \\text{ if no air conditioning}\\\\\n0 & \\text{ otherwise }\n\\end{cases}\\\\\nx_{5} & =\\begin{cases}\n1 & \\text{ if window units}\\\\\n0 & \\text{ otherwise }\n\\end{cases}\\\\\nx_{6} & =\\begin{cases}\n1 & \\text{ if heat pumps}\\\\\n0 & \\text{ otherwise }\n\\end{cases}\n\\end{align*}\n\\] We do not include an indicator variable for the last class “central air conditioning” because subjects with \\(x_{4}=0\\), \\(x_{5}=0\\), and \\(x_{6}=0\\) will be considered in the class “central air conditioning”.\nAs a general rule, if there are \\(c\\) classes for a qualitative variable, then \\(c-1\\) indicator variables will be needed.\nThe expectations become \\[\n\\begin{align*}\n\\text{no air conditioning: }E\\left[y_{i}\\right]= & \\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}+\\beta_{4}\\left(1\\right)+\\beta_{5}\\left(0\\right)+\\beta_{6}\\left(0\\right)\\\\\n= & \\left(\\beta_{0}+\\beta_{4}\\right)+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\text{window units: }E\\left[y_{i}\\right]= & \\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}+\\beta_{4}\\left(0\\right)+\\beta_{5}\\left(1\\right)+\\beta_{6}\\left(0\\right)\\\\\n= & \\left(\\beta_{0}+\\beta_{5}\\right)+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\text{heat pumps: }E\\left[y_{i}\\right]= & \\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}+\\beta_{4}\\left(0\\right)+\\beta_{5}\\left(0\\right)+\\beta_{6}\\left(1\\right)\\\\\n= & \\left(\\beta_{0}+\\beta_{6}\\right)+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\text{central air conditioning: }E\\left[y_{i}\\right]= & \\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}+\\beta_{4}\\left(0\\right)+\\beta_{5}\\left(0\\right)+\\beta_{6}\\left(0\\right)\\\\\n= & \\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\beta_{3}x_{i3}\n\\end{align*}\n\\]\nSo once again the slopes will be the same for each class but the intercept will change for different classes.\n\nExample 15.3 (CDI data) Let’s examine the CDI data from Kutner1. The data consist of the number of active physicians (\\(Y\\)), the population size, total personal income, and region of 440 counties in the United States.\nThe region variable is originally coded in the dataset as\n1 - New England 2 - North Central 3 - Southern 4 - Western\nNote that when you read this into R, they are thought of as numeric and not factors or characters. Then they are then passed to the lm engine, they are treated as any other quantitative variable.\nWe can use the step_num2factor function to convert these to factors. We can even change the names from the numbers to the name of the region by passing this function a vector of names (in the order of the numbers).\n\ndat = read_table(\"CDI.txt\")\n\nregion_names = c(\"New England\", \"North Central\",\n                 \"Southern\", \"Western\")\n\n#prepare data\ndat_recipe = recipe(num_physicians~pop+personal_income+region, \n                    data = dat) |&gt; \n  step_num2factor(region, levels = region_names) |&gt; \n  step_dummy(all_nominal_predictors())\n\n#setup model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#setup the workflow\nlm_workflow = workflow() |&gt;\n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = dat)\n\nlm_fit |&gt; tidy()\n\n# A tibble: 6 × 5\n  term                    estimate std.error statistic  p.value\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           -58.5      58.8        -0.994  3.21e- 1\n2 pop                     0.000551  0.000284    1.94   5.24e- 2\n3 personal_income         0.107     0.0133      8.07   6.80e-15\n4 region_North.Central   -3.49     78.8        -0.0443 9.65e- 1\n5 region_Southern        42.2      74.0         0.570  5.69e- 1\n6 region_Western       -149.       86.8        -1.72   8.68e- 2\n\n\nNote that we did not have to use step_dummy here due to the numbers being turned into factors by step_num2factor. The lm engine automatically turns factors into dummy variables for you. In fact, the engine will turn any character into dummy variables. So we did not need to use step_dummy in either Example 15.1 or Example 15.2 either. However, it is good practice to include step_dummy in your recipe when dealing with qualitative variables since it will allow for more control when dealing with complex data."
  },
  {
    "objectID": "15_Indicator.html#footnotes",
    "href": "15_Indicator.html#footnotes",
    "title": "15  Indicator Variables",
    "section": "",
    "text": "Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied Linear Statistical Models McGraw-Hill/lrwin series operations and decision sciences.↩︎"
  },
  {
    "objectID": "16_Linearity.html#scatterplot-matrix",
    "href": "16_Linearity.html#scatterplot-matrix",
    "title": "16  The Linearity Assumption",
    "section": "16.1 Scatterplot Matrix",
    "text": "16.1 Scatterplot Matrix\nThe first step in assessing the linearity assumption in a multiple regression model is to examine the relationships between the predictor variables and the response variable using a scatterplot matrix. This matrix provides a grid of pairwise scatterplots, allowing you to visualize potential relationships between all pairs of variables, especially the relationship between each predictor and the response variable.\nIn the context of multiple regression, we expect that the relationship between each predictor and the response is approximately linear. If the scatterplot for a given predictor and the response shows a straight-line trend, this suggests a linear relationship, which aligns with the assumption. However, if the plot exhibits a clear curved or nonlinear pattern, it may indicate that the linearity assumption is violated for that predictor, and you may need to transform the variable to better capture the relationship.\nA scatterplot matrix is a convenient way to quickly scan for any nonlinearity before fitting the model. You can create this matrix using the GGally package in R, which extends ggplot2 to allow for a grid of scatterplots. Each scatterplot shows how one variable changes in response to another, and from this, you can judge whether a linear transformation is needed for any predictor variables before fitting the regression model.\nExample:\n\nlibrary(GGally)\n\nggpairs(mtcars)\n\n\n\n\nIn this example, the ggpairs function creates a scatterplot matrix for the mtcars dataset. Each plot in the matrix shows the relationship between two variables. By focusing on the plots that involve the response variable (mpg in this case), you can assess whether the relationships between the response and each predictor (like wt or hp) appear linear. If any plot shows a nonlinear trend, it suggests that a transformation might be necessary to achieve a linear relationship.\nThis initial diagnostic step is crucial because it allows you to anticipate issues with linearity before fitting the regression model, ensuring a better fit and more reliable interpretation of the results.\n\n16.1.1 Discrete Predictors\nWhen a predictor variable has only a few discrete values, such as in the case of gear or cyl in the mtcars dataset, the scatterplot matrix will show points aligned vertically or horizontally at specific values. These discrete predictors can make it harder to judge linearity directly because the relationships are less continuous. Instead of a smooth trend, look for general patterns in how the response variable changes across the levels of the predictor. If the response values vary systematically across the predictor levels (e.g., a noticeable upward or downward shift), it may still indicate a linear trend. However, if the response varies non-linearly (e.g., higher values in the middle category and lower values on the ends), it may suggest that a transformation, such as using polynomial terms, or treating the predictor as a categorical variable, could improve the model.\nFor example, the cyl predictor in the mtcars dataset has only three distinct values (4, 6, and 8). If the corresponding mpg values show a clear linear decrease as cyl increases, this supports linearity. However, if the trend is irregular, a transformation or alternative approach may be necessary."
  },
  {
    "objectID": "16_Linearity.html#identifying-transformations",
    "href": "16_Linearity.html#identifying-transformations",
    "title": "16  The Linearity Assumption",
    "section": "16.2 Identifying Transformations",
    "text": "16.2 Identifying Transformations\nWhen the scatterplot matrix suggests nonlinearity between a predictor and the response, it may be necessary to transform the predictor variable to improve the linear relationship. Common transformations include logarithmic, square root, and polynomial transformations. These transformations help capture nonlinear patterns and make relationships more linear, ensuring the regression model provides accurate estimates.\nIn the tidyverse, transformations can be easily applied using mutate() from the dplyr package. You can then incorporate these transformed variables into your regression model within the Tidymodels framework.\n\nExample 16.1 (Log transformation in mtcars) The scatterplot matrix showed a non-linear trend for disp versus mpg. Let’s transformed this variable using a log transformation.\n\nlibrary(tidyverse)\n\n# Add a log-transformed variable for weight (wt)\nmtcars |&gt;\n  mutate(log_disp = log(disp)) |&gt; \n  select(mpg, log_disp, disp) |&gt; \n  ggpairs()\n\n\n\n\nWe see the scatterplot between mpg and log_disp appears more linear than with the untransformed disp."
  },
  {
    "objectID": "16_Linearity.html#fitting-the-model-with-transformed-variables",
    "href": "16_Linearity.html#fitting-the-model-with-transformed-variables",
    "title": "16  The Linearity Assumption",
    "section": "16.3 Fitting the Model with Transformed Variables",
    "text": "16.3 Fitting the Model with Transformed Variables\nAfter identifying and applying transformations, the next step is to fit a multiple regression model using the transformed predictors. This ensures that the model aligns with the linearity assumption, yielding more reliable predictions and inferences. Below is an example of how to fit such a model.\n\nExample 16.2 We will setup two models: one with transformed variables and one without transformed variables. We will then compare the results using \\(R^2\\). We will transform disp, hp, and wt since these three variables appear to be nonlinear in the scatterplot matrix. We will not include the discrete variables in this example.\n\nlibrary(tidymodels)\n\n#untransformed variables\ndat_recipe = recipe(mpg ~ disp + hp + drat + wt + qsec, data = mtcars) \n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nwf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\nfit_untransformed &lt;- wf |&gt; fit(mtcars)\n\nfit_untransformed |&gt; glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.849         0.820  2.56      29.2 6.89e-10     5  -72.1  158.  169.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNow we will fit the model with the transformed variables.\n\n# transformed variables\ndat_recipe = recipe(mpg ~ disp + hp + drat + wt + qsec, data = mtcars) |&gt; \n  step_mutate(\n    log_disp = log(disp),\n    log_hp = log(hp),\n    log_wt = log(wt)\n  ) |&gt; \n  step_rm(disp, hp, wt)\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nwf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\nfit_transformed &lt;- wf |&gt; fit(mtcars)\n\nfit_transformed |&gt; glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.892         0.871  2.16      43.0 9.14e-12     5  -66.7  147.  158.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNote the improvement in the coefficient of determination. It has increased with the transformed variables."
  },
  {
    "objectID": "16_Linearity.html#checking-linearity-after-fitting-the-model",
    "href": "16_Linearity.html#checking-linearity-after-fitting-the-model",
    "title": "16  The Linearity Assumption",
    "section": "16.4 Checking Linearity After Fitting the Model",
    "text": "16.4 Checking Linearity After Fitting the Model\nAfter fitting the model, it’s essential to validate that the transformations improved the linear relationship between the predictors and the response. This can be achieved by examining the residuals.\nIf the linearity assumption holds, the residuals should appear randomly scattered around zero in the residual plot, with no obvious patterns. Systematic patterns, such as curves, indicate remaining nonlinearity, suggesting that further transformations or a different model might be necessary.\n\nExample 16.3 Let’s first examine the fit with the untransformed variables.\n\n#obtain the residuals and fitted values from the fit\npredictions_untransformed = extract_fit_engine(fit_untransformed) |&gt; \n  augment()\n\npredictions_untransformed |&gt; \n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Fitted Values\", \n       y = \"Residuals\", \n       title = \"Residuals vs. Fitted Values\") +\n  theme_minimal()\n\n\n\n\nWe see from this residual plot that there is a nonlinear pattern. That is, the residuals tend to be above 0 at the low end, then they tend to be below 0 in the middle, then they tend to be back above 0 at the high end. If there are not that many predictors, you can plot the residuals against each predictor and determine which one needs to be transformed.\nLet’s first examine the fit with the untransformed variables.\n\nlibrary(gridExtra)\n\np1 = predictions_untransformed |&gt; \n  ggplot(aes(x = disp, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\np2 = predictions_untransformed |&gt; \n  ggplot(aes(x = hp, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\np3 = predictions_untransformed |&gt; \n  ggplot(aes(x = drat, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\np4 = predictions_untransformed |&gt; \n  ggplot(aes(x = wt, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\np5 = predictions_untransformed |&gt; \n  ggplot(aes(x = qsec, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\ngrid.arrange(p1, p2, p3, p4, p5, nrow = 3)\n\n\n\n\nCombinig these residuals plots with the scatterplot matrix, it appears that disp, hp, and wt are clearly nonlinear.\nLet’s now see the residual plot for the transformed variables.\n\n#obtain the residuals and fitted values from the fit\npredictions_transformed = extract_fit_engine(fit_transformed) |&gt; \n  augment()\n\npredictions_transformed |&gt; \n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Fitted Values\", \n       y = \"Residuals\", \n       title = \"Residuals vs. Fitted Values\") +\n  theme_minimal()\n\n\n\n\nThis residual plot shows no obvious nonlinear pattern. Thus, the transformations helped."
  },
  {
    "objectID": "17_Criteria.html#possible-models",
    "href": "17_Criteria.html#possible-models",
    "title": "17  Model Comparison Criteria",
    "section": "17.1 Possible Models",
    "text": "17.1 Possible Models\nFrom any set of \\(p - 1\\) predictors, \\(2^{p-1}\\) alternative models can be constructed.\nThis calculation is based on the fact that each predictor can be either included or excluded from the model.\nFor example, the \\[\n2^4 = 16\n\\] different possible subset models that can be formed from the pool of four \\(X\\) variables are \\[\n\\begin{align*}\n& \\text{None}\\\\\n& x_{1}\\\\\n& x_{2}\\\\\n& x_{3}\\\\\n& x_{4}\\\\\n& x_{1},x_{2}\\\\\n& x_{1},x_{3}\\\\\n& x_{1},x_{4}\\\\\n& x_{2},x_{3}\\\\\n& x_{2},x_{4}\\\\\n& x_{3},x_{4}\\\\\n& x_{1},x_{2},x_{3}\\\\\n& x_{1},x_{2},x_{4}\\\\\n& x_{1},x_{3},x_{4}\\\\\n& x_{2},x_{3},x_{4}\\\\\n& x_{1},x_{2},x_{3},x_{4}\n\\end{align*}\n\\]\nIf there are 10 potential predictor variables, there there would be \\[\n2^{10} = 1024\n\\] possible subset models.\nModel selection procedures, also known as subset selection or variables selection procedures, have been developed to identify a small group of regression models that are “good” according to a specified criterion.\nThis limited number might consist of three to six “good” subsets according to the criteria specified, so the investigator can then carefully study these regression models for choosing the final model.\nWhile many criteria for comparing the regression models have been developed, we will focus on six: \\[\n\\begin{align*}\n& R_{p}^{2}\\\\\n& R_{a,p}^{2}\\\\\n& AIC_{p}\\\\\n& SBC_{p}\\\\\n& PRESS_{p}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "17_Criteria.html#notation",
    "href": "17_Criteria.html#notation",
    "title": "17  Model Comparison Criteria",
    "section": "17.2 Notation",
    "text": "17.2 Notation\nBefore discussing the criteria, we will need to develop some notation. We will denote the number of potential \\(X\\) variables in the pool by \\(P-1\\).\nWe assume that all regression models contain an intercept term \\(\\beta_0\\).\nHence, the regression function containing all potential \\(X\\) variables contains \\(P\\) parameters, and the function with no \\(X\\) variables contains one parameter (\\(\\beta_0\\)).\nThe number of \\(X\\) variables in a subset will be denoted by \\(p-1\\), as always, so that there are \\(p\\) parameters in the regression function for this subset of \\(X\\) variables. Thus, we have: \\[\n1\\le p \\le P &lt; n\n\\]"
  },
  {
    "objectID": "17_Criteria.html#coefficient-of-determination-and-sse",
    "href": "17_Criteria.html#coefficient-of-determination-and-sse",
    "title": "17  Model Comparison Criteria",
    "section": "17.3 Coefficient of Determination and SSE",
    "text": "17.3 Coefficient of Determination and SSE\nClearly, we would like models that fit the data well. Thus we would like a coefficient of multiple determiantion, \\(R^2\\), to be high.\nWe will denote the number of parameters in the potential model as a subscript and write the coefficient of determination as \\(R^2_p\\).\nThe \\(R^2_p\\) criterion is equivalent to using the error sum of squares \\(SSE_p\\) as the criterion. The \\(R^2_p\\) criterion is not intended to identify the subsets that maximize this criterion.\nWe know that \\(R^2_p\\) can never decrease as additional \\(X\\) variables are included in the model. Hence, \\(R^2_p\\) will be a maximum when all \\(P - 1\\) potential X variables are included in the regression model.\nThe intent in using the \\(R^2_p\\) criterion is to find the point where adding more \\(X\\) variables is not worthwhile because it leads to a very small increase in \\(R^2_p\\).\nOften, this point is reached when only a limited number of \\(X\\) variables are included in the regression model."
  },
  {
    "objectID": "17_Criteria.html#adjusted-coefficient-of-determination-and-mse",
    "href": "17_Criteria.html#adjusted-coefficient-of-determination-and-mse",
    "title": "17  Model Comparison Criteria",
    "section": "17.4 Adjusted Coefficient of Determination and MSE",
    "text": "17.4 Adjusted Coefficient of Determination and MSE\nSince \\(R^2_{p}\\) does not take account of the number of parameters in the regression model and since max(\\(R^2_{p}\\)) can never decrease as \\(p\\) increases, the adjusted coefficient of multiple determination \\(R^2_{a,p}\\) has been suggested as an alternative criterion.\nThis coefficient takes the number of parameters in the regression model into account through the degrees of freedom.\nUsers of the \\(R^2_{a,p}\\) criterion seek to find a few subsets for which \\(R^2_{a,p}\\) is at the maximum or so close to the maximum that adding more variables is not worthwhile.\nThe \\(R^2_{a,p}\\) criterion is equivalent to using the mean square error \\(SSE_p\\) as the criterion. This come from the relationship in Equation 13.7 \\[\n\\begin{align*}\nR_{a}^{2} & =1-\\frac{\\frac{SSE}{n-p}}{\\frac{SSTO}{n-1}}\\\\\n& =1-\\left(\\frac{n-1}{n-p}\\right)\\frac{SSE}{SSTO}\\\\\n& = 1-\\frac{MSE}{\\frac{SSTO}{n-1}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "17_Criteria.html#aic-and-bic",
    "href": "17_Criteria.html#aic-and-bic",
    "title": "17  Model Comparison Criteria",
    "section": "17.5 AIC and BIC",
    "text": "17.5 AIC and BIC\nWe have seen that \\(R_{a,p}^2\\) is a criterion that penalizes models having large numbers of predictors.\nTwo popular alternatives that also provide penalties for adding predictors are Akaike’s information criterion (\\(AIC_p\\)) and Schwarz’ Bayesian criterion (\\(SBC_p\\)).\nA more popular name for \\(SBC_p\\) is Bayesian information criterion (\\(BIC_p\\)).\nWe search for models that have small values of \\(AIC_p\\), or \\(BIC_p\\), where these criteria are given by: \\[\n\\begin{align}\nAIC_{p} & =n\\ln SSE_{p}-n\\ln n+2p\\\\\nBIC_{p} & =n\\ln SSE_{p}-n\\ln n+\\left(\\ln n\\right)p\n\\end{align}\n\\tag{17.1}\\]\nNotice that for both of these measures, the first term is \\[\nn\\ln SSE_{p}\n\\] which decreases as \\(p\\) increases.\nThe second term is fixed (for a given sample size \\(n\\)), and the third term increases with the number of parameters, \\(p\\).\nModels with small \\(SSE_p\\) will do well by these criteria as long as the penalties \\[\n\\begin{align*}\n2p & \\text{ for }AIC_p \\\\\n(\\ln n)p & \\text{ for }BIC_p\n\\end{align*}\n\\] are not too large.\nIf \\(n \\ge 8\\) the penalty for \\(BIC_p\\) is larger than that for \\(AIC_p\\); hence the \\(BIC_p\\) criterion tends to favor more parsimonious models.\nA parsimonious model is a model that accomplishes a desired level of explanation or prediction with as few predictor variables as possible.\nWe consider the “best” models the ones with the lowest \\(AIC_p\\) or \\(BIC_p\\)."
  },
  {
    "objectID": "17_Criteria.html#criteria-for-prediction",
    "href": "17_Criteria.html#criteria-for-prediction",
    "title": "17  Model Comparison Criteria",
    "section": "17.6 Criteria for Prediction",
    "text": "17.6 Criteria for Prediction\nThe \\(PRESS_p\\) (prediction sum of squares) criterion is a measure of how well the use of the fitted values for a subset model can predict the observed responses \\(y_i\\).\nThe PRESS measure differs from SSE in that each fitted value \\(\\hat{y}_i\\) for the PRESS criterion is obtained by\n\ndeleting the \\(i\\)th case from the data set\nestimating the regression function for the subset model from the remaining \\(n - 1\\) cases, and\nthen using the fitted regression function to obtain the predicted value \\(\\hat{y}_{i(i)}\\) for the \\(i\\)th case.\n\nWe use the notation \\(\\hat{y}_{i(i)}\\) now for the fitted value to indicate, by the first subscript \\(i\\), that it is a predicted value for the \\(i\\)th case and, by the second subscript \\((i)\\), that the \\(i\\)th case was omitted when the regression function was fitted.\nThe PRESS prediction error for the \\(i\\)th case then is: \\[\ny_i - \\hat{y}_{i(i)}\n\\] and the \\(PRESS_p\\) criterion is the sum of the squared prediction errors over all \\(n\\) cases: \\[\n\\begin{align}\nPRESS_p = \\sum_{i=1}^n\\left(y_i - \\hat{y}_{i(i)} \\right)^2\n\\end{align}\n\\tag{17.2}\\]\nModels with small \\(PRESS_p\\) values are considered good candidate models. The reason is that when the prediction errors \\(y_i - \\hat{y}_{i(i)}\\) are small, so are the squared prediction errors and the sum of the squared prediction errors.\nThus, models with small \\(PRESS_p\\) values fit well in the sense of having small prediction errors.\nAnother measure of prediction is the predicted \\(R^{2}\\). Is is related to PRESS by \\[\n\\begin{align}\n\\text{pred }R_{p}^{2} & =1-\\frac{PRESS_{p}}{SSTO}\n\\end{align}\n\\tag{17.3}\\] Large values of pred \\(R_{p}^{2}\\) indicates a model that is “good” at prediction.\n\nExample 17.1 We can obtain \\(R^2\\), \\(R^2_a\\), \\(AIC\\), and \\(BIC\\) using the glance function after fitting a model. Let’s examine the fit to mtcars in Example 16.2.\n\nlibrary(tidymodels)\n\ndat_recipe = recipe(mpg ~ disp + hp + drat + wt + qsec, data = mtcars) |&gt; \n  step_mutate(\n    log_disp = log(disp),\n    log_hp = log(hp),\n    log_wt = log(wt)\n  ) |&gt; \n  step_rm(disp, hp, wt)\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nwf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\nfit &lt;- wf |&gt; fit(mtcars)\n\nfit |&gt; glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.892         0.871  2.16      43.0 9.14e-12     5  -66.7  147.  158.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nTo obtain the PRESS statistic, we can use the PRESS function from the MPV library.\n\nlibrary(MPV)\nfit |&gt; extract_fit_engine() |&gt; PRESS()\n\n[1] 171.8721"
  },
  {
    "objectID": "18_Stepwise.html#best-subsets-procedure",
    "href": "18_Stepwise.html#best-subsets-procedure",
    "title": "18  Stepwise and Best Subsets Regression",
    "section": "18.1 Best Subsets Procedure",
    "text": "18.1 Best Subsets Procedure\nAs noted previously, the number of possible models, \\[\n2^{p-1}\n\\] grows rapidly with the number of predictors.\nEvaluating all of the possible alternatives can be a daunting endeavor. However, for small \\(p\\) and a sample size that is not too large, we can fit all the possible models. From all of the models, we can pick a few models that are close in some criterion that we can examine further.\nWhen the pool of potential predictor variables is very large, the “best” subset algorithms may require excessive computer time.\nUnder these conditions, one of the stepwise regression procedures, described next, may need to be employed to assist in the selection of predictor variables.\n\nExample 18.1 (Surgical Unit data) The surgical unit data is available from the olsrr library. This library also has functions for conducting subsets regression.\nThis data is originally from Kutner1. It consists of data about survival of patients undergoing a liver operation. The response variable is number of days the patient survived after the operation. We will actually model the natural log of the survival time. The predictor variables are:\n\nbcs: blood clotting score\npindex: prognistic index\nenzyme_test: enzyme function test score\nliver_test: liver function test score\nage: age, in years\ngender: indicator variable for gender (0=male, 1=female)\nalc_mod: indicator variable for history of alcohol use (1=Moderate, 0=otherwise)\nalc_heavy: indicator variable for history of alcohol use (1=Heavy, 0=otherwise)\n\nThe tidymodels library does not have functions to do subsets regression (we will use olsrr instead).\n\nlibrary(tidyverse)\nlibrary(olsrr)\nlibrary(GGally)\n\ndat = surgical |&gt; \n  mutate(\n    ln_y = log(y)\n  )\n\ndat |&gt; ggpairs()\n\n\n\n\nNo obvious nonlinear relationships seen from the scatterplot matrix.\nLet’s first fit all \\(2^8=258\\) models using the ols_step_all_possible function.\n\nfit = lm(ln_y ~ bcs + pindex + enzyme_test + liver_test+\n                      age + gender + alc_mod + alc_heavy, data = dat)\n\nfits = ols_step_all_possible(fit)\n\nplot(fits)\n\n\n\n\n\n\n\nFrom these plots, we can determine a few models that may be of interest to us. Suppose that we want the model 93 which appears to be the best model with four predictors by all of the criteria. We can find this model in the subsets with the following code.\n\nfits |&gt; \n  filter(mindex == 93)\n\n  Index N                       Predictors  R-Square Adj. R-Square Mallow's Cp\n1    93 4 bcs pindex enzyme_test alc_heavy 0.8299187     0.8160345    5.733992\n\n\nIf we want to just examine the best subset of predictors for each value of \\(p\\), then we can use the ols_step_best_subset function. This will return the criteria for only the best combination of predictors for each \\(p\\). The difference between ols_step_best_subset and ols_step_all_possible is that ols_step_all_possible does return every possible model whereas ols_step_best_subset returns only the best models.\n\nsubset = ols_step_best_subset(fit) \nsubset\n\n                           Best Subsets Regression                           \n-----------------------------------------------------------------------------\nModel Index    Predictors\n-----------------------------------------------------------------------------\n     1         enzyme_test                                                    \n     2         pindex enzyme_test                                             \n     3         pindex enzyme_test alc_heavy                                   \n     4         bcs pindex enzyme_test alc_heavy                               \n     5         bcs pindex enzyme_test gender alc_heavy                        \n     6         bcs pindex enzyme_test age gender alc_heavy                    \n     7         bcs pindex enzyme_test age gender alc_mod alc_heavy            \n     8         bcs pindex enzyme_test liver_test age gender alc_mod alc_heavy \n-----------------------------------------------------------------------------\n\n                                                   Subsets Regression Summary                                                   \n--------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                          \nModel    R-Square    R-Square    R-Square      C(p)        AIC        SBIC         SBC       MSEP      FPE       HSP       APC  \n--------------------------------------------------------------------------------------------------------------------------------\n  1        0.4273      0.4162      0.3496    117.4783    51.4343    -105.4395    57.4013    7.6160    0.1463    0.0028    0.6168 \n  2        0.6632      0.6500      0.6044     50.4918    24.7668    -131.5971    32.7228    4.5684    0.0893    0.0017    0.3765 \n  3        0.7780      0.7647      0.7291     18.9015     4.2432    -150.4023    14.1881    3.0718    0.0610    0.0012    0.2575 \n  4        0.8299      0.8160      0.7863      5.7340    -8.1306    -160.5329     3.8033    2.4030    0.0486     9e-04    0.2048 \n  5        0.8375      0.8205      0.7828      5.5282    -8.5803    -160.2288     5.3426    2.3453    0.0482     9e-04    0.2032 \n  6        0.8435      0.8235      0.7836      5.7725    -8.6129    -159.4064     7.2990    2.3077    0.0482     9e-04    0.2032 \n  7        0.8460      0.8226      0.7807      7.0288    -7.4974    -157.6344    10.4035    2.3207    0.0492    0.0010    0.2076 \n  8        0.8461      0.8187      0.7711      9.0000    -5.5320    -155.2573    14.3579    2.3719    0.0511    0.0010    0.2154 \n--------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n\nplot(subset)\n\n\n\n\n\n\n\nThis dataset does not take too long to run with all possible models. However, some datasets will have many more possible predictors and more observations. Using ols_step_all_possible will take too long. We can then use an automatic search method.\n\n\n18.1.1 Choosing a Subset of Models\nThe best subsets regressions procedure leads to the identification of a small number of subsets that are “good” according to a specified criterion.\nSometimes, one may wish to consider more than one criterion in evaluating possible subsets of predictor variables.\nOnce the investigator has identified a few “good” subsets for intensive examination, a final choice of the model variables must be made.\nThis choice is aided by examining outliers, checking model assumptions, and by the investigator’s knowledge of the subject under study, and is finally confirmed through model validation studies."
  },
  {
    "objectID": "18_Stepwise.html#stepwise-regression-procedures",
    "href": "18_Stepwise.html#stepwise-regression-procedures",
    "title": "18  Stepwise and Best Subsets Regression",
    "section": "18.2 Stepwise Regression Procedures",
    "text": "18.2 Stepwise Regression Procedures\n\n18.2.1 Stepwise Regression\nIn those occasional cases when the pool of potential predictor variables contains 30 to 40 or even more variables, use of a “best” subsets algorithm may not be feasible.\nAn automatic search procedure that develops the “best” subset of \\(X\\) variables sequentially may then be helpful.\nThe forward stepwise regression procedure is probably the most widely used of the automatic search methods.\nIt was developed to economize on computational efforts as compared with the various all-possible-regressions procedures. Essentially, this search method develops a sequence of regression models, at each step adding or deleting an \\(X\\) variable.\nThe criterion for adding or deleting an \\(X\\) variable can be stated equivalently in terms of error sum of squares reduction, \\(t^*\\) statistic, \\(F^*\\) statistic, AIC, or BIC.\n\n\n18.2.2 Limitations of Stepwise Methods\nAn essential difference between stepwise procedures and the “best” subsets algorithm is that stepwise search procedures end with the identification of a single regression model as “best.”\nWith the “best” subsets algorithm, on the other hand. several regression models can be identified as “good” for final consideration.\nThe identification of a single regression model as “best” by the stepwise procedures is a major weakness of these procedures.\nExperience has shown that each of the stepwise search procedures can sometimes err by identifying a suboptimal regression model as “best.”\nIn addition, the identification of a single regression model may hide the fact that several other regression models may also be “good.”\nFinally, the “goodness” of a regression model can only be established by a thorough examination using a variety of diagnostics.\nWhat then can we do on those occasions when the pool of potential \\(X\\) variables is very large and an automatic search procedure must be utilized? Basically, we should use the subset identified by the automatic search procedure as a starting point for searching for other “good” subsets.\nOne possibility is to treat the number of \\(X\\) variables in the regression model identified by the automatic search procedure as being about the right subset size and then use the “best” subsets procedure for subsets of this and nearby sizes.\n\n\n18.2.3 Forward Stepwise Regression\nWe shall describe the forward stepwise regression search algorithm in terms of the AIC statistic.\n\nThe stepwise regression routine first fits a simple linear regression model for each of the \\(P - 1\\) potential \\(X\\) variables.\nFor each simple linear regression model, the AIC statistic is obtained.\nThe X variable with the smallest AIC is the candidate for first addition.\nAssume \\(x_7\\) is the variable entered at step 1. The stepwise regression routine now fits all regression models with two \\(X\\) variables, where \\(x_7\\) is one of the pair.\nFor each such regression model, the AIC corresponding to the newly added predictor \\(x_k\\) is obtained.\nThe \\(X\\) variable with the smallest AIC is the candidate for addition at the second stage.\nIf the AIC is smaller than AIC for model in the previous step (in this case, the model with only \\(x_7\\)), then that variable is added to the model that already has \\(x_7\\). Otherwise, the program terminates.\nSuppose \\(x_3\\) is added at the second stage. Now the stepwise regression routine examines whether any of the other \\(X\\) variables already in the model should be dropped.\nFor our illustration, there is at this stage only one other \\(X\\) variable in the model, \\(x_7\\). At later stages, there would be a number of variables in the model besides the one last added.\nThe variable with the largest AIC is the candidate for deletion. If this AIC exceeds the AIC for when the variable is not in the model, the variable is dropped from the model; otherwise, it is retained.\nSuppose \\(x_7\\) is retained so that both \\(x_3\\) and \\(x_7\\) are now in the model.\nThe stepwise regression routine now examines which \\(X\\) variable is the next candidate for addition, then examines whether any of the variables already in the model should now be dropped, and so on until no further \\(X\\) variables can either be added or deleted, at which point the search terminates.\n\nNote that the stepwise regression algorithm allows a predictor variable, brought into the model at an earlier stage, to be dropped subsequently if it is no longer helpful in conjunction with variables added at later stages.\n\n\n18.2.4 Other Stepwise Procedures\nOther stepwise procedures are available to find a ``best” subset of predictor variables. We mention two of these.\n\nForward Selection\nThe forward selection search procedure is a simplified version of forward stepwise regression, omitting the test whether a variable once entered into the model should be dropped.\n\n\nBackward Elimination\nThe backward elimination search procedure is the opposite of forward selection.\nIt begins with the model containing all potential \\(X\\) variables and identifies the one with the largest AIC. If the maximum AIC is greater than the full model, that \\(X\\) variable is dropped.\nThe model with the remaining \\(P - 2\\) \\(X\\) variables is then fitted, and the next candidate for dropping is identified.\nThis process continues until no further \\(X\\) variables can be dropped.\nA stepwise modification can also be adapted that allows variables eliminated earlier to be added later: this modification is called the backward stepwise regression procedure.\n\nExample 18.2 (Example 18.1 revisted) Let use forward stepwise to find a model.\n\nlibrary(tidyverse)\nlibrary(olsrr)\n\ndat = surgical |&gt; \n  mutate(\n    ln_y = log(y)\n  )\n\nforward_step = ols_step_both_aic(fit, details = TRUE)\n\nStepwise Selection Method \n-------------------------\n\nCandidate Terms: \n\n1 . bcs \n2 . pindex \n3 . enzyme_test \n4 . liver_test \n5 . age \n6 . gender \n7 . alc_mod \n8 . alc_heavy \n\n Step 0: AIC = 79.52928 \n ln_y ~ 1 \n\n\nVariables Entered/Removed: \n\n                          Enter New Variables                        \n---------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS      R-Sq     Adj. R-Sq \n---------------------------------------------------------------------\nenzyme_test     1    51.434     5.471     7.334    0.427        0.416 \nliver_test      1    51.977     5.397     7.408    0.421        0.410 \npindex          1    68.040     2.830     9.974    0.221        0.206 \nalc_heavy       1    73.443     1.781    11.024    0.139        0.123 \nbcs             1    78.149     0.777    12.028    0.061        0.043 \ngender          1    78.543     0.689    12.116    0.054        0.036 \nage             1    80.381     0.269    12.535    0.021        0.002 \nalc_mod         1    80.651     0.207    12.598    0.016       -0.003 \n---------------------------------------------------------------------\n\n- enzyme_test added \n\n\n Step 1 : AIC = 51.43434 \n ln_y ~ enzyme_test \n\n                         Enter New Variables                       \n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\npindex         1    24.767     8.492    4.313    0.663        0.650 \nliver_test     1    34.156     7.673    5.132    0.599        0.583 \nbcs            1    40.602     7.022    5.783    0.548        0.531 \nalc_heavy      1    44.323     6.609    6.195    0.516        0.497 \ngender         1    51.499     5.729    7.075    0.447        0.426 \nage            1    51.645     5.710    7.095    0.446        0.424 \nalc_mod        1    52.947     5.537    7.268    0.432        0.410 \n-------------------------------------------------------------------\n\n- pindex added \n\n\n Step 2 : AIC = 24.76682 \n ln_y ~ enzyme_test + pindex \n\n                     Remove Existing Variables                      \n--------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n--------------------------------------------------------------------\npindex          1    51.434     5.471    7.334    0.427        0.416 \nenzyme_test     1    68.040     2.830    9.974    0.221        0.206 \n--------------------------------------------------------------------\n\n                         Enter New Variables                       \n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nalc_heavy      1     4.243     9.963    2.842    0.778        0.765 \nbcs            1     9.084     9.696    3.109    0.757        0.743 \nliver_test     1    17.234     9.190    3.615    0.718        0.701 \nalc_mod        1    23.834     8.720    4.085    0.681        0.662 \nage            1    24.663     8.656    4.148    0.676        0.657 \ngender         1    25.727     8.574    4.231    0.670        0.650 \n-------------------------------------------------------------------\n\n- alc_heavy added \n\n\n Step 3 : AIC = 4.243223 \n ln_y ~ enzyme_test + pindex + alc_heavy \n\n                     Remove Existing Variables                      \n--------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n--------------------------------------------------------------------\nalc_heavy       1    24.767     8.492    4.313    0.663        0.650 \npindex          1    44.323     6.609    6.195    0.516        0.497 \nenzyme_test     1    56.640     5.022    7.782    0.392        0.368 \n--------------------------------------------------------------------\n\n                         Enter New Variables                       \n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nbcs            1    -8.131    10.627    2.178    0.830        0.816 \nliver_test     1    -3.424    10.428    2.376    0.814        0.799 \ngender         1     3.572    10.100    2.705    0.789        0.772 \nage            1     4.879    10.033    2.771    0.784        0.766 \nalc_mod        1     5.783     9.987    2.818    0.780        0.762 \n-------------------------------------------------------------------\n\n- bcs added \n\n\n Step 4 : AIC = -8.130569 \n ln_y ~ enzyme_test + pindex + alc_heavy + bcs \n\n                     Remove Existing Variables                      \n--------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n--------------------------------------------------------------------\nbcs             1     4.243     9.963    2.842    0.778        0.765 \nalc_heavy       1     9.084     9.696    3.109    0.757        0.743 \npindex          1    36.524     7.638    5.167    0.596        0.572 \nenzyme_test     1    57.529     5.181    7.624    0.405        0.369 \n--------------------------------------------------------------------\n\n                         Enter New Variables                       \n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\ngender         1    -8.580    10.723    2.081    0.837        0.821 \nage            1    -8.048    10.703    2.102    0.836        0.819 \nliver_test     1    -7.170    10.668    2.136    0.833        0.816 \nalc_mod        1    -6.689    10.649    2.155    0.832        0.814 \n-------------------------------------------------------------------\n\n- gender added \n\n\n Step 5 : AIC = -8.580332 \n ln_y ~ enzyme_test + pindex + alc_heavy + bcs + gender \n\n                     Remove Existing Variables                      \n--------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n--------------------------------------------------------------------\ngender          1    -8.131    10.627    2.178    0.830        0.816 \nbcs             1     3.572    10.100    2.705    0.789        0.772 \nalc_heavy       1    10.176     9.748    3.057    0.761        0.742 \npindex          1    35.768     7.895    4.910    0.617        0.585 \nenzyme_test     1    56.105     5.649    7.155    0.441        0.396 \n--------------------------------------------------------------------\n\n                         Enter New Variables                       \n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nage            1    -8.613    10.800    2.004    0.843        0.823 \nalc_mod        1    -7.151    10.745    2.059    0.839        0.819 \nliver_test     1    -7.005    10.740    2.065    0.839        0.818 \n-------------------------------------------------------------------\n\n- age added \n\n\n Step 6 : AIC = -8.612898 \n ln_y ~ enzyme_test + pindex + alc_heavy + bcs + gender + age \n\n                     Remove Existing Variables                      \n--------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n--------------------------------------------------------------------\nage             1    -8.580    10.723    2.081    0.837        0.821 \ngender          1    -8.048    10.703    2.102    0.836        0.819 \nbcs             1     4.113    10.172    2.633    0.794        0.773 \nalc_heavy       1     9.435     9.899    2.905    0.773        0.749 \npindex          1    36.193     8.036    4.769    0.628        0.589 \nenzyme_test     1    57.529     5.725    7.080    0.447        0.390 \n--------------------------------------------------------------------\n\n                         Enter New Variables                       \n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nalc_mod        1    -7.497    10.833    1.972    0.846        0.823 \nliver_test     1    -6.673    10.802    2.002    0.844        0.820 \n-------------------------------------------------------------------\n\n\nNo more variables to be added or removed.\n\nFinal Model Output \n------------------\n\n                        Model Summary                         \n-------------------------------------------------------------\nR                       0.918       RMSE               0.207 \nR-Squared               0.843       Coef. Var          3.211 \nAdj. R-Squared          0.823       MSE                0.043 \nPred R-Squared          0.784       MAE                0.162 \n-------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression     10.800         6          1.800    42.209    0.0000 \nResidual        2.004        47          0.043                     \nTotal          12.805        53                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                   \n---------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n---------------------------------------------------------------------------------------\n(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 \nenzyme_test     0.015         0.001        0.653    10.909    0.000     0.012    0.018 \n     pindex     0.014         0.002        0.473     8.051    0.000     0.010    0.017 \n  alc_heavy     0.351         0.076        0.280     4.597    0.000     0.197    0.505 \n        bcs     0.072         0.019        0.233     3.839    0.000     0.034    0.109 \n     gender     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 \n        age    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 \n---------------------------------------------------------------------------------------\n\n\nLet’s now use forward selection:\n\nlibrary(tidyverse)\nlibrary(olsrr)\n\nforward_sel = ols_step_forward_aic(fit, details = TRUE)\n\nForward Selection Method \n------------------------\n\nCandidate Terms: \n\n1 . bcs \n2 . pindex \n3 . enzyme_test \n4 . liver_test \n5 . age \n6 . gender \n7 . alc_mod \n8 . alc_heavy \n\n Step 0: AIC = 79.52928 \n ln_y ~ 1 \n\n---------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS      R-Sq     Adj. R-Sq \n---------------------------------------------------------------------\nenzyme_test     1    51.434     5.471     7.334    0.427        0.416 \nliver_test      1    51.977     5.397     7.408    0.421        0.410 \npindex          1    68.040     2.830     9.974    0.221        0.206 \nalc_heavy       1    73.443     1.781    11.024    0.139        0.123 \nbcs             1    78.149     0.777    12.028    0.061        0.043 \ngender          1    78.543     0.689    12.116    0.054        0.036 \nage             1    80.381     0.269    12.535    0.021        0.002 \nalc_mod         1    80.651     0.207    12.598    0.016       -0.003 \n---------------------------------------------------------------------\n\n\n- enzyme_test \n\n\n Step 1 : AIC = 51.43434 \n ln_y ~ enzyme_test \n\n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\npindex         1    24.767     3.021    4.313    0.663        0.650 \nliver_test     1    34.156     2.202    5.132    0.599        0.583 \nbcs            1    40.602     1.551    5.783    0.548        0.531 \nalc_heavy      1    44.323     1.139    6.195    0.516        0.497 \ngender         1    51.499     0.258    7.075    0.447        0.426 \nage            1    51.645     0.239    7.095    0.446        0.424 \nalc_mod        1    52.947     0.066    7.268    0.432        0.410 \n-------------------------------------------------------------------\n\n- pindex \n\n\n Step 2 : AIC = 24.76682 \n ln_y ~ enzyme_test + pindex \n\n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nalc_heavy      1     4.243     1.471    2.842    0.778        0.765 \nbcs            1     9.084     1.204    3.109    0.757        0.743 \nliver_test     1    17.234     0.698    3.615    0.718        0.701 \nalc_mod        1    23.834     0.228    4.085    0.681        0.662 \nage            1    24.663     0.165    4.148    0.676        0.657 \ngender         1    25.727     0.082    4.231    0.670        0.650 \n-------------------------------------------------------------------\n\n- alc_heavy \n\n\n Step 3 : AIC = 4.243223 \n ln_y ~ enzyme_test + pindex + alc_heavy \n\n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nbcs            1    -8.131     0.664    2.178    0.830        0.816 \nliver_test     1    -3.424     0.466    2.376    0.814        0.799 \ngender         1     3.572     0.137    2.705    0.789        0.772 \nage            1     4.879     0.071    2.771    0.784        0.766 \nalc_mod        1     5.783     0.024    2.818    0.780        0.762 \n-------------------------------------------------------------------\n\n- bcs \n\n\n Step 4 : AIC = -8.130569 \n ln_y ~ enzyme_test + pindex + alc_heavy + bcs \n\n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\ngender         1    -8.580     0.097    2.081    0.837        0.821 \nage            1    -8.048     0.076    2.102    0.836        0.819 \nliver_test     1    -7.170     0.042    2.136    0.833        0.816 \nalc_mod        1    -6.689     0.022    2.155    0.832        0.814 \n-------------------------------------------------------------------\n\n- gender \n\n\n Step 5 : AIC = -8.580332 \n ln_y ~ enzyme_test + pindex + alc_heavy + bcs + gender \n\n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nage            1    -8.613     0.077    2.004    0.843        0.823 \nalc_mod        1    -7.151     0.022    2.059    0.839        0.819 \nliver_test     1    -7.005     0.016    2.065    0.839        0.818 \n-------------------------------------------------------------------\n\n- age \n\n\n Step 6 : AIC = -8.612898 \n ln_y ~ enzyme_test + pindex + alc_heavy + bcs + gender + age \n\n-------------------------------------------------------------------\nVariable      DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nalc_mod        1    -7.497     0.033    1.972    0.846        0.823 \nliver_test     1    -6.673     0.002    2.002    0.844        0.820 \n-------------------------------------------------------------------\n\n\nNo more variables to be added.\n\nVariables Entered: \n\n- enzyme_test \n- pindex \n- alc_heavy \n- bcs \n- gender \n- age \n\n\nFinal Model Output \n------------------\n\n                        Model Summary                         \n-------------------------------------------------------------\nR                       0.918       RMSE               0.207 \nR-Squared               0.843       Coef. Var          3.211 \nAdj. R-Squared          0.823       MSE                0.043 \nPred R-Squared          0.784       MAE                0.162 \n-------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression     10.800         6          1.800    42.209    0.0000 \nResidual        2.004        47          0.043                     \nTotal          12.805        53                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                   \n---------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n---------------------------------------------------------------------------------------\n(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 \nenzyme_test     0.015         0.001        0.653    10.909    0.000     0.012    0.018 \n     pindex     0.014         0.002        0.473     8.051    0.000     0.010    0.017 \n  alc_heavy     0.351         0.076        0.280     4.597    0.000     0.197    0.505 \n        bcs     0.072         0.019        0.233     3.839    0.000     0.034    0.109 \n     gender     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 \n        age    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 \n---------------------------------------------------------------------------------------\n\n\nNotice that forward selection results in the same model as forward stepwise.\nLet’s now try backward elimnation.\n\nbackward_elim = ols_step_backward_aic(fit, details = TRUE)\n\nBackward Elimination Method \n---------------------------\n\nCandidate Terms: \n\n1 . bcs \n2 . pindex \n3 . enzyme_test \n4 . liver_test \n5 . age \n6 . gender \n7 . alc_mod \n8 . alc_heavy \n\n Step 0: AIC = -5.531982 \n ln_y ~ bcs + pindex + enzyme_test + liver_test + age + gender + alc_mod + alc_heavy \n\n--------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n--------------------------------------------------------------------\nliver_test     1     -7.497     0.001    1.972    0.846        0.823 \nalc_mod        1     -6.673     0.032    2.002    0.844        0.820 \nage            1     -5.552     0.074    2.044    0.840        0.816 \ngender         1     -5.277     0.084    2.055    0.840        0.815 \nbcs            1      0.557     0.318    2.289    0.821        0.794 \nalc_heavy      1     11.736     0.845    2.815    0.780        0.747 \npindex         1     31.549     2.093    4.063    0.683        0.634 \nenzyme_test    1     42.307     2.989    4.959    0.613        0.554 \n--------------------------------------------------------------------\n\n\nVariables Removed: \n\n- liver_test \n\n\n  Step 1 : AIC = -7.497389 \n ln_y ~ bcs + pindex + enzyme_test + age + gender + alc_mod + alc_heavy \n\n--------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n--------------------------------------------------------------------\nalc_mod        1     -8.613     0.033    2.004    0.843        0.823 \nage            1     -7.151     0.088    2.059    0.839        0.819 \ngender         1     -6.907     0.097    2.069    0.838        0.818 \nbcs            1      5.410     0.627    2.599    0.797        0.771 \nalc_heavy      1      9.739     0.844    2.816    0.780        0.752 \npindex         1     36.799     2.675    4.647    0.637        0.591 \nenzyme_test    1     59.420     5.093    7.065    0.448        0.378 \n--------------------------------------------------------------------\n\n- alc_mod \n\n\n  Step 2 : AIC = -8.612898 \n ln_y ~ bcs + pindex + enzyme_test + age + gender + alc_heavy \n\n--------------------------------------------------------------------\nVariable       DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n--------------------------------------------------------------------\nage            1     -8.580     0.077    2.081    0.837        0.821 \ngender         1     -8.048     0.097    2.102    0.836        0.819 \nbcs            1      4.113     0.628    2.633    0.794        0.773 \nalc_heavy      1      9.435     0.901    2.905    0.773        0.749 \npindex         1     36.193     2.764    4.769    0.628        0.589 \nenzyme_test    1     57.529     5.075    7.080    0.447        0.390 \n--------------------------------------------------------------------\n\n\nNo more variables to be removed.\n\nVariables Removed: \n\n- liver_test \n- alc_mod \n\n\nFinal Model Output \n------------------\n\n                        Model Summary                         \n-------------------------------------------------------------\nR                       0.918       RMSE               0.207 \nR-Squared               0.843       Coef. Var          3.211 \nAdj. R-Squared          0.823       MSE                0.043 \nPred R-Squared          0.784       MAE                0.162 \n-------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression     10.800         6          1.800    42.209    0.0000 \nResidual        2.004        47          0.043                     \nTotal          12.805        53                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                   \n---------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n---------------------------------------------------------------------------------------\n(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 \n        bcs     0.072         0.019        0.233     3.839    0.000     0.034    0.109 \n     pindex     0.014         0.002        0.473     8.051    0.000     0.010    0.017 \nenzyme_test     0.015         0.001        0.653    10.909    0.000     0.012    0.018 \n        age    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 \n     gender     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 \n  alc_heavy     0.351         0.076        0.280     4.597    0.000     0.197    0.505 \n---------------------------------------------------------------------------------------\n\n\nNote that olsrr does not provide the ability to do backward stepwise."
  },
  {
    "objectID": "18_Stepwise.html#footnotes",
    "href": "18_Stepwise.html#footnotes",
    "title": "18  Stepwise and Best Subsets Regression",
    "section": "",
    "text": "Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied Linear Statistical Models McGraw-Hill/lrwin series operations and decision sciences.↩︎"
  },
  {
    "objectID": "19_Ridge.html#ridge-regression",
    "href": "19_Ridge.html#ridge-regression",
    "title": "19  Ridge Regression and the LASSO",
    "section": "19.1 Ridge Regression",
    "text": "19.1 Ridge Regression\nWe have seen that multicollinearity causes the least squares estimates to be imprecise.\nIf we allow some bias in our estimators, then we can have estimators that are more precise. This is due to the relationship \\[\n\\begin{align*}\nE\\left[\\left(\\hat{\\beta}-\\beta\\right)^{2}\\right] & =Var\\left[\\hat{\\beta}\\right]+\\left(E\\left[\\hat{\\beta}\\right]-\\beta\\right)^{2}\n\\end{align*}\n\\]\nRidge regression starts by transforming the variables as \\[\n\\begin{align}\nY_{i}^{*} & =\\frac{1}{\\sqrt{n-1}}\\left(\\frac{Y_{i}-\\overline{Y}}{s_{Y}}\\right)\\nonumber\\\\\nX_{ik}^{*} & =\\frac{1}{\\sqrt{n-1}}\\left(\\frac{X_{ik}-\\overline{X}_{k}}{s_{k}}\\right)\\label{eq:w5_23}\n\\end{align}\n\\] This is known as the correlation transformation.\nThe estimates are then found by penalizing the least squares criterion \\[\n\\begin{align}\nQ & =\\sum\\left[Y_{i}^{*}-\\left(b_{1}^{*}X_{i1}^{*}+\\cdots+b_{p-1}^{*}X_{i,p-1}^{*}\\right)\\right]^{2}+\\lambda\\left[\\sum_{j=1}^{p-1}\\left(b_{j}^{*}\\right)^{2}\\right]\n\\end{align}\n\\tag{19.1}\\]\nThe added term gives a penalty for large coefficients. Thus, the estimators are biased toward 0. Because of this, the ridge estimators are called shrinkage estimators.\nThere are a number of methods for determining \\(\\lambda\\) which are beyond the scope of our class. In R, we will use the glmnet engine to fit the model.\nThe ridge estimators are more stable than the ols estimators, however, the distributional properties of these estimators are not easily found.\n\nExample 19.1 (Ridge Regression for mtcars) Let’s examine the mtcars data last seen in Example 16.3. Recall that disp, hp, and wt need to be log-transformed to make the relationships linear.\nWe will now use the “glmnet” engine for ridge regression. We set the value of \\(\\lambda\\) in Equation 19.1 with the penalty argument in linear_reg. We will just arbitrarily use 0.1 for now. We will later in this section discuss how to choose a value of penalty.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndat_recipe = recipe(mpg ~ disp + hp + drat + wt + qsec, data = mtcars) |&gt; \n  step_mutate(\n    log_disp = log(disp),\n    log_hp = log(hp),\n    log_wt = log(wt)\n  ) |&gt; \n  step_rm(disp, hp, wt) |&gt; \n  step_normalize(all_numeric_predictors())\n\n#penalty is the lambda hyperparameter\n#mixture = 0 indicates ridge regression\nlm_model = linear_reg(mixture = 0, penalty = 0.1) |&gt;\n  set_engine(\"glmnet\")\n\nwf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\nfit = wf |&gt; fit(mtcars)\n\nfit |&gt; tidy()\n\n# A tibble: 6 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   20.1       0.1\n2 drat           0.283     0.1\n3 qsec           0.362     0.1\n4 log_disp      -1.27      0.1\n5 log_hp        -1.54      0.1\n6 log_wt        -2.64      0.1\n\n\nWe see that log_wt has the biggest impact on mpg since it has the largest coefficient in magnitude. The negative sign just indicates that the linear relationship between log_wt and mpg is negative.\nWe also see that drat has the smallest impact on mpg given the other variables are in the model, followed by qsec for the second smallest impact. Let’s reexamine the scatterplot matrix to see if this make sense.\n\nlibrary(GGally)\n\nmtcars |&gt;\n  mutate(log_disp = log(disp),\n         log_hp = log(hp),\n         log_wt = log(wt)) |&gt; \n  select(mpg, drat, qsec, log_disp, log_hp, log_wt) |&gt; \n  ggpairs()\n\n\n\n\nLooking at the plots of mpg versus the other variables, we see that a linear relationship is apparent for all of the predictors. Between drat and qsec, drat appears stronger (\\(r=0.681\\)) than qsec (\\(r=0.419\\)). But why did ridge regression show us that qsec has a larger impact on mpg than drat? Ridge regression shows the impact given the other variables are included in the model. Looking at the scatterplot matrix, drat is more correlated with the other predictor variables than qsec. Thus, when every other predictor is included, drat does not explain much more variability in mpg. Thus, it is not as important.\n\n\n19.1.1 The effect of the penalty\nIn Example 19.1, we chose the penalty (\\(\\lambda\\)) arbitrarily to be 0.1. Are there other values of penalty that would fit the data better?\nWe can try different penalties using the tidy function:\n\nfit |&gt; tidy(penalty = .75)\n\n# A tibble: 6 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   20.1      0.75\n2 drat           0.348    0.75\n3 qsec           0.362    0.75\n4 log_disp      -1.34     0.75\n5 log_hp        -1.51     0.75\n6 log_wt        -2.48     0.75\n\n\nWe can even plot the value of each coefficient for different penalties.\n\nfit |&gt; extract_fit_parsnip() |&gt; autoplot()\n\n\n\n\nWe see that for some penalties, the importance of some coefficients change.\nLet’s use cross-validation to help determine the penalty.\n\n\n19.1.2 Cross-Validation\nCross-validation is a powerful resampling method used to evaluate the performance of models while minimizing bias and variance. Within the tidymodels framework, this technique ensures that models generalize well to unseen data by fitting and validating on different subsets of the dataset.\n\nTypes of Cross-Validation\n\nK-Fold Cross-Validation:\n\nThe data is divided into k equal-sized folds (or partitions).\nThe model is fit on \\(k - 1\\) folds and validated on the remaining fold.\nThis process is repeated k times, with each fold serving as the validation set once.\nFinal performance is computed by averaging metrics across all folds.\n\nRepeated K-Fold Cross-Validation:\n\nThis variant involves running k-fold cross-validation multiple times with different splits, which provides more reliable estimates by averaging over several repetitions.\n\nLeave-One-Out Cross-Validation (LOOCV):\n\nEach observation in the dataset serves as the validation set exactly once. This method can be computationally expensive but works well for small datasets.\n\n\n\n\nImplementing Cross-Validation in tidymodels\nTo implement cross-validation, the rsample package (part of tidymodels) provides essential tools for splitting the data. Below is an example of performing 5-fold cross-validation:\n\nset.seed(1004) #to reproduce results\n\n\ncv_folds = vfold_cv(mtcars, v = 5)\n\n\n\nUsing Cross-Validation with Workflows\nWithin tidymodels, you can streamline model fitting with workflows and use fit_resamples() to fit models on the resamples created by cross-validation:\n\ndat_recipe = recipe(mpg ~ disp + hp + drat + wt + qsec, data = mtcars) |&gt; \n  step_mutate(\n    log_disp = log(disp),\n    log_hp = log(hp),\n    log_wt = log(wt)\n  ) |&gt; \n  step_rm(disp, hp, wt) |&gt; \n  step_normalize(all_numeric_predictors())\n\nlm_model = linear_reg(mixture = 0, penalty = 0.1) |&gt;\n  set_engine(\"glmnet\")\n\nwf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\nfits = wf |&gt; fit_resamples(wf, resamples = cv_folds)\n\n# Collect and summarize performance metrics\ncollect_metrics(fits)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.16      5  0.144  Preprocessor1_Model1\n2 rsq     standard   0.886     5  0.0200 Preprocessor1_Model1\n\n\n\n\n\n19.1.3 Tune the Penalty\nWe can now have tidymodels try different values of penalty and determine what is the best by checking the performance on the folded data. Using the folded data helps us determine what value of penalty provides the best fit but also what value does the best job at predicting data that was not used to determine the penalty.\nWe need to setup some values of penalty to try. These values can be constructed using grid_regular. After constructing the grid of possible values of penalty to try, we can then have tidymodels do all of the fitting using tune_grid.\nWe will put all of these pieces together in the following example.\n\nExample 19.2 (Fine tune the penalty for mtcars)  \n\nset.seed(1004) #to reproduce results\n\n#prepare the recipe\ndat_recipe = recipe(mpg ~ disp + hp + drat + wt + qsec, data = mtcars) |&gt; \n  step_mutate(\n    log_disp = log(disp),\n    log_hp = log(hp),\n    log_wt = log(wt)\n  ) |&gt; \n  step_rm(disp, hp, wt) |&gt; \n  step_normalize(all_numeric_predictors())\n\n#setup the folded data, let's try 5 folds\ncv_folds = vfold_cv(mtcars, v = 5)\n\n#setup the model and set penalty to tune\nlm_model = linear_reg(mixture = 0, penalty = tune()) |&gt;\n  set_engine(\"glmnet\")\n\nwf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#setup possible values of penalty\n#this will try 100 different values of penalty\npenalty_grid = grid_regular(penalty(range=c(-6, 4)), levels = 100)\n\n#now tune the grid with everthing we just set up\ntune_model = tune_grid(\n  wf,\n  resamples = cv_folds, \n  grid = penalty_grid,\n  metrics = metric_set(rmse, rsq)\n)\n\nAt the point, differing values of penalty has been tried as determined by grid_regular. We can view the results in a few ways. We can plot the mean metric for each value of the penalty, or we can just ask R to show the top few penalties based on either “rmse” or “rsq”.\n\n#plot the metrics for different values of the penalty\ntune_model |&gt;   autoplot()\n\n\n\n#find the best penalty in terms of rmse\ntune_model |&gt; show_best(metric = \"rmse\")\n\n# A tibble: 5 × 7\n     penalty .metric .estimator  mean     n std_err .config               \n       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000001   rmse    standard    2.30     5   0.283 Preprocessor1_Model001\n2 0.00000126 rmse    standard    2.30     5   0.283 Preprocessor1_Model002\n3 0.00000159 rmse    standard    2.30     5   0.283 Preprocessor1_Model003\n4 0.00000201 rmse    standard    2.30     5   0.283 Preprocessor1_Model004\n5 0.00000254 rmse    standard    2.30     5   0.283 Preprocessor1_Model005\n\n#find the best penalty in terms of rsq\ntune_model |&gt; show_best(metric = \"rsq\")\n\n# A tibble: 5 × 7\n     penalty .metric .estimator  mean     n std_err .config               \n       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.572      rsq     standard   0.924     5  0.0139 Preprocessor1_Model058\n2 0.000001   rsq     standard   0.924     5  0.0137 Preprocessor1_Model001\n3 0.00000126 rsq     standard   0.924     5  0.0137 Preprocessor1_Model002\n4 0.00000159 rsq     standard   0.924     5  0.0137 Preprocessor1_Model003\n5 0.00000201 rsq     standard   0.924     5  0.0137 Preprocessor1_Model004\n\n\nNote the “best” penalty value differs from “rmse” to “rsq”. Looking at the plot above, we can see there is not much difference in either metric for the “best” penalty value for either case.\nLet’s now get the best penalty from one of the metrics. We will use “rsq” here.\n\nbest_penalty = tune_model |&gt; select_best(metric = \"rsq\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config               \n    &lt;dbl&gt; &lt;chr&gt;                 \n1   0.572 Preprocessor1_Model058\n\n\nWith this best penalty, we can finalize the workflow to use this penalty for the ridge regression.\n\nfinal_wf = finalize_workflow(wf, best_penalty)\n\nfinal_fit = fit(final_wf, data = mtcars)\n\nfinal_fit |&gt; tidy()\n\n# A tibble: 6 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   20.1     0.572\n2 drat           0.294   0.572\n3 qsec           0.362   0.572\n4 log_disp      -1.29    0.572\n5 log_hp        -1.53    0.572\n6 log_wt        -2.61    0.572"
  },
  {
    "objectID": "19_Ridge.html#lasso",
    "href": "19_Ridge.html#lasso",
    "title": "19  Ridge Regression and the LASSO",
    "section": "19.2 Lasso",
    "text": "19.2 Lasso\n\n19.2.1 Shrinkage as a Variable Selector\nIn ridge regression, the estimates shrink to zero for predictors that do not have a significant linear relationship on \\(Y\\) given the other variables.\nThe coefficient estimates shrink to zero, but do not equal zero.\nThe lasso (least absolute shrinkage and selection operator) is a shrinkage method like ridge, with subtle but important differences.\nThe lasso estimate is defined by \\[\n\\begin{align}\nQ_{lasso} & =\\sum_{i=1}^{n}\\left(Y_{i}-\\left(\\beta_{0}+\\beta_{1}X_{1i}+\\cdots+\\beta_{p-1}X_{p-1,i}\\right)\\right)^{2}+\\lambda\\sum_{j=1}^{p-1}\\left|\\beta_{j}\\right|\n\\end{align}\n\\tag{19.2}\\]\nSimilarly to ridge regression, the value of \\(\\lambda\\) determines how biased the estimates will be.\nAs \\(\\lambda\\) increases, the beta coefficients shrink toward zero with the least associated beta coefficients decreasing all the way to 0 before the more strongly associated beta coefficients.\nAs a result, numerous beta coefficients that are not strongly associated with the outcome are decreased to zero, which is equivalent to removing those variables from the model.\nIn this way, the lasso can be used as a variable selection method.\n\nExample 19.3 (Lasso for mtcars) The procedure to fit the lasso is very similar to ridge regression. The only change is in the mixture argument in linear_reg. We will set this to 1 to do lasso.\n\nset.seed(1004) #to reproduce results\n\n#prepare the recipe\ndat_recipe = recipe(mpg ~ disp + hp + drat + wt + qsec, data = mtcars) |&gt; \n  step_mutate(\n    log_disp = log(disp),\n    log_hp = log(hp),\n    log_wt = log(wt)\n  ) |&gt; \n  step_rm(disp, hp, wt) |&gt; \n  step_normalize(all_numeric_predictors())\n\n#setup the folded data, let's try 5 folds\ncv_folds = vfold_cv(mtcars, v = 5)\n\n#setup the model and set penalty to tune\nlm_model = linear_reg(mixture = 1, penalty = tune()) |&gt;\n  set_engine(\"glmnet\")\n\nwf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#setup possible values of penalty\n#this will try 100 different values of penalty\npenalty_grid = grid_regular(penalty(range=c(-4, 4)), levels = 100)\n\n#now tune the grid with everything we just set up\ntune_model = tune_grid(\n  wf,\n  resamples = cv_folds, \n  grid = penalty_grid,\n  metrics = metric_set(rmse, rsq)\n)\n\ntune_model |&gt;   autoplot()\n\n\n\nbest_penalty = tune_model |&gt; select_best(metric = \"rsq\")\n\nfinal_wf = finalize_workflow(wf, best_penalty)\n\nfinal_fit = fit(final_wf, data = mtcars)\n\nfinal_fit |&gt; tidy()\n\n# A tibble: 6 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   20.1     0.359\n2 drat           0       0.359\n3 qsec           0       0.359\n4 log_disp      -0.910   0.359\n5 log_hp        -1.86    0.359\n6 log_wt        -2.88    0.359\n\n\nWe see in this dataset, drat and qsec shrunk to 0. This indicates that they are not important to modeling mpg. The largest magnitude is log_wt which indicates that it has the largest impact on mpg\nIn the next example, we will fit a lasso model to the bodyfat data.\n\n\nExample 19.4 (Lasso for the bodyfat data.)  \n\ndat = read_table(\"bodyfat.txt\")\n\n#prepare the recipe\ndat_recipe = recipe(bfat ~ tri + thigh + midarm, data = dat) |&gt; \n  step_normalize(all_numeric_predictors())\n\n#setup the folded data, let's try 5 folds\ncv_folds = vfold_cv(dat, v = 5)\n\n#setup the model and set penalty to tune\nlm_model = linear_reg(mixture = 1, penalty = tune()) |&gt;\n  set_engine(\"glmnet\")\n\nwf = workflow() |&gt; \n  add_recipe(dat_recipe) |&gt; \n  add_model(lm_model)\n\n#setup possible values of penalty\n#this will try 100 different values of penalty\npenalty_grid = grid_regular(penalty(range=c(-4, 4)), levels = 100)\n\n#now tune the grid with everything we just set up\ntune_model = tune_grid(\n  wf,\n  resamples = cv_folds, \n  grid = penalty_grid,\n  metrics = metric_set(rmse, rsq)\n)\n\ntune_model |&gt; autoplot()\n\n\n\nbest_penalty = tune_model |&gt; select_best(metric = \"rsq\")\n\nfinal_wf = finalize_workflow(wf, best_penalty)\n\nfinal_fit = fit(final_wf, data = dat)\n\nfinal_fit |&gt; tidy()\n\n# A tibble: 4 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    20.2   0.0221\n2 tri             4.98  0.0221\n3 thigh           0     0.0221\n4 midarm         -1.53  0.0221\n\n\nWe see that tri has the biggest impact on bfat. The predictor thigh shrunk down to 0. Recall in previous examples when we used the bodyfat data that tri and thigh are highly correlated.\nWhen two predictor variables are highly correlated, the lasso will arbitrarily have one of those variables shrink to zero. The practitioner should understand that the one variable that was not selected could in fact be more appropriate for their model than the one that was selected."
  }
]