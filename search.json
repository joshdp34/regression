[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 3386 Regression Analysis",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 3386 - Regression Analysis.\nPrerequisites: MTH 2311 - Linear Algebra, MTH 2321 - Calculus III, and STA 3381 - Probability and Statistics\n\nCourse Description:\nA development of regression techniques including simple linear regression, multiple regression, logistic regression and Poisson regression with emphasis on model assumptions, parameter estimation, variable selection and diagnostics."
  },
  {
    "objectID": "01_Intro.html#the-probabilistic-model",
    "href": "01_Intro.html#the-probabilistic-model",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.1 The Probabilistic Model",
    "text": "1.1 The Probabilistic Model\nMost students who take an intro stats course are familiar with the idea of a random variable. Students are usually introduced to random variables using the notation \\(X\\).\nIn the technical sense, capital \\(X\\) should denote the random variable (that is, the function itself), while lowercase \\(x\\) denotes the values of that random variable. We will be loose on that convention, so you will see the lowercase used almost extensively even when discussing the random variable itself.\n\n\n\n\n\n\nReview: Random Variable\n\n\n\nA random variable is a function that assigns a numeric value to the outcomes in the sample space.\n\n\nIn regression, the random variable of interest is usually denoted as \\(y\\).\nWe want to predict or model (explain) this variable. Thus, we call this the response (or dependent) variable.\nIf we have measurements of this random variable, then we can express each value \\(y\\) as the mean value of \\(y\\) plus some random error.\nThat is, we can model the variable as \\[\n    y = E(y) + \\varepsilon\n\\tag{1.1}\\]\nwhere \\[\\begin{align*}\n    y = &\\text{ dependent variable}\\\\\n    E(y) =& \\text{ mean (or expected) value of } y\\\\\n    \\varepsilon =& \\text{ random error}\n\\end{align*}\\]\nThis model is referred to as a probabilistic model for \\(y\\). The term “probabilistic” is used because, under certain assumptions, we can make probability-based statements about the extent of the difference between \\(y\\) and \\(E(y)\\).\nFor example, we might assert that the error term, \\[\n    \\varepsilon = y - E(y)\n\\] follows a normal distribution.\nIn practice, we will use sample data to estimate the parameters of the probabilistic model—specifically, the mean \\(E(y)\\) and the random error \\(\\varepsilon\\).\nWe will later discuss a common assumption in regression: that the mean error is zero.\nIn other words, \\[\n    E(\\varepsilon) = 0\n\\]\nGiven this assumption, our best estimate of \\(\\varepsilon\\) is zero. Therefore, we only need to estimate \\(E(y)\\).\nThe simplest method of estimating \\(E(y)\\) is to use the sample mean of \\(y\\) which we will denote as \\[\\begin{align*}\n    \\bar y= \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{align*}\\]\nIf we desired to predict a value of \\(y\\), then our best prediction would be just the sample mean: \\[\\begin{align*}\n    \\hat y = \\bar y\n\\end{align*}\\] where \\(\\hat y\\) denotes a predicted value of \\(y\\).\nThis would be the case with univariate data (we only have one variable in our data: \\(y\\)).\nUnfortunately, this simple model does not take into consideration a number of variables, called independent variables, that may help predict the response variable.\nIndependent variables are also called predictor or explanatory variables.\nThe process of identifying the mathematical model that describes the relationship between \\(y\\) and a set of independent variables, and that best fits the data, is known as regression analysis."
  },
  {
    "objectID": "01_Intro.html#sec-regoverview",
    "href": "01_Intro.html#sec-regoverview",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.2 Overview of Regression Analysis",
    "text": "1.2 Overview of Regression Analysis\nWe will denote the independent variables as \\[\\begin{align*}\n    x_1, x_2, \\ldots, x_k\n\\end{align*}\\] where \\(k\\) is the number of independent variables.\nThe goal of regression analysis is to create a prediction equation that accurately relates \\(y\\) to independent variables, allowing us to predict \\(y\\) for given values of \\(x_1, x_2, \\ldots, x_k\\) with minimal prediction error.\nWhen predicting \\(y\\), we also need a measure of the reliability of our prediction, indicating how large the prediction error might be.\nThese elements form the core of regression analysis.\nBeyond predicting \\(y\\), a regression model can also estimate the mean value of \\(y\\) for specific values of \\(x_1, x_2, \\ldots, x_k\\) and explore the relationship between \\(y\\) and one or more independent variables.\nThe process of regression analysis typically involves six key steps:\n\nHypothesize the form of the model for \\(E(y)\\).\nCollect sample data.\nEstimate the model’s unknown parameters using the sample data.\nDefine the probability distribution of the random error term, estimate any unknown parameters, and validate the assumptions made about this distribution.\nStatistically assess the model’s usefulness.\nIf the model is effective, use it for prediction, estimation, and other purposes."
  },
  {
    "objectID": "01_Intro.html#collecting-the-data-for-regression",
    "href": "01_Intro.html#collecting-the-data-for-regression",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.3 Collecting the Data for Regression",
    "text": "1.3 Collecting the Data for Regression\nThe first step listed above will be discussed later.\nOnce you’ve proposed a model for \\(E(y)\\), the next step is to gather sample data to estimate the model.\nThis means collecting data on both the response variable \\(y\\) and the independent variables \\(x_1, x_2, \\ldots, x_k\\) for each observation in your sample. In regression analysis, the sample includes data on multiple variables: \\[\ny, x_1, x_2, \\ldots, x_k\n\\] This is known as multivariate data.\nRegression data can be either observational or experimental:\nFor observational data no control is exerted over the independent variables (\\(x\\)’s). For example, recording people’s ages and their corresponding blood pressure levels without influencing either.\nFor experimental data the independent variables are controlled or manipulated. For instance, setting different fertilizer amounts for crops to observe the impact on growth.\nSuppose you want to model a student’s annual GPA (\\(y\\)). One approach is to randomly select a sample of \\(n=100\\) students and record their GPA along with the values of each predictor variable.\nData for the first three students in the sample are shown in Table 1.1.\n\n\nTable 1.1: Values of the response variable and predictor variables for the first three students.\n\n\n\nStudent 1\nStudent 2\nStudent 3\n\n\n\n\nAnnual GPA \\(y\\)\n3.8\n2.7\n3.5\n\n\nStudy Hours per Week, \\(x_1\\)\n15\n5\n10\n\n\nClass Attendance, \\(x_2\\) (days)\n30\n20\n25\n\n\nExtracurriculars, \\(x_3\\)\n2\n1\n3\n\n\nAge, \\(x_4\\) (years)\n21\n19\n22\n\n\nEmployed, \\(x_5\\) (1 if yes, 0 if no)\n0\n1\n0\n\n\nLives On Campus, \\(x_6\\) (1 if yes, 0 if no)\n1\n0\n1\n\n\n\n\nIn this example, the \\(x\\) values, like study hours, class attendance, and extracurricular activities, are not predetermined before observing GPA \\(y\\); thus, the \\(x\\) values are uncontrolled. Therefore, the sample data are observational.\n\nDetermining Sample Size for Regression with Observational Data\nWhen applying regression to observational data, the required sample size for estimating the mean \\(E(y)\\) depends on three key factors:\n\nEstimated population standard deviation\nConfidence level\nDesired margin of error (half-width of the confidence interval)\n\nHowever, unlike the univariate case, \\(E(y)\\) is modeled as a function of multiple independent variables, which adds complexity. The sample size must be large enough to estimate and test all parameters in the model.\nTo ensure a sufficient sample size, a common guideline is to select a sample size \\(n\\) that is at least 10 times the number of parameters in the model.\nFor instance, if a university registrar’s office uses the following model for the annual GPA \\(y\\) of a current student:\n\\[E(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_6 x_6\\]\nwhere \\(x_1, x_2, \\dots, x_6\\) are defined in Table 1.1, the model includes six \\(\\beta\\) parameters (excluding \\(\\beta_0\\)). Therefore, they should include at least:\n\\[ 10 \\times 6 = 60 \\]\nstudents in the sample.\n\n\nExperimental Data\nThe second type of data in regression, experimental data, is generated through designed experiments where the independent variables are set in advance (i.e., controlled) before observing the value of \\(y\\).\nFor instance, consider a scenario where a researcher wants to study the effect of two independent variables—say, fertilizer amount \\(x_1\\) and irrigation level \\(x_2\\)—on the growth rate \\(y\\) of plants. The researcher could choose three levels of fertilizer (10g, 20g, and 30g) and three levels of irrigation (1L, 2L, and 3L) and measure the growth rate in one plant for each of the \\(3\\times 3=9\\) fertilizer–irrigation combinations (see Table 1.2 below).\n\n\nTable 1.2: Values of the response variable and two independent variables for the growth rate of plants.\n\n\nFertilizer, \\(x_1\\)\nIrrigation, \\(x_2\\)\nGrowth Rate, \\(y\\)\n\n\n\n\n10g\n1L\n5.2\n\n\n10g\n2L\n6.1\n\n\n10g\n3L\n5.8\n\n\n20g\n1L\n7.0\n\n\n20g\n2L\n7.5\n\n\n20g\n3L\n7.3\n\n\n30g\n1L\n8.4\n\n\n30g\n2L\n8.7\n\n\n30g\n3L\n8.1\n\n\n\n\nIn this experiment, the settings of the independent variables are controlled, in contrast to the uncontrolled nature of observational data, like in the real estate sales example.\nIn many studies, it is often not possible to control the values of the \\(x\\)’s, so most data collected for regression are observational.\nSo, why do we differentiate between these two types of data? We will learn that inferences from regression studies based on observational data have more limitations than those based on experimental data. Specifically, establishing a cause-and-effect relationship between variables is much more challenging with observational data than with experimental data."
  },
  {
    "objectID": "02_Fitting.html#the-straight-line-probabilistic-model",
    "href": "02_Fitting.html#the-straight-line-probabilistic-model",
    "title": "\n2  Fitting the Simple Linear Regression Model\n",
    "section": "\n2.1 The Straight-Line Probabilistic Model",
    "text": "2.1 The Straight-Line Probabilistic Model\nWhen studying environmental science, consider modeling the monthly carbon dioxide (CO₂) emissions \\(y\\) of a city as a function of its monthly industrial activity \\(x\\).\nThe first question to ask is: Do you believe an exact (deterministic) relationship exists between these two variables?\nIn other words, can we predict the exact value of CO₂ emissions if industrial activity is known?\nIt’s unlikely. CO₂ emissions depend on various factors beyond industrial activity, such as weather conditions, regulatory policies, and transportation levels. Even with multiple variables in the model, it’s improbable that we could predict monthly emissions precisely.\nThere will almost certainly be some variation in emissions due to random phenomena that cannot be fully explained or modeled.\nTherefore, we should propose a probabilistic model for CO₂ emissions that accounts for this random variation:\n\\[\ny = E(y) + \\varepsilon\n\\]\nThe random error component,\\(\\varepsilon\\), captures all unexplained variations in emissions caused by omitted variables or unpredictable random factors.\nThe random error plays a key role in hypothesis testing, determining confidence intervals for the model’s deterministic portion, and estimating the prediction error when using the model to predict future values of \\(y\\).\nLet’s start with the simplest probabilistic model—a first-order linear model that graphs as a straight line."
  },
  {
    "objectID": "02_Fitting.html#a-first-order-linear-model",
    "href": "02_Fitting.html#a-first-order-linear-model",
    "title": "\n2  Fitting the Simple Linear Regression Model\n",
    "section": "\n2.2 A First-Order Linear Model",
    "text": "2.2 A First-Order Linear Model\nThe first-order linear model is expessed as \\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nwhere:\n\n\n\\(y\\) is the dependent variable (also known as the response variable)\n\n\\(x\\) is the independent variable (used as a predictor of \\(y\\))\n\n\\(E(y) = \\beta_0 + \\beta_1 x\\) is the deterministic component\n\n\\(\\varepsilon\\) is the random error component\n\n\\(\\beta_0\\) is the y-intercept of the line (the point where the line intersects the y-axis)\n\n\\(\\beta_1\\) is the slope of the line (the change in the mean of \\(y\\) for every 1-unit increase in \\(x\\))\n\nWe use Greek symbols \\(\\beta_0\\) and \\(\\beta_1\\) to denote the y-intercept and slope of the line. These are population parameters with values that would only be known if we had access to the entire population of \\((x, y)\\) measurements.\n\n\n\n\n\n\nReview: Greek Letters in Notation\n\n\n\nUsually, in Statistics, lower-case Greek letters are used to denote population parameters. In our model above, we have an exception. The Greek letter \\(\\varepsilon\\) is not a parameter, but a random variable (parameters are not random variables in frequentist statistics).\n\n\nAs discussed in Section 1.2, regression can be viewed as a six-step process. For now, we’ll focus on steps 2-6, using the simple linear regression model. We’ll explore more complex models later."
  },
  {
    "objectID": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "href": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "title": "\n2  Fitting the Simple Linear Regression Model\n",
    "section": "\n2.3 Fitting the Model: The Method of Least Squares",
    "text": "2.3 Fitting the Model: The Method of Least Squares\nSuppose we have the data shown in Table 2.1 below and plotted in the scatterplot in Figure 2.1.\n\n\nTable 2.1: Data for Scatterplot\n\nx\ny\n\n\n\n1\n2\n\n\n2\n1.4\n\n\n2.75\n1.6\n\n\n4\n1.25\n\n\n6\n1\n\n\n7\n0.5\n\n\n8\n0.5\n\n\n10\n0.4\n\n\n\n\n\nlibrary(tidyverse)\nx = c(1, 2, 2.75, 4, 6, 7, 8, 10)\ny = c(2, 1.4, 1.6, 1.25, 1, 0.5, 0.5, 0.4)\ndat = tibble(x, y)\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point()\n\n\n\nFigure 2.1: Scatterplot of the data in Table 2.1\n\n\n\nWe hypothesize that a straight-line model relates y to x, as follows:\n\\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nHow can we use the data from the eight observations in Table 2.1 to estimate the unknown y-intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\))?\nWe can start by trying some lines and see how well they fit the data. But how do we measure how well a line fits the data?\nA quantitative method to evaluate how well a straight line fits a set of data is by measuring the deviations of the data points from the line.\n\n\n\n\n\n\nReview: Deviations of Response Variable\n\n\n\n\\(y\\) is the variable of interest, so we are focused on the differences between observed \\(y\\) and the predicted value of \\(y\\)\n\n\nWe calculate the magnitude of the deviations (the differences between observed and predicted values of \\(y\\)).\nThese deviations, or prediction errors, represent the vertical distances between observed and predicted values of \\(y\\).\nSuppose we try to fit the line \\[\n    \\hat{y} =2-.2x\n\\tag{2.1}\\]\nThis line can be seen in Figure 2.2.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 2, slope = -0.2, color = \"red\")\n\n\n\nFigure 2.2: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.1\n\n\n\nThe observed and predicted values of \\(y\\), their differences, and their squared differences are shown in the table below.\n\n\nTable 2.2: Deviations and squared deviations of the line in Equation 2.1 and the data in Table 2.1.\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\((y - \\hat{y})\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n1\n2\n1.8\n0.2\n0.004\n\n\n2\n1.4\n1.6\n-0.2\n0.004\n\n\n2.75\n1.6\n1.45\n0.15\n0.0225\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.8\n0.2\n0.04\n\n\n7\n0.5\n0.6\n-0.1\n0.01\n\n\n8\n0.5\n0.4\n0.1\n0.01\n\n\n10\n0.4\n0\n0.4\n0.16\n\n\n\n\nNote that the sum of the errors (SE) is 0.8, and the sum of squares of the errors (SSE), which emphasizes larger deviations from the line, is 0.325.\nWe can try another line to see if we do better at predicting \\(y\\) (that is, have smaller SSE).\nLet’s try the line \\[\n    \\hat{y} =1.8-.15x\n\\tag{2.2}\\]\nThis line can be seen in Figure 2.3.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 1.8, slope = -0.15, color = \"red\")\n\n\n\nFigure 2.3: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.2\n\n\n\nThe fit results are shown in Table 2.3.\n\n\nTable 2.3: Deviations and squared deviations of the line in Equation 2.2 and the data in Table 2.1.\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\(y - \\hat{y}\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n1\n2\n1.65\n0.35\n0.1225\n\n\n2\n1.4\n1.5\n-0.1\n0.01\n\n\n2.75\n1.6\n1.3875\n0.2125\n0.04515625\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.9\n0.1\n0.01\n\n\n7\n0.5\n0.75\n-0.25\n0.0625\n\n\n8\n0.5\n0.6\n-0.1\n0.01\n\n\n10\n0.4\n0.3\n0.1\n0.01\n\n\n\n\nThe SSE for this line is 0.2727, which is lower than the SSE for the previous line, indicating a better fit.\nWhile we could try additional lines to achieve a lower SSE, there are infinitely many possibilities since \\(\\beta_0\\) and \\(\\beta_1\\) can take any real value.\nUsing Calculus, we can attempt to minimize the SSE for the generic line \\[\\begin{align*}\n    \\hat{y} = b_0 +b_1 x\n\\end{align*}\\]\nWe will denote the sum of the squared distances with \\(Q\\): \\[\nQ=\\sum \\left(y_i-\\hat{y}_i\\right)^2\n\\tag{2.3}\\]\nWe determine the “best” line as the one that minimizes \\(Q\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo minimize \\(Q\\), we differentiate it with respect to \\(b_{0}\\) and \\(b_{1}\\): \\[\\begin{align*}\n\\frac{\\partial Q}{\\partial b_{0}} & =-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{1}} & =-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\n\\end{align*}\\]\nSetting these partial derivatives equal to 0, we have \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\\\\\n-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\n\\end{align*}\\] Looking at the first equation, we can simplify as \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum y_{i}-\\sum b_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}-nb_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}=nb_{0}+b_{1}\\sum x_{i}\n\\end{align*}\\]\nSimplifying the second equation gives us \\[\\begin{align*}\n-2\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}-b_0\\sum x_{i}-b_1\\sum x_{i}^{2}=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align*}\\]\n\n\n\nThe two equations \\[\n\\begin{align}\n\\sum y_{i} & =nb_0+b_1\\sum x_{i}\\nonumber\\\\\n\\sum x_{i}y_{i} & =b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align}\n\\tag{2.4}\\]\nare called the normal equations.\nWe now have two equations and two unknowns (\\(b_0\\) and \\(b_1\\)). We can solve the equations simultaneously. We solve the first equation for \\(b_0\\) which gives us \\[\\begin{align*}\nb_0 & =\\frac{1}{n}\\left(\\sum y_{i}-b_1\\sum x_{i}\\right)\\\\\n& =\\bar{y}-b_1\\bar{x}.\n\\end{align*}\\]\nWe now substitute this into the second equation in Equation 2.4. Solving this for \\(b_1\\) gives us \\[\\begin{align*}\n& \\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n& \\quad\\Longrightarrow\\sum x_{i}y_{i}=\\left(\\bar{y}-b_1\\bar{x}\\right)\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n&\\quad\\Longrightarrow b_1=\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}.\n\\end{align*}\\]\nThe equations \\[\n\\begin{align}\nb_0 & =\\bar{y}-b_1\\bar{x}\\\\\nb_1 & =\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{2.5}\\] are called the least squares estimators.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo show these estimators are the minimum, we take the second partial derivatives of \\(Q\\): \\[\\begin{align*}\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{0}\\right)^{2}} & =2n\\\\\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{1}\\right)^{2}} & =2\\sum x_{i}^{2}\n\\end{align*}\\] Since these second partial derivatives are both positives, then we know the least squares estimators are the minimum.\n\n\n\nThe least squares estimators in Equation 2.5 can be expressed in simpler terms if we let \\[\\begin{align*}\nSS_{xx} &= \\sum \\left(x_i-\\bar x\\right)^2 \\\\\nSS_{xy} &= \\sum \\left(x_i-\\bar x\\right)\\left(y_i - \\bar y\\right)\n\\end{align*}\\]\nThe least squares estimates become \\[\\begin{align}\n{b_1=\\frac{SS_{xy}}{SS_{xx}}}\\\\\n{b_0=\\bar{y}-b_1\\bar{x}}\n\\end{align}\\]\nTo recap: The straight line model for the response \\(y\\) in terms of \\(x\\) is \\[\\begin{align*}\n{y = \\beta_0 + \\beta_1 x + \\varepsilon}\n\\end{align*}\\]\nThe line of means is \\[\\begin{align*}\n{E(y) = \\beta_0 + \\beta_1 x }\n\\end{align*}\\]\nThe fitted line (also called the least squares line) is \\[\\begin{align*}\n{\\hat{y} = b_0 + b_1 x }\n\\end{align*}\\]\nFor a given data point, \\((x_i, y_i)\\), the observed value of \\(y\\) is denoted as \\(y_i\\) and the predicted value of \\(y\\) is obtained by substituting \\(x_i\\) into the prediction equation: \\[\\begin{align*}\n{\\hat{y}_i = b_0 + b_1 x_i }\n\\end{align*}\\]\nThe deviation of the \\(i\\)th value of \\(y\\) from its predicted value, called the \\(i\\)th residual, is \\[\\begin{align*}\n{ \\left(y_i-\\hat{y}_i\\right) }\n\\end{align*}\\] Thus, SSE is just the sum of the squared residuals."
  },
  {
    "objectID": "03_Properties.html#properties-of-the-least-squares-estimators",
    "href": "03_Properties.html#properties-of-the-least-squares-estimators",
    "title": "\n3  Properties of the Least Squares Estimators and Model Assumptions\n",
    "section": "\n3.1 Properties of the Least Squares Estimators",
    "text": "3.1 Properties of the Least Squares Estimators\n\n3.1.1 Linear Estimators\nNote that the least squares estimators are linear functions of the observations \\(y_{1},\\ldots,y_{n}\\). That is, both \\(b_0\\) and \\(b_1\\) can be written as a linear combination of the \\(y\\)’s.\nSince \\(y\\) is the variable that we want to model, we call an estimator for some parameter that takes the form of a linear combination of \\(y\\) a linear estimator.\n\n3.1.2 \\(b_1\\) as a Linear Estimator\nWe can express \\(b_1\\) as \\[\n\\begin{align}\nb_1 & =\\sum k_{i}y_{i}\n\\end{align}\n\\tag{3.1}\\] where \\[\\begin{align*}\nk_{i} & =\\frac{x_{i}-\\bar{x}}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align*}\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe first note that \\(\\sum\\left(x_{i}-\\bar{x}\\right)=0\\).\nWe now rewrite \\(b_1\\) as \\[\\begin{align*}\nb_1 & =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}-\\bar{y}\\sum\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\left(\\frac{1}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}\n\\end{align*}\\]\n\n\n\nFrom Equation 3.1, we see that \\(b_1\\) is a linear combination of the \\(y\\)’s since \\(k_{i}\\) are known constants (recall that \\(x_{i}\\) are treated as known constants).\n\n3.1.3 \\(b_0\\) as a Linear Estimator\nWe can write \\(b_0\\) as \\[\n\\begin{align}\nb_0 & =\\sum c_{i}y_{i}\n\\end{align}\n\\tag{3.2}\\] where \\[\\begin{align*}\nc_{i} & =\\frac{1}{n}-\\bar{x}k_{i}\n\\end{align*}\\]\nTherefore, \\(b_0\\) is a linear combination of \\(y_{i}\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe can rewrite \\(b_0\\) as \\[\\begin{align*}\nb_0 & =\\bar{y}-b_{1}\\bar{x}\\\\\n& =\\frac{1}{n}\\sum y_{i}-\\bar{x}\\sum k_{i}y_{i}\\\\\n& =\\sum\\left(\\frac{1}{n}-\\bar{x}k_{i}\\right)y_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "03_Properties.html#model-assumptions",
    "href": "03_Properties.html#model-assumptions",
    "title": "\n3  Properties of the Least Squares Estimators and Model Assumptions\n",
    "section": "\n3.2 Model Assumptions",
    "text": "3.2 Model Assumptions\nLet’s now focus on the random component \\(\\varepsilon\\) in the probabilistic model and its connection to the errors in estimating \\(\\beta_0\\) and \\(\\beta_1\\).\nSpecifically, we’ll explore how the probability distribution of \\(\\varepsilon\\) influences the accuracy of the model in representing the true relationship between the dependent variable \\(y\\) and the independent variable \\(x\\).\nWe make four key assumptions about the probability distribution of \\(\\varepsilon\\):\n\nThe mean of \\(\\varepsilon\\)’s probability distribution is 0. This means that, on average, the errors cancel out over an infinitely large number of experiments for each value of the independent variable \\(x\\). Consequently, the mean value of \\(y\\), \\(E(y)\\), for a given \\(x\\) is \\(E(y) = \\beta_0 + \\beta_1 x\\).\nThe variance of \\(\\varepsilon\\)’s probability distribution is constant across all values of the independent variable \\(x\\). For our linear model, this implies that the variance of \\(\\varepsilon\\) is a constant, say, \\(\\sigma^2\\), regardless of the value of \\(x\\).\nThe probability distribution of \\(\\varepsilon\\) is normal.\nThe errors for different observations are independent. In other words, the error for one value of \\(y\\) does not influence the errors for other \\(y\\) values.\n\nThe implications of the first three assumptions can be seen in Figure 3.1;, which shows distributions of errors for four particular values of \\(x\\).\n\n\n\n\nFigure 3.1: The probability distribution of \\(\\varepsilon\\). At every value of \\(x\\), there is a distribution of \\(y\\) values that satisfy the four assumptions listed above. The red points are observed values.\n\n\n\nFrom Figure 3.1, we see that the probability distributions of the errors are normal, with a mean of 0 and a constant variance \\(\\sigma^2\\).\nThe line in the middle of the curve that goes to the regression line represents the mean value of \\(y\\) for a given value of \\(x\\). The line of means is given by the equation: \\[\nE(y) = \\beta_0 + \\beta_1 x\n\\]\nThese assumptions allow us to create measures of reliability for the least squares estimators and to develop hypothesis tests to evaluate the utility of the least squares line.\nVarious diagnostic techniques are available for checking the validity of these assumptions, and these diagnostics suggest remedies when the assumptions seem invalid.\nTherefore, it is crucial to apply these diagnostic tools in every regression analysis. We will discuss these techniques in detail later.\nIn practice, the assumptions do not need to hold exactly for least squares estimators and test statistics to have the reliability we expect from a regression analysis. The assumptions will be sufficiently satisfied for many real-world applications."
  },
  {
    "objectID": "03_Properties.html#an-estimator-of-sigma2",
    "href": "03_Properties.html#an-estimator-of-sigma2",
    "title": "\n3  Properties of the Least Squares Estimators and Model Assumptions\n",
    "section": "\n3.3 An Estimator of \\(\\sigma^2\\)\n",
    "text": "3.3 An Estimator of \\(\\sigma^2\\)\n\nThe variability of random error, measured by its variance \\(\\sigma^2\\), plays a crucial role in the accuracy of estimating model parameters \\(\\beta_0\\) and \\(\\beta_1\\), as well as in the precision of predictions when using \\(\\hat{y}\\) to estimate \\(y\\) for a given value of \\(x\\). As a result, it is expected that \\(\\sigma^2\\) will appear in the formulas for confidence intervals and test statistics.\nIn most real-world scenarios, \\(\\sigma^2\\) is unknown and must be estimated using the available data. The best estimate for \\(\\sigma^2\\) is \\(s^2\\), calculated by dividing the sum of squares of residuals by the associated degrees of freedom (df). The sum of squares of residuals is given by: \\[\nSSE = \\sum \\left(y_i - \\hat{y}_i\\right)^2\n\\]\nIn a simple linear regression model, 2 degrees of freedom are used to estimate the y-intercept and slope, leaving \\((n - 2)\\) degrees of freedom for estimating the error variance. Thus, the estimate of \\(\\sigma^2\\) is: \\[\n\\begin{align*}\ns^2 &= \\frac{SSE}{n-2}\\\\\n&= \\frac{\\sum \\left(y_i - \\hat{y}_ i\\right)^2}{n-2}\n\\end{align*}\n\\]\nThis \\(s^2\\) serves as the basis for further statistical analysis, including the construction of confidence intervals and hypothesis testing.\nThe value of \\(s^2\\) is referred to as the mean square error (MSE).\nThe value \\[\\begin{align*}\ns &= \\sqrt{s^2}\n\\end{align*}\\] is referred to as the standard error of the regression model or as the root MSE (RMSE).\nUsing the empirical rule, we expect approximately 95% of the observed \\(y\\) values to lie within \\(2s\\) of their respective least squares predicted values, \\(\\hat y\\).\n\n\n\n\n\n\nReview: The Empirical Rule\n\n\n\nRecall the empirical rule applies to distributions that are mound-shaped and symmetric. It state that approximately 68% of the distribution is within one standard deviation of the mean, approximately 95% of the distribution is within two standard deviations of the mean, and approximatley 99.7% of the distribution is withing three standard deviations of the mean. Since we assume \\(\\varepsilon\\) is normally distributed, then the empirical rule holds.\n\n\n\nExample 3.2 (Example 3.1 - revisited) We can use the summary() function with the fit from lm to obtain the summary stats of the fit.\n\nfit = lm(mpg~wt, data = mtcars)\n\nfit |&gt; summary()\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe standard error (RMSE) of the fit is 3.046.\nWe can obtain the MSE with the following code:\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nwt         1 847.73  847.73  91.375 1.294e-10 ***\nResiduals 30 278.32    9.28                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe MSE is 9.28."
  },
  {
    "objectID": "04_Sampling.html#properties-of-the-constants-k_i-and-c_i",
    "href": "04_Sampling.html#properties-of-the-constants-k_i-and-c_i",
    "title": "\n4  Sampling Distribution of the Least Squares Estimators and Testing the Slope\n",
    "section": "\n4.1 Properties of the constants \\(k_i\\) and \\(c_i\\)\n",
    "text": "4.1 Properties of the constants \\(k_i\\) and \\(c_i\\)\n\nIn the previous section, we examined how the least squares estimators are linear combinations of the response variable \\(y\\). Let’s know look at the properties of the coefficients in Equation 3.1 and Equation 3.2. We will not present the proofs in this course but they are not complicated.\nThe coefficients \\(k_{i}\\) have the following properties: \\[\n\\begin{align}\n\\sum k_{i} & =0\n\\end{align}\n\\tag{4.1}\\]\n\\[\n\\begin{align}\n\\sum k_{i}x_i & =1\n\\end{align}\n\\tag{4.2}\\]\n\\[\n\\begin{align}\n\\sum k_{i}^{2} & =\\frac{1}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.3}\\]\nLikewise, the coefficients \\(c_{i}\\) have the following properties: \\[\n\\begin{align}\n\\sum c_{i} & =1\n\\end{align}\n\\tag{4.4}\\]\n\\[\n\\begin{align}\n\\sum c_{i}x_i & =0\n\\end{align}\n\\tag{4.5}\\]\n\\[\n\\begin{align}\n\\sum c_{i}^{2} & =\\frac{1}{n}+\\frac{\\left(\\bar{x}\\right)^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.6}\\]\n\n\n\n\n\n\nReview: The Expected Value of a Linear Combination\n\n\n\nRecall that the expected value of a linear combination of the random variable \\(Y\\) is \\[\nE(aY+b)=aE(Y)+b\n\\] where \\(a\\) and \\(b\\) are constants."
  },
  {
    "objectID": "04_Sampling.html#expected-values-of-b_0-and-b_1",
    "href": "04_Sampling.html#expected-values-of-b_0-and-b_1",
    "title": "\n4  Sampling Distribution of the Least Squares Estimators and Testing the Slope\n",
    "section": "\n4.2 Expected Values of \\(b_0\\) and \\(b_1\\)\n",
    "text": "4.2 Expected Values of \\(b_0\\) and \\(b_1\\)\n\nBefore finding the expectations, recall \\(E\\left(y_i\\right)=\\beta_{0}+\\beta_{1}x_i\\).\n\n4.2.1 Expected Value of \\(b_1\\)\n\nThe expected value of \\(b_1\\) is \\[\n\\begin{align*}\nE\\left[b_1\\right] & =E\\left[\\underbrace{\\sum k_{i}y_i}_{(3.1)}\\right]\\\\\n& =\\sum k_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\underbrace{\\sum k_{i}}_{(4.1)}+\\beta_{1}\\underbrace{\\sum k_{i}x_i}_{(4.2)}\\\\\n& =\\beta_{1}\n\\end{align*}\n\\]\n\n4.2.2 Expected Value of \\(b_0\\)\n\nThe expected value of \\(b_0\\) is \\[\n\\begin{align*}\nE\\left[b_0\\right] & =E\\left[\\underbrace{\\sum c_{i}y_i}_{(3.2)}\\right]\\\\\n& =\\sum c_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\underbrace{\\sum c_{i}}_{(4.4)}+\\beta_{1}\\underbrace{\\sum c_{i}x_i}_{(4.5)}\\\\\n& =\\beta_{0}\n\\end{align*}\n\\]\nTherefore, \\(b_0\\) is an unbiased estimator of \\(\\beta_0\\) and \\(b_1\\) is an unbiased estimator of \\(\\beta_1\\).\n\n\n\n\n\n\nReview: Unbiased Estimator\n\n\n\nRecall that an unbiased estimator for some parameter is an estimator that has an expected value equal to that parameter."
  },
  {
    "objectID": "04_Sampling.html#variances-of-b_0-and-b_1",
    "href": "04_Sampling.html#variances-of-b_0-and-b_1",
    "title": "\n4  Sampling Distribution of the Least Squares Estimators and Testing the Slope\n",
    "section": "\n4.3 Variances of \\(b_0\\) and \\(b_1\\)\n",
    "text": "4.3 Variances of \\(b_0\\) and \\(b_1\\)\n\nTo find the variances, we will use a result from mathematical statistics: Let \\(Y_{1},\\ldots,Y_{n}\\) be uncorrelated random variables and let \\(a_{1},\\ldots,a_{n}\\) be constants. Then \\[\n\\begin{align}\nVar\\left[\\sum a_{i}Y_i\\right] & =\\sum a_{i}^{2}Var\\left[Y_i\\right]\n\\end{align}\n\\tag{4.7}\\]\nRecall that we assume the response variables \\(y_i\\)’s are independent.\nTechnically, we assume the \\(y_i\\)’s are uncorrelated. In general, uncorrelated does not imply independent. However, if the random variables are jointly normally distributed (recall our third assumption of the model), then uncorrelated does imply independent.\nAlso, note that \\[\n\\begin{align*}\n    Var\\left[Y\\right]& = Var\\left[\\beta_0 + \\beta_1 x + \\varepsilon\\right]\\\\\n    & = Var\\left[\\varepsilon\\right]\\\\\n    & = \\sigma^2\n\\end{align*}\n\\]\n\n4.3.1 Variance of \\(b_1\\)\n\nThe variance of \\(b_1\\) is \\[\n\\begin{align}\nVar\\left[b_1\\right] & =Var\\left[\\underbrace{\\sum k_{i}y_i}_{(3.1)}\\right]\\\\\n& =\\underbrace{\\sum k_{i}^{2}}_{(4.3)}Var\\left[y_i\\right]\\\\\n& =\\frac{\\sigma^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.8}\\]\n\n4.3.2 Variance of \\(b_0\\)\n\nThe variance of \\(b_0\\) is \\[\n\\begin{align}\nVar\\left[b_0\\right] & =Var\\left[\\underbrace{\\sum c_{i}y_i}_{(3.2)}\\right]\\nonumber\\\\\n& =\\underbrace{\\sum c_{i}^{2}}_{(4.6)}Var\\left[y_i\\right]\\\\\n& =\\sigma^{2}\\left[\\frac{1}{n}+\\frac{\\left(\\bar{x}\\right)^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\\right]\n\\end{align}\n\\tag{4.9}\\]"
  },
  {
    "objectID": "04_Sampling.html#best-linear-unbiased-estimators-blues",
    "href": "04_Sampling.html#best-linear-unbiased-estimators-blues",
    "title": "\n4  Sampling Distribution of the Least Squares Estimators and Testing the Slope\n",
    "section": "\n4.4 Best Linear Unbiased Estimators (BLUEs)",
    "text": "4.4 Best Linear Unbiased Estimators (BLUEs)\nWe see from Equation 3.1 and Equation 3.2 that \\(b_0\\) and \\(b_1\\) are linear estimators.\nAny estimator for \\(\\beta_{1}\\), which we will denote as \\(\\hat{\\beta}_{0}\\), that takes the form \\[\n\\begin{align*}\n\\hat{\\beta}_{1} & =\\sum a_{i}y_i\n\\end{align*}\n\\] where \\(a_{i}\\) is some constant, is called a linear estimator.\nOf all linear estimators for \\(\\beta_0\\) and \\(\\beta_1\\) that are unbiased, the least squares estimators, \\(b_0\\) and \\(b_1\\), have the smallest variance.\nThis is summarized in the following well known theorem:\n\nTheorem 4.1 (Gauss Markov Theorem) For the simple linear regression model, the least squares estimators \\(b_0\\) and \\(b_1\\) are unbiased and have minimum variance among all unbiased linear estimators.\n\nAn estimator that is linear, unbiased, and has the smallest variance of all unbiased linear estimators is called the best linear unbiased estimator (BLUE).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nProof of the Gauss Markov Theorem:\nFor all linear estimators that are unbiased, we must have \\[\n\\begin{align*}\nE\\left[\\hat{\\beta}_{1}\\right] & =E\\left[\\sum a_{i}y_i\\right]\\\\\n& =\\sum a_{i}E\\left[y_i\\right]\\\\\n& =\\beta_{1}\n\\end{align*}\n\\] Since \\(E\\left[y_i\\right]=\\beta_{0}+\\beta_{1}x_i\\), then we must have \\[\n\\begin{align*}\nE\\left[\\hat{\\beta}_{1}\\right] & =\\sum a_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\sum a_{i}+\\beta_{1}\\sum a_{i}x_i\\\\\n& =\\beta_{1}\n\\end{align*}\n\\] Therefore, \\[\n\\begin{align*}\n\\sum a_{i} & =0\\\\\n\\sum a_{i}x_i & =1\n\\end{align*}\n\\] We now examine the variance of \\(\\hat{\\beta}_{1}\\): \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =\\sum a_{i}^{2}Var\\left[y_i\\right]\\\\\n& =\\sigma^{2}\\sum a_{i}^{2}\n\\end{align*}\n\\] Let’s now define \\(a_{i}=k_{i}+d_{i}\\) where \\(k_{i}\\) is defined in Equation 3.1. and \\(d_{i}\\) is some arbitrary constant.\nWe will show that adding a constant (whether negative or positive) to \\(k_i\\) cannot make the variance smaller. Thus, the smallest variance of the linear estimator \\(\\hat{\\beta}_1\\) is when \\(a_i=k_i\\).\nThe variance of \\(\\hat{\\beta}_{1}\\) can now be written as \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =\\sigma^{2}\\sum a_{i}^{2}\\\\\n& =\\sigma^{2}\\sum\\left(k_{i}+d_{i}\\right)^{2}\\\\\n& =\\sigma^{2}\\sum\\left(k_{i}^{2}+2k_{i}d_{i}+d_{i}^{2}\\right)\\\\\n& =Var\\left[b_1\\right]+2\\sigma^{2}\\sum k_{i}d_{i}+\\sigma^{2}\\sum d_{i}^{2}\n\\end{align*}\n\\] Examining the second term and using the expression of \\(k_{i}\\) in Equation 3.1, we see that \\[\n\\begin{align*}\n\\sum k_{i}d_{i} & =\\sum k_{i}\\left(a_{i}-k_{i}\\right)\\\\\n& =\\sum a_{i}k_{i}-\\underbrace{\\sum k_{i}^{2}}_{(4.3)}\\\\\n& =\\sum a_{i}\\frac{x_i-\\bar{x}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum a_{i}x_i-\\bar{x}\\sum a_{i}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{1-\\bar{x}\\left(0\\right)}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =0\n\\end{align*}\n\\]\nWe now have the variance of \\(\\hat{\\beta}_{1}\\) as \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =Var\\left[b_1\\right]+\\sigma^{2}\\sum d_{i}^{2}\n\\end{align*}\n\\] This variance is minimized when \\(\\sum d_{i}^{2}=0\\) which only happens when \\(d_{i}=0\\).\nThus, the unbiased linear estimator with the smallest variance is when \\(a_{i}=k_{i}\\). That is, the least squares estimator \\(b_1\\) in Equation 3.1 has the smallest variance of all unbiased linear estimators of \\(\\beta_{1}\\).\nA similar argument can be used to show that \\(b_0\\) has the smallest variance of all unbiased linear estimators of \\(\\beta_{0}\\)."
  },
  {
    "objectID": "04_Sampling.html#sampling-distribution-for-b_1",
    "href": "04_Sampling.html#sampling-distribution-for-b_1",
    "title": "\n4  Sampling Distribution of the Least Squares Estimators and Testing the Slope\n",
    "section": "\n4.5 Sampling Distribution for \\(b_1\\)\n",
    "text": "4.5 Sampling Distribution for \\(b_1\\)\n\nNow that we see that the least squares estimator \\(b_1\\) is the BLUE for \\(\\beta_{1}\\), we will now examine the sampling distribution for \\(b_1\\).\nWe previously discussed that the mean of the sampling distribution of \\(b_1\\) is \\[\nE[b_1]=\\beta_1\n\\] with a variance of \\[\n\\begin{align}\nVar\\left[b_1\\right]\n& =\\frac{\\sigma^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.10}\\]\nNote that in our model with our four assumptions, \\(y\\) is normally distributed. That is, \\[\n\\begin{align}\n    y\\sim N\\left(\\beta_0+\\beta_1 x, \\sigma^2\\right)\n\\end{align}\n\\tag{4.11}\\]\nTo learn about the sampling distributions of the least squares estimators, we will use the following theorems from mathematical statistics:\n\nTheorem 4.2 (Sum of Independent Normal Random Variables) If \\[\nY_i\\sim N\\left(\\mu_i,\\sigma_i^2\\right)\n\\] are independent, then the linear combination \\(\\sum_i a_iY_i\\) is also normally distributed where \\(a_i\\) are constants. In particular \\[\n\\sum_i a_iY_i \\sim N\\left(\\sum_i a_i\\mu_i, \\sum_i a_i^2\\sigma_i^2\\right)\n\\]\n\n\nTheorem 4.3 (Adding a Constant to a Normal Random Variable) If \\[\nY\\sim N\\left(\\mu,\\sigma^2\\right)\n\\] then for any real constant \\(c\\), \\[\nY+c\\sim N\\left(\\mu+c,\\sigma^2\\right)\n\\]\n\nSince \\(Y\\) is normally distributed by Equation 4.11, then we can apply Theorem 4.2 which implies that \\(b_1\\) is normally distributed. That is, \\[\n\\begin{align}\nb_1 & \\sim N\\left(\\beta_{1},\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\right)\n\\end{align}\n\\tag{4.12}\\]\n\n4.5.1 Standardized Score\nSince \\(b_1\\) is normally distributed, we can standardize it so that the resulting statistic will have a standard normal distribution.\nTherefore, we have \\[\n\\begin{align}\nz=\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}}} & \\sim N\\left(0,1\\right)\n\\end{align}\n\\tag{4.13}\\]\n\n4.5.2 Studentized Score\nIn practice, the standardized score \\(z\\) is not useful since we do not know the value of \\(\\sigma^{2}\\). We can estimate \\(\\sigma^{2}\\) with the statistic \\[\ns^2 = \\frac{SSE}{n-2}\n\\]\nUsing this estimate for \\(\\sigma^2\\) leads us to a \\(t\\)-score: \\[\n\\begin{align}\nt=\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}}} & \\sim t\\left(n-2\\right)\n\\end{align}\n\\tag{4.14}\\]\nWe call this \\(t\\) statistic, the studentized score.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nIt is important to note the following theorem from math stats presented here without proof:\n\nTheorem 4.4 (Distribution of the sample variance of the residuals) For the sample variance of the residuals \\(s^{2}\\), the quantity \\[\\begin{align*}\n\\frac{\\left(n-2\\right)s^{2}}{\\sigma^{2}} & =\\frac{SSE}{\\sigma^{2}}\n\\end{align*}\\] is distributed as a chi-square distribution with \\(n-2\\) degrees of freedom. That is, \\[\\begin{align*}\n\\frac{SSE}{\\sigma^{2}} & \\sim\\chi^{2}\\left(n-2\\right)\n\\end{align*}\\]\n\nWe will use another important theorem form math stats (again presented without proof):\n\nTheorem 4.5 (Ratio of independent standard normal and chi-square statistics) If \\(Z\\sim N\\left(0,1\\right)\\) and \\(W\\sim\\chi^{2}\\left(\\nu\\right)\\), and \\(Z\\) and \\(W\\) are independent, then the statistic \\[\\begin{align*}\n\\frac{Z}{\\sqrt{\\frac{W}{\\nu}}}\n\\end{align*}\\] is distributed as a Student’s \\(t\\) distribution with \\(\\nu\\) degrees of freedom.\n\nWe will take the standardized score in Equation 4.13 and divide by \\[\\begin{align*}\n\\sqrt{\\frac{\\frac{\\left(n-2\\right)s^{2}}{\\sigma^{2}}}{\\left(n-2\\right)}} & =\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}\n\\end{align*}\\] to give us \\[\\begin{align*}\nt & =\\frac{\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}}{\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}}\\\\\n& =\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}}\\\\\n& =\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}\n\\end{align*}\\] which will have a Student’s \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "04_Sampling.html#sec-modelutility",
    "href": "04_Sampling.html#sec-modelutility",
    "title": "\n4  Sampling Distribution of the Least Squares Estimators and Testing the Slope\n",
    "section": "\n4.6 Assessing the Utility of the Model: Making Inferences About the Slope",
    "text": "4.6 Assessing the Utility of the Model: Making Inferences About the Slope\nSuppose that the independent variable \\(x\\) is completely unrelated to the dependent variable \\(y\\).\nWhat could be said about the values of \\(\\beta_0\\) and \\(\\beta_1\\) in the hypothesized probabilistic model \\[\\begin{align*}\n    y = \\beta_0 +\\beta_1 x + \\varepsilon\n\\end{align*}\\] if \\(x\\) contributes no information for the prediction of \\(y\\)?\nThe implication is that the mean of \\(y\\), does not change as \\(x\\) changes. In other words, the line would just be a horizontal line.\nIf \\(E(y)\\) does not change as \\(x\\) increases, then using \\(x\\) to predict \\(y\\) in the linear model is not useful.\nRegardless of the value of \\(x\\), you always predict the same value of \\(y\\). In the straight-line model, this means that the true slope, \\(\\beta_1\\), is equal to 0.\nTherefore, to test the null hypothesis that \\(x\\) contributes no information for the prediction of \\(y\\) against the alternative hypothesis that these variables are linearly related with a slope differing from 0, we test \\[\\begin{align*}\n    H_0:\\beta_1 = 0\\\\\n    H_a:\\beta_1\\ne 0\n\\end{align*}\\]\nIf the data support the alternative hypothesis, we conclude that \\(x\\) does contribute information for the prediction of \\(y\\) using the straight-line model (although the true relationship between \\(E(y)\\) and \\(x\\) could be more complex than a straight line). Thus, to some extent, this is a test of the utility of the hypothesized model.\nThe appropriate test statistic is the studentized score given above: \\[\n\\begin{align}\n    t &= \\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}\\\\\n    &=\\frac{b_1}{\\sqrt{\\frac{s^{2}}{SS_{xx}}}}\n\\end{align}\n\\tag{4.15}\\]\nAnother way to make inferences about the slope \\(\\beta_1\\) is to estimate it using a confidence interval \\[\n\\begin{align}\n    b_1 \\pm \\left(t_{\\alpha/2}\\right)s_{b_1}\n\\end{align}\n\\tag{4.16}\\] where \\[\\begin{align*}\n    s_{b_1} = \\frac{s}{\\sqrt{SS_{xx}}}\n\\end{align*}\\]\nWe can obtain the p-value for the hypothesis test by using the summary function with an lm object. For the previous example consisting of the mtcars data.\n\nExample 4.1 (Example 3.1 - revisited)  \n\nlibrary(tidyverse)\n\nfit = lm(mpg~wt, data = mtcars)\n\nWe find the least squares estimates as\n\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nFrom the output, we see the p-value is \\(1.29\\times 10^{-10}\\). So we have sufficient evidence to conclude that the true population slope is different than zero.\nTo find the confidence interval, we can use the confint function with the lm object.\n\nconfint(fit, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept) 33.450500 41.119753\nwt          -6.486308 -4.202635\n\n\nWe 95% confident that the true population slope is in the interval \\((-6.486, -4.203)\\)"
  },
  {
    "objectID": "05_Correlation.html#the-coefficient-of-correlation",
    "href": "05_Correlation.html#the-coefficient-of-correlation",
    "title": "\n5  Correlation Coefficient and the Coefficient of Determination\n",
    "section": "\n5.1 The Coefficient of Correlation",
    "text": "5.1 The Coefficient of Correlation\nThe claim is often made that the crime rate and the unemployment rate are “highly correlated.”\nAnother popular belief is that IQ and academic performance are “correlated.” Some people even believe that the Dow Jones Industrial Average and the lengths of fashionable skirts are “correlated.”\nThus, the term correlation implies a relationship or association between two variables.\nFor the data \\((x_i,y_i)\\), \\(i=1,\\ldots,n\\), we want a measure of how well a linear model explains a linear relationship between \\(x\\) and \\(y\\).\nRecall the quantities \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\).\n\n\n\n\n\n\nReview: Different \\(SS\\) quantities\n\n\n\nRecall that \\[\n\\begin{align*}\nSS_{xx} &= \\sum\\left(x_i-\\bar{x}\\right)^2\\\\\nSS_{yy} &= \\sum\\left(y_i-\\bar{y}\\right)^2\\\\\nSS_{xy} &= \\sum\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)\n\\end{align*}\n\\]\n\n\n\\(SS_{xx}\\) and \\(SS_{yy}\\) are measures of variability of \\(x\\) and \\(y\\), respectively. That is, they indicate how \\(x\\) and \\(y\\) varies about their mean, individually.\n\\(SS_{xy}\\) is a measure of how \\(x\\) and \\(y\\) vary together.\n\nExample 5.1 (Data from Table 2.1) For example, consider the data from Table 2.1. Let’s find \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\) in R.\n\nlibrary(tidyverse)\n\nx = c(1, 2 ,2.75, 4, 6, 7, 8, 10)\ny = c(2, 1.4, 1.6, 1.25, 1, 0.5, 0.5, 0.4)\n\ndat =  tibble(x, y)\n\nybar =  mean(y)\nxbar =  mean(x)\n\nggplot(data=dat, aes(x = x, y = y)) +\n  geom_point() +\n  xlim(0,10) +\n  ylim(0,2) +\n  geom_hline(yintercept = ybar,col=\"red\") +\n  geom_vline(xintercept = xbar, col=\"red\")\n\n\n\n\n\ndev_x = x-xbar\ndev_y = y-ybar\n\ndev_xy = dev_x*dev_y\n\ndat1 = tibble(x,y,dev_x^2,dev_y^2,dev_xy)\ndat1\n\n# A tibble: 8 × 5\n      x     y `dev_x^2` `dev_y^2`  dev_xy\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1  1     2       16.8     0.844   -3.76  \n2  2     1.4      9.57    0.102   -0.986 \n3  2.75  1.6      5.49    0.269   -1.22  \n4  4     1.25     1.20    0.0285  -0.185 \n5  6     1        0.821   0.00660 -0.0736\n6  7     0.5      3.63    0.338   -1.11  \n7  8     0.5      8.45    0.338   -1.69  \n8 10     0.4     24.1     0.464   -3.34  \n\n\n\nIn the output of dat1, dev_x^2 represents \\((x_i-\\bar{x})^2\\) and dev_y^2 represents \\((y_i-\\bar{y})^2\\) for each observation. dev_xy represents \\((x_i-\\bar{x})(y_i-\\bar{y})\\) for each observation. Note that each value is negative. This is because as \\(x\\) is below \\(\\bar{x}\\), \\(y\\) is above \\(\\bar{y}\\).\nLikewise, as \\(X\\) is above \\(\\bar{x}\\), \\(Y\\) is below \\(\\bar{y}\\). In the ggplot above, the two red lines represent \\(\\bar{x}\\) (the vertical red line) and \\(\\bar{y}\\) (the horizontal red line). You can see how the observations are below or above these lines.\nWe can find the values of \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\) by\n\n#SS_XX\ndev_x^2 |&gt; sum()\n\n[1] 69.99219\n\n#SS_YY\ndev_y^2 |&gt; sum()\n\n[1] 2.389687\n\n#SS_XY\ndev_xy |&gt; sum()\n\n[1] -12.36094\n\n\n\nExample 5.2 (The trees dataset) For another example, consider the trees dataset.\nIn R, a packaged called datasets include a number of available datasets. One of the datasets is called trees.\n\nlibrary(datasets)\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\nThere are 31 total observations in this dataset. Variables measured are the Girth (actually the diameter measured at 54 in. off the ground), the Height, and the Volume of timber from each black cherry tree.\nSuppose we want to predict Volume from Girth.\nAgain, we plot the data with red lines representing \\(\\bar{x}\\) and \\(\\bar{y}\\).\n\nlibrary(datasets)\nlibrary(tidyverse)\n\nxbar = mean(trees$Girth)\nybar = mean(trees$Volume)\n\nggplot(data=trees, aes(x=Girth, y=Volume)) +\n  geom_point() +\n  geom_hline(yintercept = ybar,col=\"red\") +\n  geom_vline(xintercept = xbar, col=\"red\")\n\n\n\nx = trees$Girth\ny = trees$Volume\n\ndev_x = x-xbar\ndev_y = y-ybar\n\ndev_xy = dev_x*dev_y\n\n\ndat1 = tibble(x,y,dev_x^2,dev_y^2,dev_xy)\ndat1 |&gt; print(n=31)\n\n# A tibble: 31 × 5\n       x     y `dev_x^2` `dev_y^2`  dev_xy\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1   8.3  10.3  24.5        395.    98.3  \n 2   8.6  10.3  21.6        395.    92.4  \n 3   8.8  10.2  19.8        399.    88.8  \n 4  10.5  16.4   7.55       190.    37.8  \n 5  10.7  18.8   6.49       129.    29.0  \n 6  10.8  19.7   5.99       110.    25.6  \n 7  11    15.6   5.06       212.    32.8  \n 8  11    18.2   5.06       143.    26.9  \n 9  11.1  22.6   4.62        57.3   16.3  \n10  11.2  19.9   4.20       105.    21.0  \n11  11.3  24.2   3.80        35.7   11.6  \n12  11.4  21     3.42        84.1   17.0  \n13  11.4  21.4   3.42        76.9   16.2  \n14  11.7  21.3   2.40        78.7   13.7  \n15  12    19.1   1.56       123.    13.8  \n16  12.9  22.2   0.121       63.5    2.78 \n17  12.9  33.8   0.121       13.2   -1.26 \n18  13.3  27.4   0.00266      7.68  -0.143\n19  13.7  25.7   0.204       20.0   -2.02 \n20  13.8  24.9   0.304       27.8   -2.91 \n21  14    34.5   0.565       18.7    3.25 \n22  14.2  31.7   0.906        2.34   1.46 \n23  14.5  36.3   1.57        37.6    7.67 \n24  16    38.3   7.57        66.1   22.4  \n25  16.3  42.6   9.31       154.    37.9  \n26  17.3  55.4  16.4        637.   102.   \n27  17.5  55.7  18.1        652.   109.   \n28  17.9  58.3  21.6        791.   131.   \n29  18    51.5  22.6        455.   101.   \n30  18    51    22.6        434.    99.0  \n31  20.6  77    54.0       2193.   344.   \n\n#SS_XX\ndev_x^2 |&gt; sum()\n\n[1] 295.4374\n\n#SS_YY\ndev_y^2 |&gt; sum()\n\n[1] 8106.084\n\n#SS_XY\ndev_xy |&gt; sum()\n\n[1] 1496.644\n\n\nIn this example, most of the observations have \\((x-\\bar{x})(y-\\bar{y})\\) that are positive. This is because these observations have values of \\(x\\) that are below \\(\\bar{x}\\) and values of \\(y\\) that are below \\(\\bar{y}\\), or values of \\(x\\) that are above \\(\\bar{x}\\) and values of \\(y\\) that are above \\(\\bar{y}\\).\nThere are four observations that have a negative value of \\((x-\\bar{x})(y-\\bar{y})\\). Although they are negative, the value of \\(SS_{xy}\\) is positive due to all the observations with positive values of \\((x-\\bar{x})(y-\\bar{y})\\). Therefore, we say if \\(SS_{xy}\\) is positive, then \\(y\\) tends to increase as \\(x\\) increases. Likewise, if \\(SS_{xy}\\) is negative, then \\(y\\) tends to decrease as \\(x\\) increases.\nIf \\(SS_{xy}\\) is zero (or close to zero), then we say \\(y\\) does not tend to change as \\(x\\) increases.\n\n\n5.1.1 Defining the Correlation Coefficient\nWe first note that \\(SS_{xy}\\) cannot be greater in absolute value than the quantity \\[\n\\sqrt{SS_{xx}SS_{yy}}\n\\] We will not prove this here, but it is a direct application of the Cauchy-Schwarz inequality .\nWe define the linear correlation coefficient as \\[\n\\begin{align}\n    r=\\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}\n\\end{align}\n\\tag{5.1}\\]\n\\(r\\) is also called the Pearson correlation coefficient.\nWe note that \\[\n-1\\le r \\le 1\n\\]\nIf \\(r=0\\), then there is no linear relationship between \\(x\\) and \\(y\\).\nIf \\(r\\) is positive, then the slope of the linear relationship is positive. If \\(r\\) is negative, then the slope of the linear relationship is negative.\nThe closer \\(r\\) is to one in absolute value, the stronger the linear relationship is between \\(x\\) and \\(y\\).\n\n5.1.2 Some Examples of \\(r\\)\n\nThe best way to grasp correlation is to see examples. In Figure 5.1, scatterplots of 200 observations are shown with a least squares line.\n\n\n\n\n\n(a) \\(r=-0.079\\)\n\n\n\n\n\n(b) \\(r=-0.672\\)\n\n\n\n\n\n\n\n(c) \\(r=0.723\\)\n\n\n\n\n\n(d) \\(r=0.524\\)\n\n\n\nFigure 5.1: Examples of correlation\n\n\nNote how the value of \\(r\\) relates to how spread out the points are from the line as well as to the slope of the line.\nThe correlation coefficient, \\(r\\), quantifies the strength of the linear relationship between two variables, \\(x\\) and \\(y\\), similar to the way the least squares slope, \\(b_1\\), does. However, unlike the slope, the correlation coefficient is scaleless. This means that the value of \\(r\\) always falls between \\(\\pm 1\\), regardless of the units used for \\(x\\) and \\(y\\).\nThe calculation of \\(r\\) uses the same data that is used to fit the least squares line. Given that both \\(r\\) and \\(b_1\\) offer insight into the utility of the model, it’s not surprising that their computational formulas are related.\nIt’s also important to remember that a high correlation does not imply causality. If a high positive or negative value of \\(r\\) is observed, this does not mean that changes in \\(x\\) cause changes in \\(y\\). The only valid conclusion is that there may be a linear relationship between \\(x\\) and \\(y\\).\n\n5.1.3 The Population Correlation Coefficient\nThe correlation \\(r\\) is for the observed data which is usually from a sample. Thus, \\(r\\) is the sample correlation coefficient.\nWe could make a hypothesis about the correlation of the population based on the sample. We will denote the population correlation with \\(\\rho\\). The hypothesis we will want to test is \\[\\begin{align*}\n  H_0:\\rho = 0\\\\\nH_a:\\rho \\ne 0\n\\end{align*}\\]\nRecall the hypothesis test for the slope in Section 4.6.\nIf we test \\[\\begin{align*}\nH_{0}: & \\beta_{1}=0\\\\\nH_{a}: & \\beta_{1}\\ne0\n\\end{align*}\\] then this is equivalent to testing1 \\[\\begin{align*}\nH_{0}: & \\rho=0\\\\\nH_{a}: & \\rho\\ne0\n\\end{align*}\\] since both hypotheses test to see of there is a linear relationship between \\(x\\) and \\(y\\).\nNow note, using Equation 2.5, that \\(b_1\\) can be rewritten as \\[\n\\begin{align}\nb_1 & =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{SS_{xy}}{SS_{xx}}\\\\\n& =\\frac{rSS_{xy}}{rSS_{xx}}\\\\\n& =\\frac{rSS_{xy}}{\\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}SS_{xx}}\\\\\n& =\\frac{r\\sqrt{SS_{xx}SS_{yy}}}{SS_{xx}}\\\\\n& =r\\frac{\\sqrt{\\frac{SS_{xx}}{n-1}\\frac{SS_{yy}}{n-1}}}{\\frac{SS_{xx}}{n-1}}\\\\\n& =r\\frac{s_{X}s_{Y}}{s_{X}^{2}}\\\\\n& =r\\frac{s_{y}}{s_{X}}\n\\end{align}\n\\tag{5.2}\\] where \\(s_{y}\\) and \\(s_{x}\\) are the sample standard deviation of \\(y\\) and \\(x\\), respectively.\nThe test statistic is \\[\n\\begin{align}\nt & =\\frac{r\\sqrt{\\left(n-2\\right)}}{\\sqrt{1-r^{2}}}\n\\end{align}\n\\tag{5.3}\\]\nIf \\(H_0\\) is true, then \\(t\\) will have a Student’s \\(t\\)-distribution with \\(n-2\\) degrees of freedom.\nThe only real difference between the least squares slope \\(b_1\\) and the coefficient of correlation \\(r\\) is the measurement scale2.\nTherefore, the information they provide about the utility of the least squares model is to some extent redundant.\nFurthermore, the slope \\(b_1\\) gives us additional information on the amount of increase (or decrease) in \\(y\\) for every 1-unit increase in \\(x\\).\nFor this reason, the slope is recommended for making inferences about the existence of a positive or negative linear relationship between two variables."
  },
  {
    "objectID": "05_Correlation.html#the-coefficient-of-determination",
    "href": "05_Correlation.html#the-coefficient-of-determination",
    "title": "\n5  Correlation Coefficient and the Coefficient of Determination\n",
    "section": "\n5.2 The Coefficient of Determination",
    "text": "5.2 The Coefficient of Determination\nThe second measure of how well the model fits the data involves measuring the amount of variability in \\(y\\) that is explained by the model using \\(x\\).\nWe start by examining the variability of the variable we want to learn about. We want to learn about the response variable \\(y\\). One way to measure the variability of \\(y\\) is with \\[\nSS_{yy} = \\sum\\left(y_i-\\bar{y}\\right)^2\n\\]\nNote that \\(SS_{yy}\\) does not include the model or \\(x\\). It is just a measure of how \\(y\\) deviates from its mean \\(\\bar{y}\\).\nWe also have the variability of the points about the line. We can measure this with the sum of squares error \\[\nSSE = \\sum \\left(y_i - \\hat{y}_i\\right)^2\n\\]\nNote that SSE does include \\(x\\). This is because the fitted line \\(\\hat{y}\\) is a function of \\(x\\).\nHere are a couple of key points regarding sums of squares:\n\nIf \\(x\\) provides little to no useful information for predicting \\(y\\), then \\(SS_{yy}\\) and \\(SSE\\) will be nearly equal.\nIf \\(x\\) does provide valuable information for predicting \\(y\\), then \\(SSE\\) will be smaller than \\(SS_{yy}\\).\nIn the extreme case where all points lie exactly on the least squares line, \\(SSE = 0\\).\n\nHere’s an example to illustrate:\nSuppose we have data for two variables, hours studied (x) and test scores (y). If studying time doesn’t help predict the test score, the variation in test scores (measured by \\(SS_{yy}\\)) will be similar to the error in the prediction (measured by \\(SSE\\)). However, if studying time is a good predictor, the prediction errors will be much smaller, making \\(SSE\\) significantly smaller than \\(SS_{yy}\\). If the relationship between study time and test scores is perfect, then the error would be zero, resulting in \\(SSE = 0\\).\n\n5.2.1 Proportion of Variation Explained\nWe want to explain as much of the variation of \\(y\\) as possible. So we want to know just how much of that variation is explained by using linear regression model with \\(x\\). We can quantify this variation explained by taking the difference \\[\n\\begin{align}\n    SSR = SS_{yy}-SSE\n\\end{align}\n\\tag{5.4}\\]\nSSR is called the sum of squares regression.\nWe calculate the proportion of the variation of \\(y\\) explained by the regression model using \\(x\\) by calculating3 \\[\n\\begin{align}\n    r^2 = \\frac{SSR}{SS_{yy}}\n\\end{align}\n\\tag{5.5}\\]\n\\(r^2\\) is called the coefficient of determination4\nPractical Interpretation:\nAbout \\(100(r^2)\\%\\) of the sample variation in \\(y\\) (measured by the total sum of squares of deviations of the sample \\(y\\)-values about their mean \\(\\bar{y}\\)) can be explained by (or attributed to) using \\(x\\) to predict \\(y\\) in the straight-line model.\n\nExample 5.3 (Example 5.2 revisited) We can find the coefficient of determination using the summary function with an lm object.\n\nlibrary(datasets)\n\nfit = lm(Volume~Girth, data = trees)\n\nfit |&gt; summary()\n\n\nCall:\nlm(formula = Volume ~ Girth, data = trees)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that 93.53% of the variability in the volume of the trees can be explained by the linear model using girth to predict the volume.\nIf we want to find the correlation coefficient, we can just use the cor function on the dataframe. This will find the correlation coefficient for each pair of variables in the dataframe. Note that there can only be quantitative variables in the dataframe in order this function to work.\n\ntrees |&gt; cor()\n\n           Girth    Height    Volume\nGirth  1.0000000 0.5192801 0.9671194\nHeight 0.5192801 1.0000000 0.5982497\nVolume 0.9671194 0.5982497 1.0000000\n\n\nSo the correlation between Girth and Volume is 0.9671."
  },
  {
    "objectID": "05_Correlation.html#footnotes",
    "href": "05_Correlation.html#footnotes",
    "title": "\n5  Correlation Coefficient and the Coefficient of Determination\n",
    "section": "",
    "text": "Note: The two tests are equivalent in simple linear regression only.↩︎\nThe estimated slope is measured in the same units as \\(y\\). However, the correlation coefficient \\(r\\) is independent of scale.↩︎\nIn simple linear regression, it can be shown that this quantity is equal to the square of the simple linear coefficient of correlation \\(r\\).↩︎\nNote that some software will denote the coefficient of determination as \\(R^2\\).↩︎"
  },
  {
    "objectID": "06_Using.html#using-the-model-for-estimation-and-prediction",
    "href": "06_Using.html#using-the-model-for-estimation-and-prediction",
    "title": "\n6  Using the Simple Linear Model\n",
    "section": "\n6.1 Using the Model for Estimation and Prediction",
    "text": "6.1 Using the Model for Estimation and Prediction\nNow that we have fit the model\\[\ny = {\\beta}_0 + {\\beta}_1 x + \\varepsilon\n\\] and assessed how good of fit the model is, we can now use the model estimation and prediction.\nRecall that the mean of \\(y_{i}\\) for some value of \\(x_{i}\\) is the population line \\(\\beta_{0}+\\beta_{1}x_{i}\\) evaluated at \\(x_{i}\\).\nSo we can estimate the mean of \\(y_{i}\\) for some value of \\(x_{i}\\) by evaluating the model estimated with the least squares estimators: \\[\\begin{align*}\n\\hat{y}_{i} & =b_0+b_1x_{i}\n\\end{align*}\\]\nWe say \\(\\hat{y}_{i}\\) is a point estimator for the population mean \\(\\beta_{0}+\\beta_{1}x_{i}\\).\n\n6.1.1 The Sampling Distribution of \\(\\hat{y}\\)\n\nWe will want to make an inference for the population mean response at some value of the predictor variable \\(x_{i}\\).\nWe have a point estimator \\(\\hat{y}_{i}\\). We will now examine the sampling distribution of \\(\\hat{y}_{i}\\) and use it to make a confidence interval for the mean response \\(\\beta_{0}+\\beta_{1}x_{i}\\).\n\n6.1.2 Linear Combination of \\(y\\)\n\nWe will denote the value of \\(x\\) at which we want to estimate the mean response as \\(x_{h}\\). So the value of \\(y\\) at \\(x_{h}\\) will be \\(y_{h}\\)\nWe write \\(\\hat{y}_{h}\\) as \\[\\begin{align*}\n\\hat{y}_{h} & =\\underbrace{b_0}_{(3.2)}+\\underbrace{b_1}_{(3.1)}x_{h}\\\\\n& =\\sum c_{i}y_{i}+\\sum k_{i}y_{h}x_{h}\\\\\n& =\\sum\\left(c_{i}+k_{i}x_{h}\\right)y_{h}\n\\end{align*}\\]\nThus, \\(\\hat{y}_{j}\\) is a linear combination of the observed \\(y_{i}\\) which are normally distributed. Then by Theorem 4.2, \\(\\hat{y}_{j}\\) is normally distributed.\n\n6.1.3 The Mean of \\(\\hat{y}_h\\)\n\nUsing Theorem 4.2, we have the mean as \\[\\begin{align*}\n\\sum\\left(c_{i}+k_{i}x_{h}\\right)E\\left[y_{h}\\right] & =\\left(\\underbrace{\\sum c_{i}}_{=1}+x_{h}\\underbrace{\\sum k_{i}}_{=0}\\right)\\left(\\beta_{0}+\\beta_{1}x_{h}\\right)\\\\\n& =\\beta_{0}+\\beta_{1}x_{h}\n\\end{align*}\\]\n\n6.1.4 The Variance of \\(\\hat{y}_h\\)\n\nUsing Theorem 4.3, we have the variance as \\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\n\\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sum\\left(c_{i}+k_{i}x_{h}\\right)^{2}{Var\\left[y_{h}\\right]}\\\\\n& =\\sum\\left(\\frac{1}{n}-\\bar{x}\\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}x_{h}\\right)^{2}\\sigma^{2}\\\\\n& =\\sigma^{2}\\sum\\left(\\frac{1}{n}+\\frac{\\left(x_{i}-\\bar{x}\\right)\\left(x_{h}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)^{2}\\\\\n& =\\sigma^{2}\\sum\\left(\\frac{1}{n^{2}}+2\\left(\\frac{1}{n}\\right)\\frac{\\left(x_{i}-\\bar{x}\\right)\\left(x_{h}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{i}-\\bar{x}\\right)^{2}\\left(x_{h}-\\bar{x}\\right)^{2}}{\\left(\\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(\\frac{1}{n}+2\\left(\\frac{1}{n}\\right)\\frac{\\left(x_{h}-\\bar{x}\\right)\\sum\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}{\\left(\\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\n\n\n\nSo the sampling distribution of \\(\\hat{y}_{h}\\) is \\[\n\\begin{align}\n\\hat{y}_{h} & \\sim N\\left(\\beta_{0}+\\beta_{1}x_{h},\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\right)\n\\end{align}\n\\tag{6.1}\\]\nWe will need to estimate \\(\\sigma^{2}\\) with \\(s^{2}\\). This will mean that the confidence interval is a \\(t\\) interval.\n\n6.1.5 Confidence Interval for the Mean Response\nA \\(\\left(100-\\alpha\\right)100\\%\\) confidence interval for the mean response is \\[\n\\begin{align}\n\\hat{y}_{h} \\pm t_{\\alpha/2}\\sqrt{s^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)}\n\\end{align}\n\\tag{6.2}\\]"
  },
  {
    "objectID": "06_Using.html#predicting-the-response",
    "href": "06_Using.html#predicting-the-response",
    "title": "\n6  Using the Simple Linear Model\n",
    "section": "\n6.2 Predicting the Response",
    "text": "6.2 Predicting the Response\n\n6.2.1 The Predicted Response\nPreviously, we estimated the mean of all \\(y\\)s for some value of \\(x_{h}\\).\nSuppose we want to predict one value of the response variable \\(y\\) for some value of \\(x_{h}\\). We will denote this predicted value as \\(y_{h\\left(pred\\right)}\\).\n\n6.2.2 Prediction When the True Line is Known\nOur best point predictor will be the mean (since it is the most likely value). If we knew the the true regression line, then we could predict at \\[\\begin{align*}\ny_{h\\left(pred\\right)} & =\\beta_{0}+\\beta_{1}x_{h}\n\\end{align*}\\]\nThe variance of \\(y_{h\\left(pred\\right)}\\) would be \\[\\begin{align*}\nVar\\left[y_{h}\\right] & =\\sigma^{2}\n\\end{align*}\\]\nThen we can be \\(\\left(1-\\alpha\\right)100\\%\\) confident that the predicted value \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nz_{\\alpha/2}\\sigma\n\\end{align*}\\] units away from the line.\nIf we don’t know \\(\\sigma\\), then we could estimate it with \\(s\\) and we can be \\(\\left(1-\\alpha\\right)100\\%\\) confident that the predicted value \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nt_{\\alpha/2}s\n\\end{align*}\\] units away from the line.\n\n6.2.3 Predicting When the True Line is Unknown\nOf course, we do not know the true regression line. We will need to estimate it first.\nUsing the least squares estimators, we will predict at \\[\\begin{align*}\n\\hat{y}_{h} & =b_0+b_1x_{h}\n\\end{align*}\\]\n\n6.2.4 The Variance of the Predicted Response\nSince \\(\\hat{y}_{h}\\) is a random variable, it will have a sampling distribution. From Equation 6.1, that sampling distribution has a variance of \\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nThus, the variance of the prediction of \\(y_{h\\left(pred\\right)}\\) will be the sum of the variance of the response variable: \\[\\begin{align*}\n\\sigma^{2}\n\\end{align*}\\] and the variance of the fitted line: \\[\\begin{align*}\n\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nSo the variance of \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nVar\\left[y_{h\\left(pred\\right)}\\right] & =\\sigma^{2}+\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(1+\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nSince \\(y\\) is normally distributed, then we have a \\(\\left(100-\\alpha\\right)100\\%\\) prediction interval for \\(y_{h\\left(pred\\right)}\\) as \\[\n\\begin{align}\n{\\hat{y}_{h} \\pm t_{\\alpha/2}\\sqrt{s^{2}\\left(1+\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)}}\n\\end{align}\n\\]#eq-w2_21\n\nExample 6.1 (Example 5.2 revisited) Let’s examine the trees data from Example 5.2.\nRecall the least squares fit:\n\nlibrary(datasets)\nlibrary(tidyverse)\n\nggplot(data=trees, aes(x=Girth, y=Volume))+\n  geom_point()+\n  geom_smooth(method='lm',formula=y~x,se = F)\n\n\n\nfit = lm(Volume~Girth, data=trees)\nfit\n\n\nCall:\nlm(formula = Volume ~ Girth, data = trees)\n\nCoefficients:\n(Intercept)        Girth  \n    -36.943        5.066  \n\n\nTo find the confidence and prediction intervals, we must construct a new data frame with the value of \\(X_h\\). This value is then used, along with the lm object, to the predict function. If no value of \\(X_h\\) is provided, then predict will provide intervals for all values of \\(x\\) found in the data used in lm.\nRecall the least squares fit:\n\n# get a 95% confidence interval for the mean Volume\n# when girth is 16\nxh=data.frame(Girth=16)\n\nfit |&gt; predict(xh,interval=\"confidence\",level=0.95)\n\n       fit      lwr      upr\n1 44.11024 42.01796 46.20252\n\n# 95% prediction interval for one value of Volume when\n# girth is 16\nfit |&gt; predict(xh,interval=\"prediction\",level=0.95)\n\n       fit     lwr      upr\n1 44.11024 35.1658 53.05469\n\n\nWe can plot the confidence interval for all values of \\(x\\) by using the geom_smooth command in ggplot:\n\nggplot(data=trees, aes(x=Girth, y=Volume)) + \n  geom_point() + \n  geom_smooth(method='lm',formula=y~x)\n\n\n\n\nWe can plot prediction intervals by adding them manually:\n\npred_int = fit |&gt; predict(interval=\"prediction\",level=0.95) |&gt; as.data.frame()\ndat = cbind(trees, pred_int)\n\nggplot(data=dat, aes(x=Girth, y=Volume)) +\n  geom_point() +\n  geom_smooth(method='lm',formula=y~x) +\n  geom_line(aes(y=lwr), color = \"red\", linetype = \"dashed\") +\n  geom_line(aes(y=upr), color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method=lm, se=TRUE)\n\n\n\n\nNote that the prediction interval (the red dashed lines) is wider than the prediction interval. This is because the prediction interval has the extra source of variability.\n\n\n6.2.5 Extrapolation and Precision\nWhen using the least squares prediction equation to estimate the mean value of \\(y\\) or to predict a particular value of \\(y\\) for values of \\(x\\) outside the range of your sample data, you may encounter much larger errors than expected. This practice is known as extrapolation.\nEven though the least squares model might fit the data well within the range of sample \\(x\\) values, it can poorly represent the true model for values of \\(x\\) outside this range.\nAs the sample size \\(n\\) increases, the width of the confidence interval decreases. In theory, you can achieve as precise an estimate of the mean value of \\(y\\) as desired for any given \\(x\\) by selecting a large enough sample.\nSimilarly, the prediction interval for a new value of \\(y\\) also becomes narrower as \\(n\\) increases. However, the prediction interval has a lower limit, which is reflected in the formula:\n\\[\n\\hat{y} \\pm z_{\\alpha/2} \\sigma\n\\]\nThis means that no matter how large the sample, the interval can’t shrink below a certain size unless you reduce the standard deviation of the regression model, \\(\\sigma\\). To make more accurate predictions for new values of \\(y\\), you must improve the model—either by using a curvilinear relationship with \\(x\\), adding new independent variables, or both."
  },
  {
    "objectID": "07_Checking.html#residual-diagnostics",
    "href": "07_Checking.html#residual-diagnostics",
    "title": "\n7  Checking the Linearity and Constant Variance Assumptions\n",
    "section": "\n7.1 Residual Diagnostics",
    "text": "7.1 Residual Diagnostics\n\n7.1.1 Model Assumptions\nLet’s review the assumptions for the simple linear regression model:\n\nThe mean of the probability distribution of \\(\\varepsilon\\) is 0.\nThe variance of the probability distribution of \\(\\varepsilon\\) is constant for all settings of the independent variable \\(x\\).\nThe probability distribution of \\(\\varepsilon\\) is normal.\nThe errors associated with any two different observations are independent.\n\nAfter fitting the model, we will need to check these assumptions.\n\n7.1.2 Residuals\nWe check the assumptions of the model by examining the residuals: \\[\n\\begin{align}\n{e_{i}  =y_{i}-\\hat{y}_{i}}\n\\end{align}\n\\tag{7.1}\\]\nWe do this since the assumptions, with the exception of the linearity assumption, are based on the error terms \\(\\varepsilon_i\\). We can think of \\(e_i\\) as an observed value of \\(\\varepsilon_i\\).\n\n7.1.3 Properties of Residuals\nBelow are some properties of the residuals: \\[\n\\begin{align}\n\\sum e_{i} & =0\\\\\n\\sum x_{i}e_{i} & =0\\\\\n\\sum\\hat{y}_{i}e_{i} & =0 \\\\\n\\sum y_{i} & =\\sum\\hat{y}_{i}\n\\end{align}\n\\tag{7.2}\\]\nClearly, from Equation 7.2, the mean of the residuals is \\[\n\\begin{align}\n    \\bar{e}_i=0\n\\end{align}\n\\tag{7.3}\\]\nThe variance of all \\(n\\) residuals, \\(e_1,\\ldots,e_n\\) is \\[\n\\begin{align}\n\\frac{\\sum\\left(e_{i}-\\bar{e}\\right)^{2}}{n-2} & =\\frac{\\sum e_{i}^{2}}{n-2}\\\\\n& =\\frac{SSE}{n-2}\\\\\n& =MSE\\\\\n& =s^{2}\n\\end{align}\n\\tag{7.4}\\]\n\n7.1.4 Semistudentized Residuals\nIt will be helpful to studentize each residuals. As always, we do this by subtracting off the mean, \\(\\bar{e}_{i}\\), and dividing by the standard error of \\(e_{i}\\).\nWe know by Equation 7.3 that \\(\\bar{e}_{i}=0\\).\nIn Equation 7.4, we said the variance of the sample of the \\(e_{i}\\)’s is MSE. For each individual \\(e_{i}\\), the standard error is not quite \\(\\sqrt{MSE}\\). The actual standard error is dependent on the predictor variable(s). We will discuss this more in multiple regression.\nFor now, we will use the approximation \\(\\sqrt{MSE}\\) and calculate \\[\\begin{align*}\ne_{i}^{*} & =\\frac{e_{i}-\\bar{e}}{\\sqrt{MSE}}\\\\\n& =\\frac{e_{i}}{\\sqrt{MSE}}\n\\end{align*}\\] We call \\(e_{i}^{*}\\) the semistudentized residual since the standard error is an approximation."
  },
  {
    "objectID": "07_Checking.html#the-linearity-assumption",
    "href": "07_Checking.html#the-linearity-assumption",
    "title": "\n7  Checking the Linearity and Constant Variance Assumptions\n",
    "section": "\n7.2 The Linearity Assumption",
    "text": "7.2 The Linearity Assumption\n\n7.2.1 Residual Plots\nWe can check the linearity assumption by plotting the residuals vs the predictor variable or plotting the residuals vs the fitted values.\nWe usually examine a scatterplot to determine if a linear relationship between \\(x\\) and \\(y\\) is appropriate. There are times when the scatterplot makes it difficult to see if a nonlinear relationship exists. This may be the case if the observed \\(y\\) are close to the fitted line \\(\\hat{y}_i\\). This usually means the slope is steep.\n\nExample 7.1 (Weight and Height Data) In this example, we will consider the weights (in kg) and heights (in m) of 16 women ages 30-39. The dataset is from kaggle.\n\nlibrary(tidyverse)\n\ndat = read_csv(\"Weight_Height.csv\")\n\nggplot(dat, aes(x=Weight, y=Height))+\n  geom_point()\n\n\n\n#fit the model\nfit = lm(Weight~Height, data=dat)\n\n#plot with regression line\nggplot(dat, aes(x=Weight, y=Height))+\n  geom_point()+\n  geom_smooth(method=\"lm\", formula=y~x, se=F)\n\n\n\n#make dataset with Weight, the fitted values, and residuals\ndat2 = tibble(x = dat$Weight, \n              yhat = fit$fitted.values, \n              e = fit$residuals)\n\n#plot x by residuals\nggplot(dat2, aes(x=x, y=e))+\n  geom_point()+\n  geom_hline(yintercept = 0, col=\"red\")\n\n\n\n#plot fitted values by residuals\nggplot(dat2, aes(x=yhat, y=e))+\n  geom_point()+\n  geom_hline(yintercept = 0, col=\"red\")\n\n\n\n\n\n\n7.2.2 Plotting against Predictor Variable or Fitted Values\nPlotting the residuals against \\(x\\) will provide the same information as plotting the residuals against \\(\\hat{y}\\) for the simple linear regression model.\nWhen more predictor variables are considered, then plotting against the \\(x\\) variables and plotting against \\(\\hat{y}\\) may provide different information. It is usually helpful to plot both in that case.\n\n7.2.3 Data Transformation for Linearity\nWhen the linearity assumption does not hold (as seen in the residual plots), then a nonlinear model may be considered or a transformation on either \\(x\\) or \\(y\\) can be attempted to make the relationship linear.\n\n7.2.4 Transforming \\(x\\)\n\nTransforming the response variable \\(y\\) may lead to issues with other assumptions such as the constant variance assumption or the normality of \\(\\varepsilon\\) assumption.\nIf our only concern is the linearity assumption, then transforming \\(x\\) will be the best option. This transformation may be a square root transformation \\(\\sqrt{X}\\), a log transformation \\(\\log{X}\\), or some power transformation \\(X^{p}\\) were \\(p\\) is some real number.\nSometimes a transformation of \\(x\\) will not be enough to satisfy the linearity assumption. In that case, the simple linear regression model should be abandoned in favor of a nonlinear model."
  },
  {
    "objectID": "07_Checking.html#homogeneity-of-variance",
    "href": "07_Checking.html#homogeneity-of-variance",
    "title": "\n7  Checking the Linearity and Constant Variance Assumptions\n",
    "section": "\n7.3 Homogeneity of Variance",
    "text": "7.3 Homogeneity of Variance\n\n7.3.1 Residual Plots\nAs we did previously, we can plot the residuals against the predictor variable \\(x\\) or against the fitted values \\(\\hat{y}\\) to help determine whether the variance of the error term \\(\\varepsilon\\) is constant.\nWhen the variance is constant, we say the model has homoscedasticity. When the variance is nonconstant, we say the model has heteroscedasticity.\n\n7.3.2 Absolute Residuals and Squared Residuals\nWhen examining a residual plot for non-constant variance, we look for any clear changes in the spread of the residuals. One common clear pattern seen when heteroscedasticity is present is a cone pattern.\nWe are usually not concerned about the sign of the residual when examining for heteroscedasticity. Thus, it is common to plot the absolute residuals or the squared residuals vs the predictor variable or fitted values.\nUsually a least squares line is then fit to the absolute residual or squared residuals plot. If this line has a significant slope, then this gives evidence that the variance is nonconstant.\n\nExample 7.2 (Diastolic Blood Pressure Data) We will model the diastolic blood pressure by the age of 54 healthy adult women. The data are found in Kutner et al1.\n\nlibrary(tidyverse)\n\ndat = read.table(\"bloodpressure.txt\", header=T)\n\nfit = lm(dbp~age, data=dat)\n\n#add the fit line without using geom_smooth\nggplot(dat, aes(x=age, y=dbp))+\n  geom_point()+\n  geom_abline(slope = fit$coefficients[2],\n              intercept = fit$coefficients[1])\n\n\n\n\nBy examining the scatterplot of dbp vs age, we already see evidence of nonconstant variance.\n\ndat$e = fit |&gt; resid()\ndat$yhat = fit |&gt; fitted()\n\n#use the geom_smooth function to add a least squares line\n#for the residuals, the least squares line will always be #horizontal at 0\nggplot(dat, aes(x=age, y=e))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n#plot the squared residuals and add least squares line\nggplot(dat, aes(x=age, y=e^2))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n#plot the squared residuals and add least squares line\nggplot(dat, aes(x=age, y=abs(e)))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n\nIn any of the residual plots we examine, we see the “cone” shape of the residuals which indicates the variance is nonconstant.\n\n\n7.3.3 Tests for Heteroscedasticity\nWe can set up a hypothesis test for heteroscedasticity: \\[\\begin{align*}\nH_0:&\\text{ the variance is constant}\\\\\nH_a:&\\text{ the variance is non-constant}\n\\end{align*}\\]\nThe procedures we will use usually test for variance that increases or decreases over the values of \\(x\\). That is, the spread of the points about the line is a cone shape.\n\n7.3.4 Levene’s Test and Brown-Forsythe Test\nThe Levene test2 starts by dividing the range of the predictor variable \\(x\\) into \\(k\\) intervals. For each of the intervals, calculate the mean of the residuals in that interval \\(\\bar{e}_{j}\\) where \\(j\\) denotes the \\(j\\)th interval.\nNow, define the absolute deviation from the mean \\[\nd_{ik}=|e_{ik}-\\bar{e}_{j}|\n\\]\nAn ANOVA F-test is then performed on the \\(k\\) groups. The ANOVA F-test will be discussed more later in the course. A small p-value is evidence that the variance is non-constant over the values of \\(x\\).\nThe Brown-Forsythe test3 is a modification of the Levene test in which the median \\(\\tilde{e}_k\\) is used instead of the mean \\(\\bar{e}_l\\). This test is robust against nonnormal errors.\n\n7.3.5 Breusch-Pagan Test\nA second test for non-constant variance is the Breusch-Pagan test4. This test assumes the variance for each \\(\\varepsilon_i\\) is related to the values of \\(x\\) in the following way: \\[\n\\ln \\sigma^2_i = \\gamma_0 + \\gamma_1 X_i\n\\] Hence, a linear regression is assumed between \\(\\sigma^2\\) and \\(x\\). We can fit the line by squaring the residuals and regressing on \\(x\\). The third plot in example above shows the squared residuals plotted against \\(x\\) and the fitted line.\nIf the variance is non-constant, then the slope of this line will be non-zero. Thus, a test for the slope is conducted. A small p-value is evidence that the variance is non-constant.\nBecause it is testing the slope, the Brown-Forsythe test assumes the error terms are independent and normally distributed.\n\nExample 7.3 (Example 7.2 revisited) Let’s examine the blood pressure data from Example 7.2 again.\nIf you want the Brown-Forsythe test, then you will need to determine the \\(k\\) groups yourself and then use the bf.test function in onewaytests package.\nIn the lmtest package, the Breusch-Pagan test can be conducted by using the bptest function.\n\nlibrary(tidyverse)\nlibrary(lmtest)\n\ndat = read.table(\"bloodpressure.txt\", header=T)\n\nfit = lm(dbp~age, data=dat)\n\n#Breusch-Pagan test\nbptest(dbp~age, data=dat)\n\n\n    studentized Breusch-Pagan test\n\ndata:  dbp ~ age\nBP = 12.541, df = 1, p-value = 0.0003981\n\n\nSince the p-value is low, then there is sufficient evidence to conclude the variance is not constant through different values of \\(x\\)."
  },
  {
    "objectID": "07_Checking.html#footnotes",
    "href": "07_Checking.html#footnotes",
    "title": "\n7  Checking the Linearity and Constant Variance Assumptions\n",
    "section": "",
    "text": "Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied Linear Statistical Models McGraw-Hill/lrwin series operations and decision sciences.↩︎\nLevene, H. (1960) Robust Tests for Equality of Variances. In: Olkin, I., Ed., Contributions to Probability and Statistics, Stanford University Press, Palo Alto, 278-292.↩︎\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for the equality of variances. Journal of the American statistical association, 69(346), 364-367.↩︎\nBreusch, T. S., & Pagan, A. R. (1979). A simple test for heteroscedasticity and random coefficient variation. Econometrica: Journal of the econometric society, 1287-1294.↩︎"
  },
  {
    "objectID": "08_Checking2.html#checking-for-outliers",
    "href": "08_Checking2.html#checking-for-outliers",
    "title": "\n8  Checking the Normality and Independence Assumptions and Outliers\n",
    "section": "\n8.1 Checking for Outliers",
    "text": "8.1 Checking for Outliers\n\n8.1.1 Outliers With Respect to the Predictor Variable and the Response Variable\nWhen checking for outliers, we must think in terms of two dimensions since we have two variables, \\(x\\) and \\(y\\).\nSince we are interested in modeling the response variable \\(y\\) in the model, we will mainly be concerned with outliers with respect to \\(y\\). However, outliers with respect to \\(x\\) may also be of concern if it affects the fitted line.\n\n8.1.2 Effect on the Fitted Line\nA demonstration on the effect of outliers on the fitted line can be found at http://www.jpstats.org/Regression/ch_03_04.html#sub1_1\n\n\n8.1.3 Detecting Outliers with Semistudentized Residual Plots\nSince the effect on the fitted line is determined mainly by how far the point is from the line, we will identify outliers by examining the residuals, in particular, the semistudentized residuals \\[\\begin{align*}\ne_{i}^{*} & =\\frac{e_{i}-\\bar{e}}{\\sqrt{MSE}}\\\\\n& =\\frac{e_{i}}{\\sqrt{MSE}}\n\\end{align*}\\]\nWe can plot \\(e_{i}^{*}\\) against \\(x\\) or against \\(\\hat{y}\\). A general rule of thumb is any value of \\(e_{i}^{*}\\) below -4 or above 4 should be considered an outlier. Note that this rule is only applicable to large \\(n\\).\nWe will discuss other methods for detecting outliers after we cover multiple regression.\n\nExample 8.1 (Calculating Semistudentized Residuals) Let’s examine 50 observations of \\(x\\) and \\(y\\):\n\nlibrary(tidyverse)\n\ndat = read_csv(\"example_08_01.csv\")\n\ndat |&gt; \n  ggplot(aes(x=x,y=y))+\n    geom_point()+\n    geom_smooth(method = \"lm\")\n\n\n\nfit = lm(y~x, data=dat)\n\n#calculate semistudentized residuals\ndat = dat |&gt; \n  mutate(e.star = fit$residuals / summary(fit)$sigma)\n\ndat |&gt; \n  ggplot(aes(x=x, y=e.star))+\n    geom_point()\n\n\n\n\nWe can see from the semistudentized plot that there is one observation with a value of \\(e^*\\) below -3. Although this is not below the rule of thumb of -4, we note that our sample size \\(n\\) is only moderate and not large. So we may want to investigate an observation with a value of \\(e^*\\) below -3.\n\nOnce we see there is a potential outlier, we must investigate why it is an outlier. If the observation is unusually small or large due to a data recording error, then perhaps the value can be corrected or just deleted from the dataset. If we cannot determine this is the cause of the outlier for certain, then we should not remove the observation. This observation could be unusual just due to chance."
  },
  {
    "objectID": "08_Checking2.html#correlated-error-terms",
    "href": "08_Checking2.html#correlated-error-terms",
    "title": "\n8  Checking the Normality and Independence Assumptions and Outliers\n",
    "section": "\n8.2 Correlated Error Terms",
    "text": "8.2 Correlated Error Terms\n\n8.2.1 Assumption of Independence\nSince we are assuming the random error term \\(\\varepsilon\\) are normal, we want to check to see the uncorrelated errors assumption. If there is no correlation between the residuals, then we can assume independence.\n\n8.2.2 Residual Sequence Plots\nThe usual cause of correlation in the residuals is data taken in some type of sequence such as time or space. When the error terms are correlated over time or some other sequence, we say they are serially correlated or autocorrelated.\nWhen the data are taken in some sequence, a sequence plot of the residuals may show a pattern indicating autocorrelation. In a sequence plot, the residuals are plotted against the observation index \\(i\\). If there is no autocorrelation, then the residuals should be “randomly” spread about zero. If there is a pattern, then there is evidence of autocorrelation.\n\n8.2.3 Autocorrelation Function Plot\nSometime a residual sequence plot may not show an obvious pattern but autocorrelation may still exist.\nAnother plot that helps examine correlation that may not be visible in the sequence plot is the autocorrelation function plot (ACF).\nIn the ACF plot, correlations are calculated between residuals some \\(k\\) index away. That is, \\[\n\\begin{align}\nr_{k} & =\\widehat{Cor}\\left[e_{i},e_{i+k}\\right]\\\\\n& =\\frac{\\sum_{i=1}^{n-k}\\left(e_{i}-\\bar{e}\\right)\\left(e_{i+k}-\\bar{e}\\right)}{\\sum_{i=1}^{n}\\left(e_{i}-\\bar{e}\\right)^{2}}\n\\end{align}\n\\tag{8.1}\\]\nIn an ACF plot, \\(r_k\\) is plotted for varying values of \\(k\\). If the value of \\(r_k\\) is larger in magnitude than some threshold shown on the plot (usually a 95% confidence interval), then we consider this evidence of autocorrelation.\n\n8.2.4 Tests for Autocorrelation\nIn addition to examining serial plots and ACF plots, tests can be conducted for significant autocorrelation. In each of these tests, the null hypothesis is there is no autocorrelation.\n\n8.2.5 Durbin-Watson Test\nThe Durbin-Watson1 test is for autocorrelation at \\(k=1\\) in Equation 8.1. That is, it tests for correlation one index (one time point) away.\nThe Durbin-Watson test can be conducted in R with the dwtest function in the lmtest package.\n\n8.2.6 Ljung-Box Test\nThe Ljung-Box2 test differs from the Durbin-Watson test in that it tests for overall correlation over all lags up to \\(k\\) in Equation 8.1. For example, if \\(k=4\\) then the Ljung-Box test is for significant autocorrelation over all lags up to \\(k=4\\).\nThe Ljung-Box test can be conducted in R with the Box.test function with the argument type=(\"Ljung\"). This function is in base R.\n\n8.2.7 Breusch-Godfrey Test\nThe Breusch-Godfrey34 test is similar to the Ljung-Box test in that it tests for overall correlation over all lags up to \\(k\\). The difference between the two test is not of concern in the regression models we will examine in this course. When using time series models, then the Breusch-Godfrey test is preferred over the Ljung-Box test due to asymptotic justification.\nThe Breusch-Godfrey test can be conducted in R with the bgtest function in the lmtest package.\n\nExample 8.2 (Portland Crime Data) Let’s look at data collected on the mean temperature for each day in Portland, OR, and the number of non-violent crimes reported that day. The crime data was part of a public database gathered from www.portlandoregon.gov. The data are presented in order by day. The variable \\(x\\) in the dataset is the day index number.\n\nlibrary(tidyverse)\nlibrary(lmtest)\nlibrary(forecast)\n\ndat = read_csv(\"PortlandWeatherCrime.csv\")\n\n#the file does not have a name for the index variable\nnames(dat)[1] = \"day\"\n\ndat |&gt; \n  ggplot(aes(x=Mean_Temp, y=Num_Total_Crimes))+\n    geom_point()+\n    geom_smooth(method=\"lm\")\n\n\n\nfit = lm(Num_Total_Crimes~Mean_Temp, data=dat)\nfit |&gt; summary()\n\n\nCall:\nlm(formula = Num_Total_Crimes ~ Mean_Temp, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-168.198  -41.055    0.149   40.455  183.680 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 281.0344     6.4456   43.60   &lt;2e-16 ***\nMean_Temp     4.3061     0.1116   38.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.69 on 1765 degrees of freedom\nMultiple R-squared:  0.4576,    Adjusted R-squared:  0.4573 \nF-statistic:  1489 on 1 and 1765 DF,  p-value: &lt; 2.2e-16\n\ndat$res = fit |&gt; resid()\n\nggplot(dat, aes(x=day, y=res))+\n  geom_point()\n\n\n\n\nWe can see that the residuals have a pattern where the values at the lower levels of the index tend to be below zero whereas the values at the higher levels of the index tend to be above zero. This is evidence of autocorrelation in the residuals.\n\nggAcf(dat$res)\n\n\n\n\nThe values of the ACF at all lags are beyond the blue guideline for significant autocorreleation.\nNote that in the Ljung-Box test and the Breusch-Godfrey test below, we tested up to lag 7. We chose this lag since the data was taken over time and it would make sense for values at seven days apart to be similar. That is, we expect the number of crimes on Mondays to be similar, the number of crimes on Tuesdays to be similar, etc.\n\ndwtest(fit)\n\n\n    Durbin-Watson test\n\ndata:  fit\nDW = 0.66764, p-value &lt; 2.2e-16\nalternative hypothesis: true autocorrelation is greater than 0\n\nBox.test(dat$res, lag=7,type=\"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  dat$res\nX-squared = 3865, df = 7, p-value &lt; 2.2e-16\n\nbgtest(fit, order=7)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 7\n\ndata:  fit\nLM test = 977.84, df = 7, p-value &lt; 2.2e-16\n\n\n\nWhen the assumption of independence is violated, then a difference in the \\(y\\) values could help remove the autocorrelation. This difference is \\[\ny^{\\prime} = y_i - y_{i-k}\n\\] where \\(k\\) is some max lag where autocorrelation is significant. This difference \\(Y^{\\prime}\\) is then regressed on \\(x\\). This difference may not help, in which case a time series model would be necessary."
  },
  {
    "objectID": "08_Checking2.html#normality-of-the-residuals",
    "href": "08_Checking2.html#normality-of-the-residuals",
    "title": "\n8  Checking the Normality and Independence Assumptions and Outliers\n",
    "section": "\n8.3 Normality of the Residuals",
    "text": "8.3 Normality of the Residuals\n\n8.3.1 The Normality Assumption\nIf we do not have normality of the error terms, then the t-tests and t-intervals for \\({\\beta}_0\\) and \\({\\beta}_1\\) would not be valid.\nFurthermore, the confidence interval for the mean response and the prediction interval for the response would not be valid.\nWe can check the normality of error terms by examining the residuals of the fitted line.\n\n8.3.2 Graphically Checking Normality\nWe can graphically check the distribution of the residuals. The two most common ways to do this is with a histogram or with a normal probability plot.\nAnother (more general) name for a normal probability plot is a normal quantile-quantile (QQ) plot.\nFor a histogram, we check to see if the shape is approximately close to that of a normal distribution.\nFor a QQ plot, we check to see if the points approximately follow a straight line. Major departures from a straight line indicates nonnormality.\nIt is important to note that we will never see an exact normal distribution is real-world data. Thus, we will always look for approximate normality in the residuals.\nThe inferences discussed previously are still valid for small departure of normality. However, major departures from normality will lead to incorrect p-values in the hypothesis tests and incorrect coverages in the intervals.\n\n8.3.3 Examples of QQ-plots\nBelow are some examples of histograms and QQ-plots for some simulated datasets.\n\n\n\n\n\n(a) Normal - Histogram\n\n\n\n\n\n(b) Normal - QQ plot\n\n\n\n\n\n\n\n(c) Right skewed - Histogram\n\n\n\n\n\n(d) Right skewed - QQ plot\n\n\n\n\n\n\n\n(e) Heavy right skewed - Histogram\n\n\n\n\n\n(f) Heavy right skewed - QQ plot\n\n\n\n\n\n\n\n(g) Left skewed - Histogram\n\n\n\n\n\n(h) Left skewed - QQ plot\n\n\n\n\n\n\n\n(i) Heavy tails - Histogram\n\n\n\n\n\n(j) Heavy tails - QQ plot\n\n\n\n\n\n\n\n(k) No tails - Histogram\n\n\n\n\n\n(l) No tails - QQ plot\n\n\n\nFigure 8.1: Examples of QQ-Plots\n\n\n\n8.3.4 The Shapiro-Wilk Test\nThere are a number of hypothesis test for normality. The most popular test is the Shapiro-Wilk5 test. This test has been found to have the most power among many of the other tests for normality6.\nIn the Shapiro-Wilk test, the null hypothesis is that the data are normally distributed and the alternative is that the data are not normally distributed.\nThis test can be conducted using the shapiro.test function in base R."
  },
  {
    "objectID": "08_Checking2.html#footnotes",
    "href": "08_Checking2.html#footnotes",
    "title": "\n8  Checking the Normality and Independence Assumptions and Outliers\n",
    "section": "",
    "text": "Durbin, J., & Watson, G. S. (1951). Testing for Serial Correlation in Least Squares Regression. II. Biometrika. Vol 38. (pp. 159-177).↩︎\nLjung, G. M., & Box, G. E. (1978). On a measure of lack of fit in time series models. Biometrika, 65(2), 297-303.↩︎\nBreusch, T. S. (1978). Testing for Autocorrelation in Dynamic Linear Models. Australian Economic Papers. 17: 334–355.↩︎\nGodfrey, L. G. (1978). Testing Against General Autoregressive and Moving Average Error Models when the Regressors Include Lagged Dependent Variables. Econometrica. 46: 1293–1301↩︎\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52(3-4), 591-611.↩︎\nRazali, N. M., & Wah, Y. B. (2011). Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of statistical modeling and analytics, 2(1), 21-33.↩︎"
  },
  {
    "objectID": "09_Tidymodels.html#specifying-the-model",
    "href": "09_Tidymodels.html#specifying-the-model",
    "title": "\n9  Simple Linear Regression with Tidymodels\n",
    "section": "\n9.1 Specifying the Model",
    "text": "9.1 Specifying the Model\nIn tidymodels, the first step in model building is to define the type of model you want to fit. The parsnip package is the tool within tidymodels that handles model specification. Unlike the traditional approach where you directly use a modeling function like lm(), parsnip separates the model definition from the model fitting, making the process more modular and adaptable to different engines (i.e., different computational backends).\n\n9.1.1 Model Specification with parsnip\n\nThe primary function to specify a model in parsnip is linear_reg(), which indicates that you are building a linear regression model. However, linear_reg() does not fit the model by itself. Instead, it allows you to define the general structure of the model, which is then paired with a computational engine. An engine is the specific function or package that will perform the calculations. For linear regression, the default engine is \"lm\", which corresponds to R’s base lm() function.\nLet’s break down how the model specification works:\n\n# Specify a linear regression model\nlm_model = linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\n\nlinear_reg(): This specifies that we are interested in a linear regression model. The model type is abstracted, allowing us to focus on the statistical method (linear regression) rather than the computational details.\n\nset_engine(\"lm\"): This command specifies that we will use the \"lm\" engine to estimate the linear regression parameters. parsnip supports other engines, such as glmnet for penalized regression models and stan for Bayesian regression models, which allows for great flexibility in model building.\n\n9.1.2 Hyperparameters and Engine Customization\nYou can also specify hyperparameters, which are settings that influence the model fitting process. For simple linear regression with the \"lm\" engine, there are no specific hyperparameters to set, but with more complex models (such as regularized regression), you might need to tune parameters like the penalty term or the mixture parameter for Lasso or Ridge regression (discussed in a later chapter).\nAn example of specifying a model with hyperparameters (for demonstration purposes):\n# Example for models that require hyperparameters\nridge_model = linear_reg(penalty = 0.1, mixture = 0) |&gt;\n  set_engine(\"glmnet\")\nAlthough these hyperparameters are not necessary for simple linear regression, parsnip enables seamless transition to more complex models, and this flexibility is a key feature of the tidymodels framework.\n\n9.1.3 Choosing the Right Model and Engine\nOne of the strengths of the parsnip package is that it decouples the conceptual model (linear regression) from the computational engine (e.g., \"lm\", \"glmnet\", \"stan\"). This allows the user to switch engines without changing the model structure.\nFor example, suppose you wanted to fit the same linear regression model using a Bayesian approach via the stan engine:\n# Specify a Bayesian linear regression model\nbayesian_lm_model = linear_reg() |&gt;\n  set_engine(\"stan\")\nIn this case, the model structure remains the same, but the estimation procedure differs (frequentist vs. Bayesian). This level of abstraction enhances the flexibility of your workflow, especially as you explore different modeling approaches.\n\n9.1.4 Benefits of Model Specification with parsnip\n\nThe model specification process in parsnip offers several key benefits. Its modularity allows for the separation of model specification from model fitting, enabling you to reuse the same model definition across different datasets or engines. This framework provides flexibility, making it easy to switch between various engines, such as lm(), glmnet(), or stan, without altering the core model structure.\nAdditionally, parsnip enhances scalability by simplifying the process of extending models. For example, moving from simple linear regression to more complex models like Ridge or Lasso regression only requires adjusting the engine and hyperparameters, without the need to rewrite your code.\nIn this section, we have seen how to specify a linear regression model using parsnip and set the engine to \"lm\". Next, we will define the relationship between the predictor and response variables and organize our workflow before fitting the model."
  },
  {
    "objectID": "09_Tidymodels.html#defining-the-workflow",
    "href": "09_Tidymodels.html#defining-the-workflow",
    "title": "\n9  Simple Linear Regression with Tidymodels\n",
    "section": "\n9.2 Defining the Workflow",
    "text": "9.2 Defining the Workflow\nIn the tidymodels ecosystem, workflows serve as a central organizing structure that ties together the components of a model-building process. A workflow in tidymodels allows you to combine different elements, such as the model specification, preprocessing steps, and the formula that defines the relationship between the predictor(s) and response. This section will explain the importance of workflows and demonstrate how to create one for simple linear regression.\n\n9.2.1 What is a Workflow?\nA workflow can be thought of as a blueprint that organizes how data flows through different steps of the modeling process. It encapsulates model specification and data preprocessing tasks, which can include transformations such as scaling, normalization, or encoding categorical variables. In traditional modeling approaches, these steps are often written separately, which can lead to code that is harder to manage and prone to errors, especially as the complexity of the model increases.\nWith workflows, the entire process becomes more organized and reproducible. The key benefit is that you don’t need to repeatedly specify how the data should be prepared or how the model should be fit. Once a workflow is defined, it can be reused or modified easily to fit different models or datasets.\n\n9.2.2 Creating a Workflow for Simple Linear Regression\nFor a simple linear regression model, our workflow consists of two main components: the formula that defines the relationship between the response and predictor variables, and the model specification itself. Since simple linear regression does not require extensive preprocessing (like handling categorical variables or missing data), the workflow is relatively straightforward.\nLet’s begin by constructing the workflow for predicting miles per gallon (mpg) using horsepower (hp) from the mtcars dataset. Recall that the model specification (lm_model) was defined in the previous section.\n\n# Define a workflow\nlm_workflow = workflow() |&gt;\n  add_model(lm_model) |&gt;\n  add_formula(mpg ~ hp)\n\nIn this code:\n\n\nworkflow() initializes an empty workflow.\n\nadd_model() adds the linear regression model (lm_model) that was specified earlier using the parsnip package.\n\nadd_formula() defines the model’s formula, which indicates that the response variable mpg is modeled as a function of the predictor hp.\n\nThe formula-based approach used in this workflow is intuitive for most users familiar with R’s base modeling functions, such as lm() and glm(). However, workflows also support more advanced techniques, such as specifying custom preprocessing steps with the recipes package, which we will explore later.\n\n9.2.3 The Formula Interface\nThe formula interface in tidymodels is a key aspect of defining relationships between variables. The tilde (~) symbol is used to separate the response variable (on the left-hand side) from the predictor(s) (on the right-hand side). In the example mpg ~ hp, the model will predict mpg using hp as the predictor. This is a simple linear regression model, where the model assumes a linear relationship between these two variables.\nIf there were multiple predictors, the formula would include them on the right-hand side, separated by a plus sign (+), such as mpg ~ hp + wt + qsec, indicating a multiple linear regression model. The formula syntax allows for flexible model building, including interaction terms (* or :) and transformations (e.g., log() or polynomial terms).\n\n9.2.4 Preprocessing in Workflows\nOne of the primary advantages of using workflows is the ability to integrate preprocessing steps seamlessly into the model-building process. While simple linear regression may not require extensive preprocessing, workflows can easily handle more complex tasks, such as:\n\n\nStandardizing predictors: Scaling variables to have mean 0 and standard deviation 1.\n\nHandling missing data: Imputing missing values.\n\nEncoding categorical variables: Converting factors into dummy or one-hot encoded variables.\n\nFor example, suppose that instead of horsepower (hp), we had a categorical variable representing car transmission type (am), and we wanted to include it as a predictor. In this case, we could use the recipes package to preprocess the data by encoding the categorical variable. The workflow can be extended as follows:\n# Example of adding preprocessing using recipes\nrecipe = recipe(mpg ~ hp + am, data = mtcars) |&gt;\n  step_dummy(all_nominal_predictors())\n\nlm_workflow = workflow() |&gt;\n  add_model(lm_model) |&gt;\n  add_recipe(recipe)\nIn this example, recipe() initializes a preprocessing recipe that converts the categorical predictor am into a set of dummy variables. step_dummy() is one of many preprocessing steps available in the recipes package.\n\n9.2.5 Why Use Workflows?\nWorkflows provide several key benefits when building models with tidymodels:\n\n\nReproducibility: By encapsulating the entire modeling process into a workflow, the model can be easily reproduced, reducing the chances of human error. Each step is clearly defined and can be executed in a consistent manner.\n\nModularity: You can modify or swap out different components of the workflow—such as changing the model from linear regression to a different type of regression—without rewriting significant portions of your code.\n\nSeparation of Concerns: Workflows help you organize your code by clearly separating data preprocessing, model specification, and model fitting. This makes it easier to debug, extend, or modify your models later on.\n\nPreprocessing Integration: Workflows enable the integration of complex preprocessing pipelines, ensuring that data transformations and model fitting occur seamlessly within a single structure. This is especially useful for more advanced models, where data preprocessing steps are often critical to model performance.\n\nIn this section, we introduced the concept of workflows in tidymodels, which serves as a crucial tool for organizing and managing the different components of the modeling process. For simple linear regression, the workflow was relatively simple, involving a formula and a model specification. However, workflows can be extended to handle more complex models and preprocessing tasks as we will see in later chapters. By using workflows, you ensure that your model-building process is organized, reproducible, and adaptable to future changes."
  },
  {
    "objectID": "09_Tidymodels.html#fitting-the-model",
    "href": "09_Tidymodels.html#fitting-the-model",
    "title": "\n9  Simple Linear Regression with Tidymodels\n",
    "section": "\n9.3 Fitting the Model",
    "text": "9.3 Fitting the Model\nOnce the model and workflow have been defined, the next step is to fit the model to the data. In tidymodels, this is done using the fit() function, which estimates the model parameters (such as the intercept and slope in simple linear regression) based on the data provided. The fitting process involves using the workflow to apply the model specification and formula to the dataset.\n\n9.3.1 The fit() Function\nThe fit() function is a central function in the modeling process that takes the workflow and the data as inputs. It applies the formula and estimates the model’s parameters by minimizing the SSE (for linear regression models) or another appropriate criterion depending on the model type.\nFor our simple linear regression model, we fit the model using the mtcars dataset, where the response variable is mpg (miles per gallon) and the predictor is hp (horsepower).\n\n# Fit the model\nlm_fit = lm_workflow |&gt;\n  fit(data = mtcars)\n\nIn this command:\n\n\nlm_workflow: The workflow containing the model and the formula for simple linear regression.\n\ndata = mtcars: The dataset used to fit the model. The model will use this data to estimate the parameters \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope).\n\nThe fit() function then processes the data, applies the specified formula (mpg ~ hp), and estimates the model parameters using the specified engine (lm in this case). In the background, tidymodels calls the R base lm() function to fit the model, but the workflow structure abstracts these technical details, making it easier to switch between engines without altering the core syntax.\n\n9.3.2 Model Output\nAfter fitting the model, it’s essential to inspect the model’s output to understand the estimated relationship between the predictor and response variables. The fitted workflow object (lm_fit) contains a number of important details, including the estimated coefficients for the linear model, the residuals, and performance metrics.\nLet’s start by examining the output of the fitted model:\n\n# View the model fit summary\nlm_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nmpg ~ hp\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)           hp  \n   30.09886     -0.06823  \n\n\nThe output of lm_fit provides an overview of the fitted model, including the coefficients for the intercept and slope. This information is crucial for interpreting the relationship between the predictor and the response variable. Specifically, the estimated intercept (\\(b_0\\)) represents the expected value of mpg when hp is zero, and the slope (\\(b_1\\)) represents the expected change in mpg for each one-unit increase in hp.\n\n9.3.3 Extracting Model Coefficients\nTo better understand the results, you can extract the estimated coefficients from the model using the tidy() function from the broom package, which is part of the tidymodels ecosystem. This function provides a clean, easy-to-read summary of the model’s coefficients, along with other important statistics such as standard errors, t-values, and p-values.\n\n# Extract coefficients\ntidy(lm_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  30.1       1.63       18.4  6.64e-18\n2 hp           -0.0682    0.0101     -6.74 1.79e- 7\n\n\nThis table shows the estimated coefficients for the intercept and slope:\n\n\nIntercept: The estimated intercept (\\(b_0\\)) is 30.10, meaning that when hp is zero, the predicted mpg is approximately 30.10. In this example, the interpretation is not practical.\n\nSlope: The estimated slope (\\(b_1\\)) is -0.068, indicating that for each additional unit of horsepower, the mpg decreases by about 0.068 miles per gallon, on average. The negative sign confirms an inverse relationship between mpg and hp.\n\nAdditionally, the table provides the standard errors, t-statistics, and p-values, which are useful for assessing the statistical significance of the estimated coefficients. In this case, both the intercept and the slope have very small p-values (less than 0.001), suggesting that they are statistically significant at conventional significance levels (e.g., 0.05).\n\n9.3.4 Model Predictions\nAfter fitting the model, it’s often useful to make predictions based on new or existing data. The predict() function allows you to generate predictions from the fitted model. In this case, we’ll generate predictions using the original dataset (mtcars), though in practice, you would typically use the model to predict on new, unseen data.\n\n# Make predictions on the original data\npredictions = lm_fit |&gt;\n  predict(new_data = mtcars)\n\n# Add predictions to the original data\nmtcars_with_preds = mtcars |&gt;\n  select(mpg, hp) |&gt; \n  mutate(predicted_mpg = predictions$.pred)\n\n# View the predictions\nhead(mtcars_with_preds)\n\n                   mpg  hp predicted_mpg\nMazda RX4         21.0 110      22.59375\nMazda RX4 Wag     21.0 110      22.59375\nDatsun 710        22.8  93      23.75363\nHornet 4 Drive    21.4 110      22.59375\nHornet Sportabout 18.7 175      18.15891\nValiant           18.1 105      22.93489\n\n\nIn this example, we use the predict() function to generate predicted values of mpg for each observation in the mtcars dataset. These predictions are stored in a new column called predicted_mpg. Comparing the predicted values to the actual values allows us to assess how well the model fits the data.\n\n9.3.5 Performance Metrics\nOnce predictions have been made, it’s important to evaluate the model’s performance. One common metric for regression models is the Root Mean Squared Error (RMSE), which measures the average difference between the predicted and actual values. Lower RMSE values indicate better model performance.\nYou can calculate the RMSE and other performance metrics using the yardstick package, another component of the tidymodels framework.\n\n# Calculate RMSE\nmetrics = lm_fit |&gt;\n  predict(new_data = mtcars) |&gt;\n  bind_cols(mtcars) |&gt;\n  metrics(truth = mpg, estimate = .pred)\n\n# View performance metrics\nmetrics\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       3.74 \n2 rsq     standard       0.602\n3 mae     standard       2.91 \n\n\nThis will output the RMSE, along with other useful metrics like the Mean Absolute Error (MAE) and R-squared.\n\n\nrmse: The Root Mean Squared Error (RMSE) is approximately 3.74, meaning that, on average, the model’s predictions are off by about 3.74 miles per gallon.\n\nrsq: The R-squared value is 0.60, indicating that the model explains about 60% of the variation in mpg.\n\nIn this section, we demonstrated how to fit a simple linear regression model using the fit() function in tidymodels. We also explored how to extract and interpret the model coefficients, make predictions, and evaluate model performance using metrics like RMSE and R-squared. By abstracting the fitting process into a workflow, tidymodels makes it easy to manage and evaluate models in a structured and reproducible way. In the next section, we will delve into model diagnostics to assess how well the assumptions of linear regression are met in this fitted model."
  },
  {
    "objectID": "09_Tidymodels.html#model-diagnostics",
    "href": "09_Tidymodels.html#model-diagnostics",
    "title": "\n9  Simple Linear Regression with Tidymodels\n",
    "section": "\n9.4 Model Diagnostics",
    "text": "9.4 Model Diagnostics\nAfter fitting a linear regression model, it’s essential to check the underlying assumptions to ensure that the model is appropriate for the data and that the results can be trusted. These assumptions include linearity, constant variance (homoscedasticity) of the residuals, independence of errors, and normally distributed errors. Violations of these assumptions can lead to biased estimates or misleading inference. In this section, we will focus on diagnosing these assumptions using residual plots and other techniques, leveraging the tidymodels framework.\n\n9.4.1 Residuals and Fitted Values\nTo start our diagnostics, we will generate the residuals and fitted values using the augment() function from the broom package, which adds additional information such as residuals and fitted values to the original dataset. To use augment we must first extract the fitted model from the overall fitted workflow with the extract_fit_engine function.\n\n# Augment the model with residuals and fitted values\npredictions = extract_fit_engine(lm_fit) |&gt; \n  augment()\n\n# View the first few rows of augmented data\nhead(predictions)\n\n# A tibble: 6 × 8\n    ..y    hp .fitted .resid   .hat .sigma  .cooksd .std.resid\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1  21     110    22.6 -1.59  0.0405   3.92 0.00374      -0.421\n2  21     110    22.6 -1.59  0.0405   3.92 0.00374      -0.421\n3  22.8    93    23.8 -0.954 0.0510   3.92 0.00173      -0.253\n4  21.4   110    22.6 -1.19  0.0405   3.92 0.00210      -0.315\n5  18.7   175    18.2  0.541 0.0368   3.93 0.000389      0.143\n6  18.1   105    22.9 -4.83  0.0432   3.82 0.0369       -1.28 \n\n\n\n9.4.2 Plotting Residuals vs. Fitted Values\nA key diagnostic plot for linear regression is the residuals versus fitted values plot. This plot helps assess the assumption of linearity and constant variance (homoscedasticity). In a well-behaved model, the residuals should exhibit no clear pattern and should have constant spread across all levels of the fitted values.\nLet’s create the residuals vs. fitted values plot:\n\n# Plot residuals vs. fitted values\nggplot(predictions, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs. Fitted Values\") +\n  theme_minimal()\n\n\n\n\nIn this plot:\n\nThe x-axis represents the fitted values (predicted mpg from the model).\nThe y-axis represents the residuals (differences between observed and predicted mpg).\nThe dashed red line at zero represents where the residuals would lie if the model were perfect.\n\n9.4.3 Checking Homoscedasticity (Constant Variance)\nAnother assumption of linear regression is that the residuals should have constant variance, also known as homoscedasticity. If the spread of residuals increases or decreases with the fitted values, the assumption of constant variance is violated, which can lead to inefficient estimates and biased standard errors.\nIn the residuals vs. fitted values plot, this would manifest as a “funnel” shape, where the residuals get larger (or smaller) as the fitted values increase. If you observe this pattern, it suggests heteroscedasticity, and you may need to consider a transformation of the response variable or use robust standard errors to account for the non-constant variance.\n\n9.4.4 Normality of Residuals\nLinear regression assumes that the residuals are normally distributed. A common way to check this assumption is to create a Q-Q (quantile-quantile) plot, which compares the quantiles of the residuals to the quantiles of a normal distribution. If the residuals are normally distributed, the points should fall roughly along a 45-degree line.\nLet’s create a Q-Q plot using ggplot2:\n\n# Q-Q plot of residuals\nggplot(predictions, aes(sample = .resid)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal()\n\n\n\n\nIn the Q-Q plot:\n\nIf the residuals are normally distributed, the points will fall along the 45-degree reference line.\nDeviations from this line, especially at the tails, suggest that the residuals may not follow a normal distribution.\n\nIf the normality assumption is violated (e.g., heavy tails or skewness), it may be necessary to apply a transformation to the response variable or explore alternative models that do not assume normality, such as generalized linear models.\n\n9.4.5 Checking for Independence of Errors\nAnother important assumption is that the errors (or residuals) should be independent of one another. This is especially relevant in time series data, where errors may be autocorrelated (i.e., residuals at one time point may be related to residuals at nearby time points). We can used the residuals in an ACF plot to determine if there is significant autocorrelation at a lag.\n\nlibrary(forecast)\n\nggAcf(predictions$.resid)\n\n\n\n\n\n9.4.6 Detecting Outliers\nTo identify potential outliers, you can plot the standardized residuals, which are residuals divided by their estimated standard deviation. Residuals with an absolute value greater than 2 or 3 are often considered potential outliers.\n\n# Plot standardized residuals\nggplot(predictions, aes(x = .fitted, y = .std.resid)) +\n  geom_point() +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Fitted Values\", y = \"Standardized Residuals\", title = \"Standardized Residuals vs. Fitted Values\") +\n  theme_minimal()\n\n\n\n\nIn this plot:\n\nPoints outside the dashed lines (at standardized residuals of -2 and 2) are potential outliers. These points should be investigated further to understand why they deviate from the model’s predictions."
  },
  {
    "objectID": "09_Tidymodels.html#summary",
    "href": "09_Tidymodels.html#summary",
    "title": "\n9  Simple Linear Regression with Tidymodels\n",
    "section": "\n9.5 Summary",
    "text": "9.5 Summary\nIn this chapter, we demonstrated how to use the tidymodels framework to fit a simple linear regression model. We covered the key steps, including specifying the model with parsnip, building a workflow, and fitting the model. Additionally, we explored how to extract model results and perform basic diagnostics.\nThe tidymodels ecosystem offers a flexible and organized approach to model building, and in future chapters, we will expand this framework to handle more complex regression models and advanced techniques such as cross-validation and model tuning."
  },
  {
    "objectID": "10_Intro_Multiple.html#types-of-models",
    "href": "10_Intro_Multiple.html#types-of-models",
    "title": "10  An Intro to Multiple Regression",
    "section": "10.1 Types of Models",
    "text": "10.1 Types of Models\nWhen we discuss the simple linear regression model \\[\n\\begin{align*}\n    y = \\beta_0 + \\beta_1 x +\\varepsilon\n\\end{align*}\n\\] stated previously that it is “simple” because there is only one predictor variable.\nThe model is “linear in the parameters” because every parameter is only to the first power and is not multiplied or divided by another parameter.\nThe model is also “linear in the predictor variable” because \\(x\\) appears only with an exponent of one (instead of \\(x^2\\), \\(x^{1/2}\\), etc.).\nA “linear model” means it is linear in the parameters (not necessarily linear in the predictor variables). A model that is linear in both the parameters and the predictor variables is called a first-order model."
  },
  {
    "objectID": "10_Intro_Multiple.html#multiple-predictor-variables",
    "href": "10_Intro_Multiple.html#multiple-predictor-variables",
    "title": "10  An Intro to Multiple Regression",
    "section": "10.2 Multiple Predictor Variables",
    "text": "10.2 Multiple Predictor Variables\nOften when modeling some response variable \\(y\\), one predictor variable may not be adequate. Thus, more multiple predictor variable can be used to model \\(y\\).\nAs in simple linear regression, we will assume models that are linear in the parameters.\nWe will present the multiple linear regression model that is linear in the parameters but not necessarily linear in the predictor variables. This type of model is called the general linear regression model.\n\n10.2.1 The Multiple Regression Model\nThe general model is \\[\n\\begin{align}\ny_{i}= & \\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\cdots+\\beta_{p-1}x_{i,p-1}+\\varepsilon_{i}\\\\\n= & \\beta_{0}+\\sum_{k=1}^{p-1}\\beta_{k}x_{ik}+\\varepsilon_{i}\\\\\n& \\varepsilon\\overset{iid}{\\sim}N\\left(0,\\sigma^{2}\\right)\n\\end{align}\n\\tag{10.1}\\] where \\[\n\\begin{align*}\n& \\beta_{0},\\beta_{1},\\ldots,\\beta_{p-1}\\text{ are parameters}\\\\\n& x_{i1},\\ldots,x_{i,p-1}\\text{ are known constants}\\\\\n& i=1,\\ldots,n\n\\end{align*}\n\\]\nThe predictor variable \\(x_{k}\\) can be raised to some power or transformed in some other way. Also, the predictor variable can be the product of two variables. When this is the case, we say the term is an interaction term. We will discuss these more later.\n\n\n10.2.2 Assumptions About the Predictor Variables\nIn multiple regression, we are interested in how the predictor variables relate to the response variable. In particular, we want to know:\n\nHow important are the difference predictor variables in modeling \\(y\\)?\nWhat is the effect of a given predictor variable on predicting \\(y\\)?\nAre any of the predictor variables unnecessary in modeling \\(y\\) and therefore be dropped from the model?\nAre there any predictor variables not included in the model that should be included?\n\nTheses questions are relatively simple to answer if the predictor variables are uncorrelated among themselves.\nUnfortunately, in real world application, especially for observational studies, the predictor variables tend to be correlated among themselves. When this is the case, we say that multicollinearity exists.\nJust due to chance, there will always be some correlation among the predictor variables. In general, we will assume the correlation among the predictor variables is low. If the correlation is high, then this may present problems in the analyses. As we discuss the inferences and assumptions going further, we will discuss the problem of multicollinearity in more detail."
  },
  {
    "objectID": "10_Intro_Multiple.html#estimating-the-multiple-regression-model",
    "href": "10_Intro_Multiple.html#estimating-the-multiple-regression-model",
    "title": "10  An Intro to Multiple Regression",
    "section": "10.3 Estimating the Multiple Regression Model",
    "text": "10.3 Estimating the Multiple Regression Model\n\n10.3.1 Minimizing the SSE\nAs we did in the simple linear regression case, we want to fit model Equation 10.1 to the observed data.\nThe fitted line in the multiple regression case is \\[\n\\begin{align}\n\\hat{y}_i = b_0 + b_1 x_{i1} + b_2 x_{i2}+\\cdots +b_{p-1}x_{i,p-1}\n\\end{align}\n\\tag{10.2}\\] The estimates \\(b_0,b_1,\\ldots,b_{p-1}\\) are found by minimizing the squared distances between the observed values \\(y_i\\) and the fitted values \\(\\hat{y}_i\\). The sum of the squared distances is now \\[\n\\begin{align}\nQ=\\sum \\left(y_i-\\left(b_0 + b_1 x_{i1} + b_2 x_{i2}+\\cdots +b_{p-1}x_{i,p-1}\\right)\\right)^2\\\n\\end{align}\n\\tag{10.3}\\] in the multiple regression case.\nNote that model Equation 10.1 is no longer a line. It is a plane when \\(p=3\\) and a hyperplane when \\(p&gt;3\\).\n\n\n10.3.2 Case with Two Predictor Variables\nWhen there are two predictor variables (\\(p=3\\)), we will take three partial derivatives of Equation 10.3 with respect to \\(b_0\\), \\(b_1\\), and \\(b_2\\). This leads us to \\[\n\\begin{align}\n\\frac{\\partial Q}{\\partial b_{0}} & =-2\\sum\\left(y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{2i}\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{1}} & =-2\\sum x_{i1}\\left(y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{2i}\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{2}} & =-2\\sum x_{i2}\\left(y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{2i}\\right)\n\\end{align}\n\\tag{10.4}\\]\n\n\n10.3.3 The Normal Equations\nSetting the partial derivative equal to zero and rearranging the terms lead us to the normal equations \\[\n\\begin{align}\n\\sum y_{i} & =nb_{0}\\sum x_{i1}+b_{2}\\sum x_{i2}\\\\\n\\sum x_{i1}y_{i} & =b_{0}\\sum x_{i1}+b_{1}\\sum x_{i1}^{2}+b_{2}\\sum x_{i1}x_{i2}\\\\\n\\sum x_{i2}y_{i} & =b_{0}\\sum x_{i2}+b_{1}\\sum x_{i1}x_{i2}+b_{2}\\sum x_{i2}^{2}\n\\end{align}\n\\tag{10.5}\\]\n\n\n10.3.4 The Least Squares Estimators\nSolving the normal equations for \\(b_0\\), \\(b_1\\), and \\(b_2\\) gives use the least squares estimators \\[\n\\begin{align}\nb_{1} & =\\frac{\\left(\\sum x_{i2}^{2}\\right)\\left(\\sum x_{i1}y_{i}\\right)-\\left(\\sum x_{i1}x_{i2}\\right)\\left(\\sum x_{i2}y_{i}\\right)}{\\left(\\sum x_{i1}^{2}\\right)\\left(\\sum x_{i2}^{2}\\right)-\\left(\\sum x_{i1}x_{i2}\\right)^{2}}\\\\\nb_{2} & =\\frac{\\left(\\sum x_{i1}^{2}\\right)\\left(\\sum x_{i2}y_{i}\\right)-\\left(\\sum x_{i1}x_{i2}\\right)\\left(\\sum x_{i1}y_{i}\\right)}{\\left(\\sum x_{i1}^{2}\\right)\\left(\\sum x_{i2}^{2}\\right)-\\left(\\sum x_{i1}x_{i2}\\right)^{2}}\\\\\nb_{0} & =\\bar{Y}-b_{1}\\bar{X}_{1}-b_{2}\\bar{X}_{2}\n\\end{align}\n\\tag{10.6}\\]\nWe see that the expression for the least squares estimators become cumbersome even for \\(p=3\\). As more variables are added to the model, the equations become even more cumbersome.\nWe can simplify notation by utilizing matrices to represent the model. We will present some basic notation and operations for matrices and then present the model using matrices."
  },
  {
    "objectID": "10_Intro_Multiple.html#a-primer-on-matrices",
    "href": "10_Intro_Multiple.html#a-primer-on-matrices",
    "title": "10  An Intro to Multiple Regression",
    "section": "10.4 A Primer on Matrices",
    "text": "10.4 A Primer on Matrices\n\n10.4.1 Matrices\nA matrix is a rectangular array of elements arranged in rows and columns.\nAn example of a matrix is: \\[\\begin{align*}\n\\left[\\begin{array}{ccc}\n8.3 & 70 & 10.3\\\\\n8.6 & 65 & 10.3\\\\\n8.8 & 63 & 10.2\\\\\n10.5 & 72 & 16.4\\\\\n\\end{array}\\right]\n\\end{align*}\\]\nThis matrix represents some of the data from the dataset. The values in the first column represents Girth, the second column represents Height, and the third column represents Volume.\nEach row corresponds to a tree. The first row represents the values for the first tree. It has 8.3 for Girth, 70 for Height, and 10.3 for Volume.\nSo this matrix gives the values of three variables for four trees.\n\n\n10.4.2 Notation\nEach value of the matrix is called an element of that matrix. We denote the elements as \\(a_{ij}\\) for the element in the \\(i\\)th row and the \\(j\\)th column. Note that the first subscript identifies the row number and the second the column number.\nSo for the matrix above, the elements can be denotes as \\[\n\\begin{align*}\n\\left[\\begin{array}{ccc}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\\\\\na_{41} & a_{42} & a_{43}\n\\end{array}\\right]\n\\end{align*}\n\\]\nA matrix may be denoted by a symbol such as \\(\\bf{A}\\), \\(\\bf{X}\\), or \\(\\bf{Z}\\). The matrix could also be a greek symbol such as \\(\\bf{\\Omega}\\). The symbol is in boldface to identify that it refers to a matrix.\nThus, we might define for the above matrix; \\[\n\\begin{align*}\n\\bf{A} =\\left[\\begin{array}{ccc}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\\\\\na_{41} & a_{42} & a_{43}\n\\end{array}\\right]\n\\end{align*}\n\\]\nAnother notation we could use is: \\[\n\\textbf{A}=\\left[a_{ij}\\right]\\qquad i=1,\\ldots,4; j=1,2,3\n\\]\nThis notation avoids the need for writing out all elements of the matrix by stating only the general element.\nSometimes we will specify the matrix with the dimension below the matrix symbol. For example, a \\(r\\) x \\(c\\) matrix can be expressed as\n\n\n10.4.3 Matrix Dimensions\nThe dimension of the matrix above is 4 x 3, since there are four rows and three columns.\nRecall that the trees dataset has 31 observations. So a matrix representing the full dataset would be 31 x 3.\nNote that in giving the dimension of a matrix, we always specify the number of rows first and then the number of columns.\nSo a \\(r\\) x \\(c\\) matrix can be expressed as \\[\n\\begin{align*}\n\\underset{r\\times c}{{\\bf A}} & =\\left[\\begin{array}{cccc}\na_{11} & a_{12} & \\cdots & a_{1c}\\\\\na_{21} & a_{22} & \\cdots & a_{2c}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\na_{r1} & a_{r2} & \\cdots & a_{rc}\n\\end{array}\\right]\n\\end{align*}\n\\] or in the compact form \\[\n\\begin{align*}\n\\underset{r\\times c}{{\\bf A}} & =\\left[a_{ij}\\right]\\qquad i=1,\\ldots,r;j=1,\\ldots,c\n\\end{align*}\n\\] Again, the dimensions may or may not be given under the matrix symbol.\n\n\n10.4.4 Square Matrices\nA matrix is said to be square if the number of rows equals the number of columns. For example, the matrices \\[\n\\begin{align*}\n\\left[\\begin{array}{cc}\na_{11} & a_{12}\\\\\na_{21} & a_{22}\n\\end{array}\\right]\n\\end{align*}\n\\] and\n\\[\n\\begin{align*}\n\\left[\\begin{array}{ccc}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\\\\\n\\end{array}\\right]\n\\end{align*}\n\\] are both square matrices.\n\n\n10.4.5 Vectors\nA matrix containing only one column is called a column vector or simply a vector.\nTwo examples are: \\[\n\\begin{align*}\n\\textbf{A}=\\left[\\begin{array}{c}\n1\\\\\n20\\\\\n7\n\\end{array}\\right] & \\qquad\\textbf{B}=\\left[\\begin{array}{c}\nb_{1}\\\\\nb_{2}\\\\\nb_{3}\\\\\nb_{4}\\\\\nb_{5}\n\\end{array}\\right]\n\\end{align*}\n\\]\nNote that the elements only have one subscript in \\(\\bf{B}\\) since there is only one column. The subscript indicates only the row.\nA matrix containing only one row is called a row vector.\nTwo examples are: \\[\n\\begin{align*}\n\\textbf{B}^{\\prime}=\\left[\\begin{array}{ccc}\n15 & 25 & 50\\end{array}\\right] & \\qquad\\boldsymbol{\\delta}^{\\prime}=\\left[\\begin{array}{cc}\n\\delta_{1} & \\delta_{2}\\end{array}\\right]\n\\end{align*}\n\\]\nWe use the prime (\\({}^\\prime\\)) symbol for row vectors for reasons to be seen next.\n\n\n10.4.6 Transpose\nThe transpose of a matrix \\(\\bf{A}\\) is another matrix, denoted by \\(\\textbf{A}^{\\prime}\\), that is obtained by interchanging corresponding columns and rows of the matrix \\(\\bf{A}\\).\nFor example, if: \\[\n\\begin{align*}\n\\underset{3\\times2}{\\textbf{A}}=\\left[\\begin{array}{cc}\n1 & 7\\\\\n12 & 4\\\\\n5 & 9\n\\end{array}\\right]\n\\end{align*}\n\\] then the transpose \\(\\bf{A}^\\prime\\) is: \\[\n\\begin{align*}\n\\underset{2\\times3}{\\textbf{A}^{\\prime}}=\\left[\\begin{array}{ccc}\n1 & 12 & 5\\\\\n7 & 4 & 9\n\\end{array}\\right]\n\\end{align*}\n\\]\nNote that the first column of \\(\\bf{A}\\) is the first row of \\(\\bf{A}^\\prime\\), and similarly the second column of \\(\\bf{A}\\) is the second row of \\(\\bf{A}^\\prime\\).\nNote that the dimension of \\(\\bf{A}\\) becomes reversed for the dimension of \\(\\bf{A}^\\prime\\).\nNote that the transpose of a column vector is a row vector, and vice versa.\nThis is the reason why we used the symbol \\(\\bf{B}^\\prime\\) earlier to identify a row vector, since it may be thought of as the transpose of a column vector \\(\\bf{B}\\).\n\n\n10.4.7 Symmetric Matrices\nA matrix is said to be symmetric if \\(\\bf{A}=\\bf{A}^\\prime\\).\nA symmetric matrix \\(\\bf{A}\\) has elements \\(a_{ij}=a_{ji}\\). Clearly, a symmetric matrix must be a square matrix.\n\n\n10.4.8 Diagonal Matrices}\nA square matrix is said to be diagonal if all of the off-diagonal elements are zero.\nFor example \\[\n\\begin{align*}\n{\\bf A} & =\\left[\\begin{array}{cccc}\na_{11} & 0 & 0 & 0\\\\\n0 & a_{22} & 0 & 0\\\\\n0 & 0 & a_{33} & 0\\\\\n0 & 0 & 0 & a_{44}\n\\end{array}\\right]\n\\end{align*}\n\\] is a diagonal matrix.\n\n\n10.4.9 Indentity Matrix\nThe identity matrix is a diagonal matrix with ones for all the diagonal elements. The identity matrix is denoted with with \\(\\bf{I}\\).\nFor Example \\[\n\\begin{align*}\n{\\bf I} & =\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n\\end{align*}\n\\] is a 4 x 4 identity matrix.\n\n\n10.4.10 Matrices and Vectors of Ones and Zeros\nA matrix of with ones for all the elements is denoted as \\[\n\\begin{align*}\n{\\bf J} & =\\left[\\begin{array}{cccc}\n1 & 1 & \\cdots & 1\\\\\n1 & 1 & \\cdots & 1\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & 1 & \\cdots & 1\n\\end{array}\\right]\n\\end{align*}\n\\]\nA vector with ones for all the elements is denoted as \\[\n\\begin{align*}\n{\\bf 1} & =\\left[\\begin{array}{c}\n1\\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{array}\\right]\n\\end{align*}\n\\]\nLikewise a vector of zeros is denoted as \\[\n\\begin{align*}\n{\\bf 0} & =\\left[\\begin{array}{c}\n0\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\n10.4.11 Matrix Addition and Subtraction\nAdding or subtracting two matrices requires that they have the same dimension.\nThe sum, or difference, of two matrices is another matrix whose elements each consist of the sum, or difference, of the corresponding elements of the two matrices.\nSuppose: \\[\n\\begin{align*}\n\\underset{3\\times2}{\\textbf{A}}=\\left[\\begin{array}{cc}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{array}\\right] & \\qquad\\underset{3\\times2}{\\textbf{B}}=\\left[\\begin{array}{cc}\n1 & 2\\\\\n2 & 3\\\\\n3 & 4\n\\end{array}\\right]\n\\end{align*}\n\\] then \\[\n\\begin{align*}\n\\underset{3\\times2}{\\textbf{A}+\\textbf{B}=} & \\left[\\begin{array}{cc}\n1+1 & 4+2\\\\\n2+2 & 5+3\\\\\n3+3 & 6+4\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n2 & 6\\\\\n4 & 8\\\\\n6 & 10\n\\end{array}\\right]\n\\end{align*}\n\\]\nSimilarly: \\[\n\\begin{align*}\n\\underset{3\\times2}{\\textbf{A}-\\textbf{B}=} & \\left[\\begin{array}{cc}\n1-1 & 4-2\\\\\n2-2 & 5-3\\\\\n3-3 & 6-4\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n0 & 2\\\\\n0 & 2\\\\\n0 & 2\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\n10.4.12 Matrix Multiplication\nThe addition and subtraction rules discussed above are fairly straight forward and similar to addition and subtraction of (non-matrix) numbers.\nMultiplication of matrices are not as straight forward as multiplication of (non-matrix) numbers.\n\nMultiplication of a Matrix by a Scalar\nA scalar is an ordinary number or a symbol representing a number.\nIn multiplication of a matrix by a scalar, every element of the matrix is multiplied by the scalar.\nFor example, suppose the matrix \\(\\textbf{A}\\) is given by \\[\n\\begin{align*}\n\\textbf{A}=\\left[\\begin{array}{cc}\n1 & 3\\\\\n5 & 7\n\\end{array}\\right]\n\\end{align*}\n\\]\nThen \\(2\\textbf{A}\\), where 2 is the scalar, equals \\[\n\\begin{align*}\n2\\textbf{A}=2\\left[\\begin{array}{cc}\n1 & 3\\\\\n5 & 7\n\\end{array}\\right] & =\\left[\\begin{array}{cc}\n2 & 6\\\\\n10 & 14\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nMultiplication of a Matrix by a Matrix\nConsider the two matrices: \\[\n\\begin{align*}\n\\underset{2\\times2}{\\textbf{A}}=\\left[\\begin{array}{cc}\n1 & 2\\\\\n3 & 4\n\\end{array}\\right] & \\qquad\\underset{2\\times2}{\\textbf{B}}=\\left[\\begin{array}{cc}\n5 & 6\\\\\n7 & 8\n\\end{array}\\right]\n\\end{align*}\n\\]\nMultiplying \\(\\bf{A}\\) by \\(\\bf{B}\\) is found by a multiplying the elements of each row vector by the elements of each each column vector and then summing the products.\nFor example, to find the element in the first row and the first column of the product \\(\\textbf{AB}\\), we work with the first row of \\(\\textbf{A}\\) and the first column of \\(\\textbf{B}\\): \\[\n\\begin{align*}\n\\begin{array}{cc}\n& \\textbf{A}\\\\\n& \\left[\\begin{array}{cc}\n{\\color{red}1} & {\\color{red}2}\\\\\n3 & 4\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\\begin{array}{c}\n\\textbf{B}\\\\\n\\left[\\begin{array}{cc}\n{\\color{red}5} & 6\\\\\n{\\color{red}7} & 8\n\\end{array}\\right]\\\\\n\\begin{array}{cc}\n& \\end{array}\n\\end{array} & =\\begin{array}{cc}\n& \\textbf{AB}\\\\\n&  \\left[\\begin{array}{cc}\n\\color{red}{\\left(1\\right)\\left(5\\right)+\\left(2\\right)\\left(7\\right)} &\\\\\n\\\\\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\\\\\n& = \\begin{array}{cc}\n& \\textbf{AB}\\\\\n&  \\left[\\begin{array}{cc}\n\\color{red}{19} &\\\\\n\\\\\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\n\\end{align*}\n\\]\nTo find the element in the first row and second column of \\(\\textbf{AB}\\): \\[\n\\begin{align*}\n\\begin{array}{cc}\n& \\textbf{A}\\\\\n& \\left[\\begin{array}{cc}\n{\\color{red}1} & {\\color{red}2}\\\\\n3 & 4\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\\begin{array}{c}\n\\textbf{B}\\\\\n\\left[\\begin{array}{cc}\n5 & \\color{red}{6}\\\\\n7 & \\color{red}{8}\n\\end{array}\\right]\\\\\n\\begin{array}{cc}\n& \\end{array}\n\\end{array} & =\\begin{array}{cc}\n& \\textbf{AB}\\\\\n&  \\left[\\begin{array}{cc}\n33&\n\\color{red}{\\left(1\\right)\\left(6\\right)+\\left(2\\right)\\left(8\\right)} \\\\\n\\\\\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\\\\\n& = \\begin{array}{cc}\n& \\textbf{AB}\\\\\n&  \\left[\\begin{array}{cc}\n33 & \\color{red}{22}\\\\\n\\\\\n\\end{array}\\right]\\\\\n\\\\\n\\end{array}\n\\end{align*}\n\\]\nContinuing this process we get \\[\n\\begin{align*}\n\\underset{2\\times2}{\\textbf{AB}} & =\\left[\\begin{array}{cc}\n\\left(1\\right)\\left(5\\right)+\\left(2\\right)\\left(7\\right) & \\left(1\\right)\\left(6\\right)+\\left(2\\right)\\left(8\\right)\\\\\n\\left(3\\right)\\left(5\\right)+\\left(4\\right)\\left(7\\right) & \\left(3\\right)\\left(6\\right)+\\left(4\\right)\\left(8\\right)\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n19 & 22\\\\\n43 & 50\n\\end{array}\\right]\n\\end{align*}\n\\]\nNote that the order in matrix multiplication is important. In general, \\(\\textbf{AB} \\ne \\textbf{BA}\\). In fact, even though the product \\(\\textbf{AB}\\) may be defined, the product \\(\\textbf{BA}\\) may not be defined at all.\nIn general, the product \\(\\textbf{AB}\\) is defined only when the number of columns in \\(\\textbf{A}\\) equals the number of rows in \\(\\textbf{B}\\).\nFor example: \\[\n\\begin{align*}\n\\underset{{\\color{red}2}\\times{\\color{blue}3}}{\\textbf{A}} & \\quad\\underset{{\\color{blue}3}\\times{\\color{red}1}}{\\textbf{B}}=\\underset{{\\color{red}2}\\times{\\color{red}1}}{\\textbf{AB}}\n\\end{align*}\n\\] is defined since the number of columns of \\(\\textbf{A}\\) (3) is equal to the number of rows of \\(\\textbf{B}\\) (3).\nHowever, note that \\[\n\\begin{align*}\n\\underset{{\\color{blue}3}\\times{\\color{red}1}}{\\textbf{B}}\\quad\\underset{{\\color{red}2}\\times{\\color{blue}3}}{\\textbf{A}}\n\\end{align*}\n\\] is not defined since the number of columns of \\(\\textbf{B}\\) (1) is not equal to the number of rows of \\(\\textbf{A}\\) (2).\nWhen obtaining the product \\(\\textbf{AB}\\), we say that \\(\\textbf{A}\\) is postmultiplied by \\(\\textbf{B}\\) or \\(\\textbf{B}\\) is premultiplied by \\(\\textbf{A}\\).\n\n\nInverse of a Matrix\nFor ordinary (non-matrix) numbers, the inverse of a number is its reciprocal. Thus, the inverse of 2 is \\(\\frac{1}{2}\\)\nA number multiplied by its inverse always equals 1: \\[\n\\begin{align*}\n&2\\cdot\\frac{1}{2}=\\frac{1}{2}\\cdot2=1\n\\end{align*}\n\\]\nIn matrix algebra, the inverse of a matrix \\(\\textbf{A}\\) is another matrix, denoted by \\(\\textbf{A}^{-1}\\), such that: \\[\n\\textbf{A}^{-1}\\textbf{A}=\\textbf{A}\\textbf{A}^{-1}=\\textbf{I}\n\\] where \\(\\textbf{I}\\) is the identity matrix.\nThus, the identity matrix \\(\\textbf{I}\\) plays the same role as the number 1 in ordinary algebra.\nAn inverse of a matrix is defined only for square matrices.\nEven so, many square matrices do not have inverses.\nIf a square matrix does have an inverse, the inverse is unique.\nIf a the inverse of a matrix does not exist, then we say the matrix is singular. If the inverse does exist, then we say the matrix is nonsingular.\n\n\n\n10.4.13 Basic Matrix Results\nBelow are some basic results for matrices presented without proof. They will be useful as we use matrices in regression. \\[\n\\begin{align}\n\\textbf{A}+\\textbf{B} & =\\textbf{B}+\\textbf{A} &\\\\\n\\left(\\textbf{A}+\\textbf{B}\\right)+\\textbf{C} & =\\textbf{A}+\\left(\\textbf{B}+\\textbf{C}\\right) &\\\\\n\\left(\\textbf{A}\\textbf{B}\\right)\\textbf{C} & =\\textbf{A}\\left(\\textbf{B}\\textbf{C}\\right)&\\\\\n\\textbf{C}\\left(\\textbf{A}+\\textbf{B}\\right) & =\\textbf{C}\\textbf{A}+\\textbf{C}\\textbf{B}&\\\\\nk\\left(\\textbf{A}+\\textbf{B}\\right) & =k\\textbf{A}+k\\textbf{B}&\\\\\n\\left(\\textbf{A}^{\\prime}\\right)^{\\prime} & =\\textbf{A}&\\\\\n\\left(\\textbf{A}+\\textbf{B}\\right)^{\\prime} & =\\textbf{A}^{\\prime}+\\textbf{B}^{\\prime}&\\\\\n\\left(\\textbf{A}\\textbf{B}\\right)^{\\prime} & =\\textbf{B}^{\\prime}\\textbf{A}^{\\prime}&\\\\\n\\left(\\textbf{A}\\textbf{B}\\textbf{C}\\right)^{\\prime} & =\\textbf{C}^{\\prime}\\textbf{B}^{\\prime}\\textbf{A}^{\\prime}&\\\\\n\\left(\\textbf{A}^{-1}\\right)^{-1} & =\\textbf{A}&\\\\\n\\left(\\textbf{A}^{\\prime}\\right)^{-1} & =\\left(\\textbf{A}^{-1}\\right)^{\\prime}&\n\\end{align}\n\\]\n\n\n10.4.14 Matrix Differentiation\nThere are a number of results when using matrix calculus which are beyond the scope of this course. We will present a few results for matrix differentiation that will be useful in multiple regression.\nIt is important to note that matrix calculus can be confusing due to notational conventions that are used in various fields. There are two main conventions (although the two are sometimes mixed by some authors) that are based how to take a derivative with respect to a vector. One convention is the numerator layout and the other is the denominator layout. Below, we will present the results using numerator layout.\nIn all the results that follow, let \\(d\\) be a scalar, \\({\\bf A}\\) be a \\(n\\times1\\) vector with elements \\([a_{i}]\\), \\({\\bf B}\\) be a \\(m\\times1\\) vector with elements \\([b_{i}]\\), and \\({\\bf C}\\) be a \\(p\\times q\\) matrix with elements \\([c_{ij}]\\).\n\nVector by a Scalar\n\\[\n\\begin{align*}\n\\frac{\\partial{\\bf A}}{\\partial d} & =\\left[\\begin{array}{c}\n\\frac{\\partial a_{1}}{\\partial d}\\\\\n\\frac{\\partial a_{2}}{\\partial d}\\\\\n\\vdots\\\\\n\\frac{\\partial a_{n}}{\\partial d}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nScalar by a Vector\n\\[\n\\begin{align*}\n\\frac{\\partial d}{\\partial{\\bf A}}  =\\left[\\begin{array}{cccc}\n\\frac{\\partial d}{\\partial a_{1}} & \\frac{\\partial d}{\\partial a_{2}} & \\cdots & \\frac{\\partial d}{\\partial a_{n}}\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nVector by a Vector\n\\[\n\\begin{align*}\n\\frac{\\partial{\\bf A}}{\\partial{\\bf B}} & =\\left[\\begin{array}{cccc}\n\\frac{\\partial a_{1}}{\\partial b_{1}} & \\frac{\\partial a_{1}}{\\partial b_{2}} & \\cdots & \\frac{\\partial a_{1}}{\\partial b_{m}}\\\\\n\\frac{\\partial a_{2}}{\\partial b_{1}} & \\frac{\\partial a_{2}}{\\partial b_{2}} & \\cdots & \\frac{\\partial a_{2}}{\\partial b_{m}}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial a_{n}}{\\partial b_{1}} & \\frac{\\partial a_{n}}{\\partial b_{2}} & \\cdots & \\frac{\\partial a_{n}}{\\partial b_{m}}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nMatrix by a Scalar\n\\[\n\\begin{align*}\n\\frac{\\partial{\\bf C}}{\\partial d} & =\\left[\\begin{array}{cccc}\n\\frac{\\partial c_{11}}{\\partial d} & \\frac{\\partial c_{12}}{\\partial d} & \\cdots & \\frac{\\partial c_{1q}}{\\partial d}\\\\\n\\frac{\\partial c_{21}}{\\partial d} & \\frac{\\partial c_{22}}{\\partial d} & \\cdots & \\frac{\\partial c_{2q}}{\\partial d}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial c_{p1}}{\\partial d} & \\frac{\\partial c_{p2}}{\\partial d} & \\cdots & \\frac{\\partial c_{pq}}{\\partial d}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nScalar by a Matrix\n\\[\n\\begin{align*}\n\\frac{\\partial d}{\\partial{\\bf C}} & =\\left[\\begin{array}{cccc}\n\\frac{\\partial d}{\\partial c_{11}} & \\frac{\\partial d}{\\partial c_{21}} & \\cdots & \\frac{\\partial d}{\\partial c_{p1}}\\\\\n\\frac{\\partial d}{\\partial c_{12}} & \\frac{\\partial d}{\\partial c_{22}} & \\cdots & \\frac{\\partial d}{\\partial c_{p2}}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial d}{\\partial c_{1q}} & \\frac{\\partial d}{\\partial c_{2q}} & \\cdots & \\frac{\\partial d}{\\partial c_{pq}}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nCommon Derivatives Involving Matrices\n\\[4\n\\begin{align*}\n& \\frac{\\partial{\\bf A}^{\\prime}{\\bf A}}{\\partial{\\bf A}}=2{\\bf A}^{\\prime}\\\\\n& \\frac{\\partial{\\bf A}^{\\prime}{\\bf B}}{\\partial{\\bf B}}=\\frac{\\partial{\\bf B}^{\\prime}{\\bf A}}{\\partial{\\bf B}}={\\bf A}^{\\prime} &  & \\text{(provided }m=n)\\\\\n& \\frac{\\partial{\\bf \\left({\\bf A}^{\\prime}{\\bf B}\\right)^{2}}}{\\partial{\\bf A}}=2{\\bf A}^{\\prime}{\\bf B}{\\bf B}^{\\prime} &  & \\text{(provided }m=n)\\\\\n& \\frac{\\partial{\\bf C}{\\bf A}}{\\partial{\\bf A}}={\\bf C} &  & \\text{(provided }q=n)\\\\\n& \\frac{\\partial{\\bf A}^{\\prime}{\\bf C}}{\\partial{\\bf A}}={\\bf C}^{\\prime} &  & \\text{(provided }p=n)\\\\\n& \\frac{\\partial{\\bf A}^{\\prime}{\\bf C}{\\bf A}}{\\partial{\\bf A}}={\\bf A}^{\\prime}\\left({\\bf C}+{\\bf C}^{\\prime}\\right) &  & \\text{(provided }n=p=q)\n\\end{align*}\n\\]\n\n\n\n10.4.15 Random Matrices\nA random matrix contains elements that are random variables.\nThus, the vector of the response vector \\[\n\\begin{align*}\n{\\bf Y} & =\\left[\\begin{array}{c}\nY_{1}\\\\\nY_{2}\\\\\n\\vdots\\\\\nY_{n}\n\\end{array}\\right]\n\\end{align*}\n\\] is a random vector since the \\(Y_i\\) elements are random variables.\n\nExpected Value\nThe expected value of \\({\\bf Y}\\) is a matrix (or vector) that has elements that are the expected values of the elements of \\({\\bf Y}\\). Thus, \\[\n\\begin{align*}\n{\\bf E}\\left[{\\bf Y}\\right] & =\\left[\\begin{array}{c}\nE\\left[Y_{1}\\right]\\\\\nE\\left[Y_{2}\\right]\\\\\n\\vdots\\\\\nE\\left[Y_{n}\\right]\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\nVariance-Covariance Matrix\nWhen working with random vectors, we will be interested in the variance of the individual elements \\[\n\\begin{align*}\nVar\\left[Y_{i}\\right]\n\\end{align*}\n\\] along with the covariance between pairs of elements \\[\n\\begin{align*}\nCov\\left[Y_{i},Y_{j}\\right] & \\text{ }i\\ne j.\n\\end{align*}\n\\]\nAll of these variances and covariances are given in the variance-covariance matrix or simply covariance matrix: \\[\n\\begin{align*}\n{\\bf Cov}\\left[{\\bf Y}\\right] & =\\left[\\begin{array}{cccc}\nVar\\left[Y_{1}\\right] & Cov\\left[Y_{1},Y_{2}\\right] & \\cdots & Cov\\left[Y_{1},Y_{n}\\right]\\\\\nCov\\left[Y_{2},Y_{1}\\right] & Var\\left[Y_{2}\\right] & \\cdots & Cov\\left[Y_{2},Y_{n}\\right]\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nCov\\left[Y_{n},Y_{1}\\right] & Cov\\left[Y_{n},Y_{2}\\right] & \\cdots & Var\\left[Y_{n}\\right]\n\\end{array}\\right]\n\\end{align*}\n\\]\nNote that \\({\\bf Cov}\\left[{\\bf Y}\\right]\\) is a symmetric matrix since \\(Cov\\left[Y_{i},Y_{j}\\right]=Cov\\left[Y_{j},Y_{i}\\right]\\)."
  }
]