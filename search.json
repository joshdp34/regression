[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 3386 Regression Analysis",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 3386 - Regression Analysis.\nPrerequisites: MTH 2311 - Linear Algebra, MTH 2321 - Calculus III, and STA 3381 - Probability and Statistics\n\nCourse Description:\nA development of regression techniques including simple linear regression, multiple regression, logistic regression and Poisson regression with emphasis on model assumptions, parameter estimation, variable selection and diagnostics."
  },
  {
    "objectID": "01_Intro.html#the-probabilistic-model",
    "href": "01_Intro.html#the-probabilistic-model",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.1 The Probabilistic Model",
    "text": "1.1 The Probabilistic Model\nMost students who take an intro stats course are familiar with the idea of a random variable. Students are usually introduced to random variables using the notation \\(X\\).\nIn the technical sense, capital \\(X\\) should denote the random variable (that is, the function itself), while lowercase \\(x\\) denotes the values of that random variable. We will be loose on that convention, so you will see the lowercase used almost extensively even when discussing the random variable itself.\n\n\n\n\n\n\nReview: Random Variable\n\n\n\nA random variable is a function that assigns a numeric value to the outcomes in the sample space.\n\n\nIn regression, the random variable of interest is usually denoted as \\(y\\).\nWe want to predict or model (explain) this variable. Thus, we call this the response (or dependent) variable.\nIf we have measurements of this random variable, then we can express each value \\(y\\) as the mean value of \\(y\\) plus some random error.\nThat is, we can model the variable as \\[\n    y = E(y) + \\varepsilon\n\\tag{1.1}\\]\nwhere \\[\\begin{align*}\n    y = &\\text{ dependent variable}\\\\\n    E(y) =& \\text{ mean (or expected) value of } y\\\\\n    \\varepsilon =& \\text{ random error}\n\\end{align*}\\]\nThis model is referred to as a probabilistic model for \\(y\\). The term “probabilistic” is used because, under certain assumptions, we can make probability-based statements about the extent of the difference between \\(y\\) and \\(E(y)\\).\nFor example, we might assert that the error term, \\[\n    \\varepsilon = y - E(y)\n\\] follows a normal distribution.\nIn practice, we will use sample data to estimate the parameters of the probabilistic model—specifically, the mean \\(E(y)\\) and the random error \\(\\varepsilon\\).\nWe will later discuss a common assumption in regression: that the mean error is zero.\nIn other words, \\[\n    E(\\varepsilon) = 0\n\\]\nGiven this assumption, our best estimate of \\(\\varepsilon\\) is zero. Therefore, we only need to estimate \\(E(y)\\).\nThe simplest method of estimating \\(E(y)\\) is to use the sample mean of \\(y\\) which we will denote as \\[\\begin{align*}\n    \\bar y= \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{align*}\\]\nIf we desired to predict a value of \\(y\\), then our best prediction would be just the sample mean: \\[\\begin{align*}\n    \\hat y = \\bar y\n\\end{align*}\\] where \\(\\hat y\\) denotes a predicted value of \\(y\\).\nThis would be the case with univariate data (we only have one variable in our data: \\(y\\)).\nUnfortunately, this simple model does not take into consideration a number of variables, called independent variables, that may help predict the response variable.\nIndependent variables are also called predictor or explanatory variables.\nThe process of identifying the mathematical model that describes the relationship between \\(y\\) and a set of independent variables, and that best fits the data, is known as regression analysis."
  },
  {
    "objectID": "01_Intro.html#sec-regoverview",
    "href": "01_Intro.html#sec-regoverview",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.2 Overview of Regression Analysis",
    "text": "1.2 Overview of Regression Analysis\nWe will denote the independent variables as \\[\\begin{align*}\n    x_1, x_2, \\ldots, x_k\n\\end{align*}\\] where \\(k\\) is the number of independent variables.\nThe goal of regression analysis is to create a prediction equation that accurately relates \\(y\\) to independent variables, allowing us to predict \\(y\\) for given values of \\(x_1, x_2, \\ldots, x_k\\) with minimal prediction error.\nWhen predicting \\(y\\), we also need a measure of the reliability of our prediction, indicating how large the prediction error might be.\nThese elements form the core of regression analysis.\nBeyond predicting \\(y\\), a regression model can also estimate the mean value of \\(y\\) for specific values of \\(x_1, x_2, \\ldots, x_k\\) and explore the relationship between \\(y\\) and one or more independent variables.\nThe process of regression analysis typically involves six key steps:\n\nHypothesize the form of the model for \\(E(y)\\).\nCollect sample data.\nEstimate the model’s unknown parameters using the sample data.\nDefine the probability distribution of the random error term, estimate any unknown parameters, and validate the assumptions made about this distribution.\nStatistically assess the model’s usefulness.\nIf the model is effective, use it for prediction, estimation, and other purposes."
  },
  {
    "objectID": "01_Intro.html#collecting-the-data-for-regression",
    "href": "01_Intro.html#collecting-the-data-for-regression",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.3 Collecting the Data for Regression",
    "text": "1.3 Collecting the Data for Regression\nThe first step listed above will be discussed later.\nOnce you’ve proposed a model for \\(E(y)\\), the next step is to gather sample data to estimate the model.\nThis means collecting data on both the response variable \\(y\\) and the independent variables \\(x_1, x_2, \\ldots, x_k\\) for each observation in your sample. In regression analysis, the sample includes data on multiple variables: \\[\ny, x_1, x_2, \\ldots, x_k\n\\] This is known as multivariate data.\nRegression data can be either observational or experimental:\nFor observational data no control is exerted over the independent variables (\\(x\\)’s). For example, recording people’s ages and their corresponding blood pressure levels without influencing either.\nFor experimental data the independent variables are controlled or manipulated. For instance, setting different fertilizer amounts for crops to observe the impact on growth.\nSuppose you want to model a student’s annual GPA (\\(y\\)). One approach is to randomly select a sample of \\(n=100\\) students and record their GPA along with the values of each predictor variable.\nData for the first three students in the sample are shown in Table 1.1.\n\n\nTable 1.1: Values of the response variable and predictor variables for the first three students.\n\n\n\nStudent 1\nStudent 2\nStudent 3\n\n\n\n\nAnnual GPA \\(y\\)\n3.8\n2.7\n3.5\n\n\nStudy Hours per Week, \\(x_1\\)\n15\n5\n10\n\n\nClass Attendance, \\(x_2\\) (days)\n30\n20\n25\n\n\nExtracurriculars, \\(x_3\\)\n2\n1\n3\n\n\nAge, \\(x_4\\) (years)\n21\n19\n22\n\n\nEmployed, \\(x_5\\) (1 if yes, 0 if no)\n0\n1\n0\n\n\nLives On Campus, \\(x_6\\) (1 if yes, 0 if no)\n1\n0\n1\n\n\n\n\nIn this example, the \\(x\\) values, like study hours, class attendance, and extracurricular activities, are not predetermined before observing GPA \\(y\\); thus, the \\(x\\) values are uncontrolled. Therefore, the sample data are observational.\n\nDetermining Sample Size for Regression with Observational Data\nWhen applying regression to observational data, the required sample size for estimating the mean \\(E(y)\\) depends on three key factors:\n\nEstimated population standard deviation\nConfidence level\nDesired margin of error (half-width of the confidence interval)\n\nHowever, unlike the univariate case, \\(E(y)\\) is modeled as a function of multiple independent variables, which adds complexity. The sample size must be large enough to estimate and test all parameters in the model.\nTo ensure a sufficient sample size, a common guideline is to select a sample size \\(n\\) that is at least 10 times the number of parameters in the model.\nFor instance, if a university registrar’s office uses the following model for the annual GPA \\(y\\) of a current student:\n\\[E(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_6 x_6\\]\nwhere \\(x_1, x_2, \\dots, x_6\\) are defined in Table 1.1, the model includes six \\(\\beta\\) parameters (excluding \\(\\beta_0\\)). Therefore, they should include at least:\n\\[ 10 \\times 6 = 60 \\]\nstudents in the sample.\n\n\nExperimental Data\nThe second type of data in regression, experimental data, is generated through designed experiments where the independent variables are set in advance (i.e., controlled) before observing the value of \\(y\\).\nFor instance, consider a scenario where a researcher wants to study the effect of two independent variables—say, fertilizer amount \\(x_1\\) and irrigation level \\(x_2\\)—on the growth rate \\(y\\) of plants. The researcher could choose three levels of fertilizer (10g, 20g, and 30g) and three levels of irrigation (1L, 2L, and 3L) and measure the growth rate in one plant for each of the \\(3\\times 3=9\\) fertilizer–irrigation combinations (see Table 1.2 below).\n\n\nTable 1.2: Values of the response variable and two independent variables for the growth rate of plants.\n\n\nFertilizer, \\(x_1\\)\nIrrigation, \\(x_2\\)\nGrowth Rate, \\(y\\)\n\n\n\n\n10g\n1L\n5.2\n\n\n10g\n2L\n6.1\n\n\n10g\n3L\n5.8\n\n\n20g\n1L\n7.0\n\n\n20g\n2L\n7.5\n\n\n20g\n3L\n7.3\n\n\n30g\n1L\n8.4\n\n\n30g\n2L\n8.7\n\n\n30g\n3L\n8.1\n\n\n\n\nIn this experiment, the settings of the independent variables are controlled, in contrast to the uncontrolled nature of observational data, like in the real estate sales example.\nIn many studies, it is often not possible to control the values of the \\(x\\)’s, so most data collected for regression are observational.\nSo, why do we differentiate between these two types of data? We will learn that inferences from regression studies based on observational data have more limitations than those based on experimental data. Specifically, establishing a cause-and-effect relationship between variables is much more challenging with observational data than with experimental data."
  },
  {
    "objectID": "02_Fitting.html#the-straight-line-probabilistic-model",
    "href": "02_Fitting.html#the-straight-line-probabilistic-model",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.1 The Straight-Line Probabilistic Model",
    "text": "2.1 The Straight-Line Probabilistic Model\nWhen studying environmental science, consider modeling the monthly carbon dioxide (CO₂) emissions \\(y\\) of a city as a function of its monthly industrial activity \\(x\\).\nThe first question to ask is: Do you believe an exact (deterministic) relationship exists between these two variables?\nIn other words, can we predict the exact value of CO₂ emissions if industrial activity is known?\nIt’s unlikely. CO₂ emissions depend on various factors beyond industrial activity, such as weather conditions, regulatory policies, and transportation levels. Even with multiple variables in the model, it’s improbable that we could predict monthly emissions precisely.\nThere will almost certainly be some variation in emissions due to random phenomena that cannot be fully explained or modeled.\nTherefore, we should propose a probabilistic model for CO₂ emissions that accounts for this random variation:\n\\[\ny = E(y) + \\varepsilon\n\\]\nThe random error component,\\(\\varepsilon\\), captures all unexplained variations in emissions caused by omitted variables or unpredictable random factors.\nThe random error plays a key role in hypothesis testing, determining confidence intervals for the model’s deterministic portion, and estimating the prediction error when using the model to predict future values of \\(y\\).\nLet’s start with the simplest probabilistic model—a first-order linear model that graphs as a straight line."
  },
  {
    "objectID": "02_Fitting.html#a-first-order-linear-model",
    "href": "02_Fitting.html#a-first-order-linear-model",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.2 A First-Order Linear Model",
    "text": "2.2 A First-Order Linear Model\nThe first-order linear model is expessed as \\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nwhere:\n\n\\(y\\) is the dependent variable (also known as the response variable)\n\\(x\\) is the independent variable (used as a predictor of \\(y\\))\n\\(E(y) = \\beta_0 + \\beta_1 x\\) is the deterministic component\n\\(\\varepsilon\\) is the random error component\n\\(\\beta_0\\) is the y-intercept of the line (the point where the line intersects the y-axis)\n\\(\\beta_1\\) is the slope of the line (the change in the mean of \\(y\\) for every 1-unit increase in \\(x\\))\n\nWe use Greek symbols \\(\\beta_0\\) and \\(\\beta_1\\) to denote the y-intercept and slope of the line. These are population parameters with values that would only be known if we had access to the entire population of \\((x, y)\\) measurements.\n\n\n\n\n\n\nReview: Greek Letters in Notation\n\n\n\nUsually, in Statistics, lower-case Greek letters are used to denote population parameters. In our model above, we have an exception. The Greek letter \\(\\varepsilon\\) is not a parameter, but a random variable (parameters are not random variables in frequentist statistics).\n\n\nAs discussed in Section 1.2, regression can be viewed as a six-step process. For now, we’ll focus on steps 2-6, using the simple linear regression model. We’ll explore more complex models later."
  },
  {
    "objectID": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "href": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.3 Fitting the Model: The Method of Least Squares",
    "text": "2.3 Fitting the Model: The Method of Least Squares\nSuppose we have the data shown in Table 2.1 below and plotted in the scatterplot in Figure 2.1.\n\n\nTable 2.1: Data for Scatterplot\n\n\nx\ny\n\n\n\n\n1\n2\n\n\n2\n1.4\n\n\n2.75\n1.6\n\n\n4\n1.25\n\n\n6\n1\n\n\n7\n0.5\n\n\n8\n0.5\n\n\n10\n0.4\n\n\n\n\n\nlibrary(tidyverse)\nx = c(1, 2, 2.75, 4, 6, 7, 8, 10)\ny = c(2, 1.4, 1.6, 1.25, 1, 0.5, 0.5, 0.4)\ndat = tibble(x, y)\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point()\n\n\n\n\nFigure 2.1: Scatterplot of the data in Table 2.1\n\n\n\n\nWe hypothesize that a straight-line model relates y to x, as follows:\n\\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nHow can we use the data from the eight observations in Table 2.1 to estimate the unknown y-intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\))?\nWe can start by trying some lines and see how well they fit the data. But how do we measure how well a line fits the data?\nA quantitative method to evaluate how well a straight line fits a set of data is by measuring the deviations of the data points from the line.\n\n\n\n\n\n\nReview: Deviations of Response Variable\n\n\n\n\\(y\\) is the variable of interest, so we are focused on the differences between observed \\(y\\) and the predicted value of \\(y\\)\n\n\nWe calculate the magnitude of the deviations (the differences between observed and predicted values of \\(y\\)).\nThese deviations, or prediction errors, represent the vertical distances between observed and predicted values of \\(y\\).\nSuppose we try to fit the line \\[\n    \\hat{y} =2-.2x\n\\tag{2.1}\\]\nThis line can be seen in Figure 2.2.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 2, slope = -0.2, color = \"red\")\n\n\n\n\nFigure 2.2: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.1\n\n\n\n\nThe observed and predicted values of \\(y\\), their differences, and their squared differences are shown in the table below.\n\n\nTable 2.2: Deviations and squared deviations of the line in Equation 2.1 and the data in Table 2.1.\n\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\((y - \\hat{y})\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n\n1\n2\n1.8\n0.2\n0.004\n\n\n2\n1.4\n1.6\n-0.2\n0.004\n\n\n2.75\n1.6\n1.45\n0.15\n0.0225\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.8\n0.2\n0.04\n\n\n7\n0.5\n0.6\n-0.1\n0.01\n\n\n8\n0.5\n0.4\n0.1\n0.01\n\n\n10\n0.4\n0\n0.4\n0.16\n\n\n\n\nNote that the sum of the errors (SE) is 0.8, and the sum of squares of the errors (SSE), which emphasizes larger deviations from the line, is 0.325.\nWe can try another line to see if we do better at predicting \\(y\\) (that is, have smaller SSE).\nLet’s try the line \\[\n    \\hat{y} =1.8-.15x\n\\tag{2.2}\\]\nThis line can be seen in Figure 2.3.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 1.8, slope = -0.15, color = \"red\")\n\n\n\n\nFigure 2.3: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.2\n\n\n\n\nThe fit results are shown in Table 2.3.\n\n\nTable 2.3: Deviations and squared deviations of the line in Equation 2.2 and the data in Table 2.1.\n\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\(y - \\hat{y}\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n\n1\n2\n1.65\n0.35\n0.1225\n\n\n2\n1.4\n1.5\n-0.1\n0.01\n\n\n2.75\n1.6\n1.3875\n0.2125\n0.04515625\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.9\n0.1\n0.01\n\n\n7\n0.5\n0.75\n-0.25\n0.0625\n\n\n8\n0.5\n0.6\n-0.1\n0.01\n\n\n10\n0.4\n0.3\n0.1\n0.01\n\n\n\n\nThe SSE for this line is 0.2727, which is lower than the SSE for the previous line, indicating a better fit.\nWhile we could try additional lines to achieve a lower SSE, there are infinitely many possibilities since \\(\\beta_0\\) and \\(\\beta_1\\) can take any real value.\nUsing Calculus, we can attempt to minimize the SSE for the generic line \\[\\begin{align*}\n    \\hat{y} = b_0 +b_1 x\n\\end{align*}\\]\nWe will denote the sum of the squared distances with \\(Q\\): \\[\nQ=\\sum \\left(y_i-\\hat{y}_i\\right)^2\n\\tag{2.3}\\]\nWe determine the “best” line as the one that minimizes \\(Q\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo minimize \\(Q\\), we differentiate it with respect to \\(b_{0}\\) and \\(b_{1}\\): \\[\\begin{align*}\n\\frac{\\partial Q}{\\partial b_{0}} & =-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{1}} & =-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\n\\end{align*}\\]\nSetting these partial derivatives equal to 0, we have \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\\\\\n-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\n\\end{align*}\\] Looking at the first equation, we can simplify as \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum y_{i}-\\sum b_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}-nb_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}=nb_{0}+b_{1}\\sum x_{i}\n\\end{align*}\\]\nSimplifying the second equation gives us \\[\\begin{align*}\n-2\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}-b_0\\sum x_{i}-b_1\\sum x_{i}^{2}=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align*}\\]\n\n\n\nThe two equations \\[\n\\begin{align}\n\\sum y_{i} & =nb_0+b_1\\sum x_{i}\\nonumber\\\\\n\\sum x_{i}y_{i} & =b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align}\n\\tag{2.4}\\]\nare called the normal equations.\nWe now have two equations and two unknowns (\\(b_0\\) and \\(b_1\\)). We can solve the equations simultaneously. We solve the first equation for \\(b_0\\) which gives us \\[\\begin{align*}\nb_0 & =\\frac{1}{n}\\left(\\sum y_{i}-b_1\\sum x_{i}\\right)\\\\\n& =\\bar{y}-b_1\\bar{x}.\n\\end{align*}\\]\nWe now substitute this into the second equation in Equation 2.4. Solving this for \\(b_1\\) gives us \\[\\begin{align*}\n& \\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n& \\quad\\Longrightarrow\\sum x_{i}y_{i}=\\left(\\bar{y}-b_1\\bar{x}\\right)\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n&\\quad\\Longrightarrow b_1=\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}.\n\\end{align*}\\]\nThe equations \\[\n\\begin{align}\nb_0 & =\\bar{y}-b_1\\bar{x}\\\\\nb_1 & =\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{2.5}\\] are called the least squares estimators.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo show these estimators are the minimum, we take the second partial derivatives of \\(Q\\): \\[\\begin{align*}\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{0}\\right)^{2}} & =2n\\\\\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{1}\\right)^{2}} & =2\\sum x_{i}^{2}\n\\end{align*}\\] Since these second partial derivatives are both positives, then we know the least squares estimators are the minimum.\n\n\n\nThe least squares estimators in Equation 2.5 can be expressed in simpler terms if we let \\[\\begin{align*}\nSS_{xx} &= \\sum \\left(x_i-\\bar x\\right)^2 \\\\\nSS_{xy} &= \\sum \\left(x_i-\\bar x\\right)\\left(y_i - \\bar y\\right)\n\\end{align*}\\]\nThe least squares estimates become \\[\\begin{align}\n{b_1=\\frac{SS_{xy}}{SS_{xx}}}\\\\\n{b_0=\\bar{y}-b_1\\bar{x}}\n\\end{align}\\]\nTo recap: The straight line model for the response \\(y\\) in terms of \\(x\\) is \\[\\begin{align*}\n{y = \\beta_0 + \\beta_1 x + \\varepsilon}\n\\end{align*}\\]\nThe line of means is \\[\\begin{align*}\n{E(y) = \\beta_0 + \\beta_1 x }\n\\end{align*}\\]\nThe fitted line (also called the least squares line) is \\[\\begin{align*}\n{\\hat{y} = b_0 + b_1 x }\n\\end{align*}\\]\nFor a given data point, \\((x_i, y_i)\\), the observed value of \\(y\\) is denoted as \\(y_i\\) and the predicted value of \\(y\\) is obtained by substituting \\(x_i\\) into the prediction equation: \\[\\begin{align*}\n{\\hat{y}_i = b_0 + b_1 x_i }\n\\end{align*}\\]\nThe deviation of the \\(i\\)th value of \\(y\\) from its predicted value, called the \\(i\\)th residual, is \\[\\begin{align*}\n{ \\left(y_i-\\hat{y}_i\\right) }\n\\end{align*}\\] Thus, SSE is just the sum of the squared residuals."
  },
  {
    "objectID": "03_Properties.html#properties-of-the-least-squares-estimators",
    "href": "03_Properties.html#properties-of-the-least-squares-estimators",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.1 Properties of the Least Squares Estimators",
    "text": "3.1 Properties of the Least Squares Estimators\n\n3.1.1 Linear Estimators\nNote that the least squares estimators are linear functions of the observations \\(y_{1},\\ldots,y_{n}\\). That is, both \\(b_0\\) and \\(b_1\\) can be written as a linear combination of the \\(y\\)’s.\nSince \\(y\\) is the variable that we want to model, we call an estimator for some parameter that takes the form of a linear combination of \\(y\\) a linear estimator.\n\n\n3.1.2 \\(b_1\\) as a Linear Estimator\nWe can express \\(b_1\\) as \\[\n\\begin{align}\nb_1 & =\\sum k_{i}y_{i}\n\\end{align}\n\\tag{3.1}\\] where \\[\\begin{align*}\nk_{i} & =\\frac{x_{i}-\\bar{x}}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align*}\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe first note that \\(\\sum\\left(x_{i}-\\bar{x}\\right)=0\\).\nWe now rewrite \\(b_1\\) as \\[\\begin{align*}\nb_1 & =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}-\\bar{y}\\sum\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\left(\\frac{1}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}\n\\end{align*}\\]\n\n\n\nFrom Equation 3.1, we see that \\(b_1\\) is a linear combination of the \\(y\\)’s since \\(k_{i}\\) are known constants (recall that \\(x_{i}\\) are treated as known constants).\n\n\n3.1.3 \\(b_0\\) as a Linear Estimator\nWe can write \\(b_0\\) as \\[\n\\begin{align}\nb_0 & =\\sum c_{i}y_{i}\n\\end{align}\n\\tag{3.2}\\] where \\[\\begin{align*}\nc_{i} & =\\frac{1}{n}-\\bar{x}k_{i}\n\\end{align*}\\]\nTherefore, \\(b_0\\) is a linear combination of \\(y_{i}\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe can rewrite \\(b_0\\) as \\[\\begin{align*}\nb_0 & =\\bar{y}-b_{1}\\bar{x}\\\\\n& =\\frac{1}{n}\\sum y_{i}-\\bar{x}\\sum k_{i}y_{i}\\\\\n& =\\sum\\left(\\frac{1}{n}-\\bar{x}k_{i}\\right)y_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "03_Properties.html#model-assumptions",
    "href": "03_Properties.html#model-assumptions",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.2 Model Assumptions",
    "text": "3.2 Model Assumptions\nLet’s now focus on the random component \\(\\varepsilon\\) in the probabilistic model and its connection to the errors in estimating \\(\\beta_0\\) and \\(\\beta_1\\).\nSpecifically, we’ll explore how the probability distribution of \\(\\varepsilon\\) influences the accuracy of the model in representing the true relationship between the dependent variable \\(y\\) and the independent variable \\(x\\).\nWe make four key assumptions about the probability distribution of \\(\\varepsilon\\):\n\nThe mean of \\(\\varepsilon\\)’s probability distribution is 0. This means that, on average, the errors cancel out over an infinitely large number of experiments for each value of the independent variable \\(x\\). Consequently, the mean value of \\(y\\), \\(E(y)\\), for a given \\(x\\) is \\(E(y) = \\beta_0 + \\beta_1 x\\).\nThe variance of \\(\\varepsilon\\)’s probability distribution is constant across all values of the independent variable \\(x\\). For our linear model, this implies that the variance of \\(\\varepsilon\\) is a constant, say, \\(\\sigma^2\\), regardless of the value of \\(x\\).\nThe probability distribution of \\(\\varepsilon\\) is normal.\nThe errors for different observations are independent. In other words, the error for one value of \\(y\\) does not influence the errors for other \\(y\\) values.\n\nThe implications of the first three assumptions can be seen in Figure 3.1;, which shows distributions of errors for four particular values of \\(x\\).\n\n\n\n\n\nFigure 3.1: The probability distribution of \\(\\varepsilon\\). At every value of \\(x\\), there is a distribution of \\(y\\) values that satisfy the four assumptions listed above. The red points are observed values.\n\n\n\n\nFrom Figure 3.1, we see that the probability distributions of the errors are normal, with a mean of 0 and a constant variance \\(\\sigma^2\\).\nThe line in the middle of the curve that goes to the regression line represents the mean value of \\(y\\) for a given value of \\(x\\). The line of means is given by the equation: \\[\nE(y) = \\beta_0 + \\beta_1 x\n\\]\nThese assumptions allow us to create measures of reliability for the least squares estimators and to develop hypothesis tests to evaluate the utility of the least squares line.\nVarious diagnostic techniques are available for checking the validity of these assumptions, and these diagnostics suggest remedies when the assumptions seem invalid.\nTherefore, it is crucial to apply these diagnostic tools in every regression analysis. We will discuss these techniques in detail later.\nIn practice, the assumptions do not need to hold exactly for least squares estimators and test statistics to have the reliability we expect from a regression analysis. The assumptions will be sufficiently satisfied for many real-world applications."
  },
  {
    "objectID": "03_Properties.html#an-estimator-of-sigma2",
    "href": "03_Properties.html#an-estimator-of-sigma2",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.3 An Estimator of \\(\\sigma^2\\)",
    "text": "3.3 An Estimator of \\(\\sigma^2\\)\nThe variability of random error, measured by its variance \\(\\sigma^2\\), plays a crucial role in the accuracy of estimating model parameters \\(\\beta_0\\) and \\(\\beta_1\\), as well as in the precision of predictions when using \\(\\hat{y}\\) to estimate \\(y\\) for a given value of \\(x\\). As a result, it is expected that \\(\\sigma^2\\) will appear in the formulas for confidence intervals and test statistics.\nIn most real-world scenarios, \\(\\sigma^2\\) is unknown and must be estimated using the available data. The best estimate for \\(\\sigma^2\\) is \\(s^2\\), calculated by dividing the sum of squares of residuals by the associated degrees of freedom (df). The sum of squares of residuals is given by: \\[\nSSE = \\sum \\left(y_i - \\hat{y}_i\\right)^2\n\\]\nIn a simple linear regression model, 2 degrees of freedom are used to estimate the y-intercept and slope, leaving \\((n - 2)\\) degrees of freedom for estimating the error variance. Thus, the estimate of \\(\\sigma^2\\) is: \\[\n\\begin{align*}\ns^2 &= \\frac{SSE}{n-2}\\\\\n&= \\frac{\\sum \\left(y_i - \\hat{y}_ i\\right)^2}{n-2}\n\\end{align*}\n\\]\nThis \\(s^2\\) serves as the basis for further statistical analysis, including the construction of confidence intervals and hypothesis testing.\nThe value of \\(s^2\\) is referred to as the mean square error (MSE).\nThe value \\[\\begin{align*}\ns &= \\sqrt{s^2}\n\\end{align*}\\] is referred to as the standard error of the regression model or as the root MSE (RMSE).\nUsing the empirical rule, we expect approximately 95% of the observed \\(y\\) values to lie within \\(2s\\) of their respective least squares predicted values, \\(\\hat y\\).\n\n\n\n\n\n\nReview: The Empirical Rule\n\n\n\nRecall the empirical rule applies to distributions that are mound-shaped and symmetric. It state that approximately 68% of the distribution is within one standard deviation of the mean, approximately 95% of the distribution is within two standard deviations of the mean, and approximatley 99.7% of the distribution is withing three standard deviations of the mean. Since we assume \\(\\varepsilon\\) is normally distributed, then the empirical rule holds.\n\n\n\nExample 3.2 (Example 3.1 - revisited) We can use the summary() function with the fit from lm to obtain the summary stats of the fit.\n\nfit = lm(mpg~wt, data = mtcars)\n\nfit |&gt; summary()\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe standard error (RMSE) of the fit is 3.046.\nWe can obtain the MSE with the following code:\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nwt         1 847.73  847.73  91.375 1.294e-10 ***\nResiduals 30 278.32    9.28                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe MSE is 9.28."
  },
  {
    "objectID": "04_Sampling.html#properties-of-the-constants-k_i-and-c_i",
    "href": "04_Sampling.html#properties-of-the-constants-k_i-and-c_i",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.1 Properties of the constants \\(k_i\\) and \\(c_i\\)",
    "text": "4.1 Properties of the constants \\(k_i\\) and \\(c_i\\)\nIn the previous section, we examined how the least squares estimators are linear combinations of the response variable \\(y\\). Let’s know look at the properties of the coefficients in Equation 3.1 and Equation 3.2. We will not present the proofs in this course but they are not complicated.\nThe coefficients \\(k_{i}\\) have the following properties: \\[\n\\begin{align}\n\\sum k_{i} & =0\n\\end{align}\n\\tag{4.1}\\]\n\\[\n\\begin{align}\n\\sum k_{i}x_i & =1\n\\end{align}\n\\tag{4.2}\\]\n\\[\n\\begin{align}\n\\sum k_{i}^{2} & =\\frac{1}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.3}\\]\nLikewise, the coefficients \\(c_{i}\\) have the following properties: \\[\n\\begin{align}\n\\sum c_{i} & =1\n\\end{align}\n\\tag{4.4}\\]\n\\[\n\\begin{align}\n\\sum c_{i}x_i & =0\n\\end{align}\n\\tag{4.5}\\]\n\\[\n\\begin{align}\n\\sum c_{i}^{2} & =\\frac{1}{n}+\\frac{\\left(\\bar{x}\\right)^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.6}\\]\n\n\n\n\n\n\nReview: The Expected Value of a Linear Combination\n\n\n\nRecall that the expected value of a linear combination of the random variable \\(Y\\) is \\[\nE(aY+b)=aE(Y)+b\n\\] where \\(a\\) and \\(b\\) are constants."
  },
  {
    "objectID": "04_Sampling.html#expected-values-of-b_0-and-b_1",
    "href": "04_Sampling.html#expected-values-of-b_0-and-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.2 Expected Values of \\(b_0\\) and \\(b_1\\)",
    "text": "4.2 Expected Values of \\(b_0\\) and \\(b_1\\)\nBefore finding the expectations, recall \\(E\\left(y_i\\right)=\\beta_{0}+\\beta_{1}x_i\\).\n\n4.2.1 Expected Value of \\(b_1\\)\nThe expected value of \\(b_1\\) is \\[\n\\begin{align*}\nE\\left[b_1\\right] & =E\\left[\\underbrace{\\sum k_{i}y_i}_{(3.1)}\\right]\\\\\n& =\\sum k_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\underbrace{\\sum k_{i}}_{(4.1)}+\\beta_{1}\\underbrace{\\sum k_{i}x_i}_{(4.2)}\\\\\n& =\\beta_{1}\n\\end{align*}\n\\]\n\n\n4.2.2 Expected Value of \\(b_0\\)\nThe expected value of \\(b_0\\) is \\[\n\\begin{align*}\nE\\left[b_0\\right] & =E\\left[\\underbrace{\\sum c_{i}y_i}_{(3.2)}\\right]\\\\\n& =\\sum c_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\underbrace{\\sum c_{i}}_{(4.4)}+\\beta_{1}\\underbrace{\\sum c_{i}x_i}_{(4.5)}\\\\\n& =\\beta_{0}\n\\end{align*}\n\\]\nTherefore, \\(b_0\\) is an unbiased estimator of \\(\\beta_0\\) and \\(b_1\\) is an unbiased estimator of \\(\\beta_1\\).\n\n\n\n\n\n\nReview: Unbiased Estimator\n\n\n\nRecall that an unbiased estimator for some parameter is an estimator that has an expected value equal to that parameter."
  },
  {
    "objectID": "04_Sampling.html#variances-of-b_0-and-b_1",
    "href": "04_Sampling.html#variances-of-b_0-and-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.3 Variances of \\(b_0\\) and \\(b_1\\)",
    "text": "4.3 Variances of \\(b_0\\) and \\(b_1\\)\nTo find the variances, we will use a result from mathematical statistics: Let \\(Y_{1},\\ldots,Y_{n}\\) be uncorrelated random variables and let \\(a_{1},\\ldots,a_{n}\\) be constants. Then \\[\n\\begin{align}\nVar\\left[\\sum a_{i}Y_i\\right] & =\\sum a_{i}^{2}Var\\left[Y_i\\right]\n\\end{align}\n\\tag{4.7}\\]\nRecall that we assume the response variables \\(y_i\\)’s are independent.\nTechnically, we assume the \\(y_i\\)’s are uncorrelated. In general, uncorrelated does not imply independent. However, if the random variables are jointly normally distributed (recall our third assumption of the model), then uncorrelated does imply independent.\nAlso, note that \\[\n\\begin{align*}\n    Var\\left[Y\\right]& = Var\\left[\\beta_0 + \\beta_1 x + \\varepsilon\\right]\\\\\n    & = Var\\left[\\varepsilon\\right]\\\\\n    & = \\sigma^2\n\\end{align*}\n\\]\n\n4.3.1 Variance of \\(b_1\\)\nThe variance of \\(b_1\\) is \\[\n\\begin{align}\nVar\\left[b_1\\right] & =Var\\left[\\underbrace{\\sum k_{i}y_i}_{(3.1)}\\right]\\\\\n& =\\underbrace{\\sum k_{i}^{2}}_{(4.3)}Var\\left[y_i\\right]\\\\\n& =\\frac{\\sigma^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.8}\\]\n\n\n4.3.2 Variance of \\(b_0\\)\nThe variance of \\(b_0\\) is \\[\n\\begin{align}\nVar\\left[b_0\\right] & =Var\\left[\\underbrace{\\sum c_{i}y_i}_{(3.2)}\\right]\\nonumber\\\\\n& =\\underbrace{\\sum c_{i}^{2}}_{(4.6)}Var\\left[y_i\\right]\\\\\n& =\\sigma^{2}\\left[\\frac{1}{n}+\\frac{\\left(\\bar{x}\\right)^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\\right]\n\\end{align}\n\\tag{4.9}\\]"
  },
  {
    "objectID": "04_Sampling.html#best-linear-unbiased-estimators-blues",
    "href": "04_Sampling.html#best-linear-unbiased-estimators-blues",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.4 Best Linear Unbiased Estimators (BLUEs)",
    "text": "4.4 Best Linear Unbiased Estimators (BLUEs)\nWe see from Equation 3.1 and Equation 3.2 that \\(b_0\\) and \\(b_1\\) are linear estimators.\nAny estimator for \\(\\beta_{1}\\), which we will denote as \\(\\hat{\\beta}_{0}\\), that takes the form \\[\n\\begin{align*}\n\\hat{\\beta}_{1} & =\\sum a_{i}y_i\n\\end{align*}\n\\] where \\(a_{i}\\) is some constant, is called a linear estimator.\nOf all linear estimators for \\(\\beta_0\\) and \\(\\beta_1\\) that are unbiased, the least squares estimators, \\(b_0\\) and \\(b_1\\), have the smallest variance.\nThis is summarized in the following well known theorem:\n\nTheorem 4.1 (Gauss Markov Theorem) For the simple linear regression model, the least squares estimators \\(b_0\\) and \\(b_1\\) are unbiased and have minimum variance among all unbiased linear estimators.\n\nAn estimator that is linear, unbiased, and has the smallest variance of all unbiased linear estimators is called the best linear unbiased estimator (BLUE).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nProof of the Gauss Markov Theorem:\nFor all linear estimators that are unbiased, we must have \\[\n\\begin{align*}\nE\\left[\\hat{\\beta}_{1}\\right] & =E\\left[\\sum a_{i}y_i\\right]\\\\\n& =\\sum a_{i}E\\left[y_i\\right]\\\\\n& =\\beta_{1}\n\\end{align*}\n\\] Since \\(E\\left[y_i\\right]=\\beta_{0}+\\beta_{1}x_i\\), then we must have \\[\n\\begin{align*}\nE\\left[\\hat{\\beta}_{1}\\right] & =\\sum a_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\sum a_{i}+\\beta_{1}\\sum a_{i}x_i\\\\\n& =\\beta_{1}\n\\end{align*}\n\\] Therefore, \\[\n\\begin{align*}\n\\sum a_{i} & =0\\\\\n\\sum a_{i}x_i & =1\n\\end{align*}\n\\] We now examine the variance of \\(\\hat{\\beta}_{1}\\): \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =\\sum a_{i}^{2}Var\\left[y_i\\right]\\\\\n& =\\sigma^{2}\\sum a_{i}^{2}\n\\end{align*}\n\\] Let’s now define \\(a_{i}=k_{i}+d_{i}\\) where \\(k_{i}\\) is defined in Equation 3.1. and \\(d_{i}\\) is some arbitrary constant.\nWe will show that adding a constant (whether negative or positive) to \\(k_i\\) cannot make the variance smaller. Thus, the smallest variance of the linear estimator \\(\\hat{\\beta}_1\\) is when \\(a_i=k_i\\).\nThe variance of \\(\\hat{\\beta}_{1}\\) can now be written as \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =\\sigma^{2}\\sum a_{i}^{2}\\\\\n& =\\sigma^{2}\\sum\\left(k_{i}+d_{i}\\right)^{2}\\\\\n& =\\sigma^{2}\\sum\\left(k_{i}^{2}+2k_{i}d_{i}+d_{i}^{2}\\right)\\\\\n& =Var\\left[b_1\\right]+2\\sigma^{2}\\sum k_{i}d_{i}+\\sigma^{2}\\sum d_{i}^{2}\n\\end{align*}\n\\] Examining the second term and using the expression of \\(k_{i}\\) in Equation 3.1, we see that \\[\n\\begin{align*}\n\\sum k_{i}d_{i} & =\\sum k_{i}\\left(a_{i}-k_{i}\\right)\\\\\n& =\\sum a_{i}k_{i}-\\underbrace{\\sum k_{i}^{2}}_{(4.3)}\\\\\n& =\\sum a_{i}\\frac{x_i-\\bar{x}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum a_{i}x_i-\\bar{x}\\sum a_{i}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{1-\\bar{x}\\left(0\\right)}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =0\n\\end{align*}\n\\]\nWe now have the variance of \\(\\hat{\\beta}_{1}\\) as \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =Var\\left[b_1\\right]+\\sigma^{2}\\sum d_{i}^{2}\n\\end{align*}\n\\] This variance is minimized when \\(\\sum d_{i}^{2}=0\\) which only happens when \\(d_{i}=0\\).\nThus, the unbiased linear estimator with the smallest variance is when \\(a_{i}=k_{i}\\). That is, the least squares estimator \\(b_1\\) in Equation 3.1 has the smallest variance of all unbiased linear estimators of \\(\\beta_{1}\\).\nA similar argument can be used to show that \\(b_0\\) has the smallest variance of all unbiased linear estimators of \\(\\beta_{0}\\)."
  },
  {
    "objectID": "04_Sampling.html#sampling-distribution-for-b_1",
    "href": "04_Sampling.html#sampling-distribution-for-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.5 Sampling Distribution for \\(b_1\\)",
    "text": "4.5 Sampling Distribution for \\(b_1\\)\nNow that we see that the least squares estimator \\(b_1\\) is the BLUE for \\(\\beta_{1}\\), we will now examine the sampling distribution for \\(b_1\\).\nWe previously discussed that the mean of the sampling distribution of \\(b_1\\) is \\[\nE[b_1]=\\beta_1\n\\] with a variance of \\[\n\\begin{align}\nVar\\left[b_1\\right]\n& =\\frac{\\sigma^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.10}\\]\nNote that in our model with our four assumptions, \\(y\\) is normally distributed. That is, \\[\n\\begin{align}\n    y\\sim N\\left(\\beta_0+\\beta_1 x, \\sigma^2\\right)\n\\end{align}\n\\tag{4.11}\\]\nTo learn about the sampling distributions of the least squares estimators, we will use the following theorems from mathematical statistics:\n\nTheorem 4.2 (Sum of Independent Normal Random Variables) If \\[\nY_i\\sim N\\left(\\mu_i,\\sigma_i^2\\right)\n\\] are independent, then the linear combination \\(\\sum_i a_iY_i\\) is also normally distributed where \\(a_i\\) are constants. In particular \\[\n\\sum_i a_iY_i \\sim N\\left(\\sum_i a_i\\mu_i, \\sum_i a_i^2\\sigma_i^2\\right)\n\\]\n\n\nTheorem 4.3 (Adding a Constant to a Normal Random Variable) If \\[\nY\\sim N\\left(\\mu,\\sigma^2\\right)\n\\] then for any real constant \\(c\\), \\[\nY+c\\sim N\\left(\\mu+c,\\sigma^2\\right)\n\\]\n\nSince \\(Y\\) is normally distributed by Equation 4.11, then we can apply Theorem 4.2 which implies that \\(b_1\\) is normally distributed. That is, \\[\n\\begin{align}\nb_1 & \\sim N\\left(\\beta_{1},\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\right)\n\\end{align}\n\\tag{4.12}\\]\n\n4.5.1 Standardized Score\nSince \\(b_1\\) is normally distributed, we can standardize it so that the resulting statistic will have a standard normal distribution.\nTherefore, we have \\[\n\\begin{align}\nz=\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}}} & \\sim N\\left(0,1\\right)\n\\end{align}\n\\tag{4.13}\\]\n\n\n4.5.2 Studentized Score\nIn practice, the standardized score \\(z\\) is not useful since we do not know the value of \\(\\sigma^{2}\\). We can estimate \\(\\sigma^{2}\\) with the statistic \\[\ns^2 = \\frac{SSE}{n-2}\n\\]\nUsing this estimate for \\(\\sigma^2\\) leads us to a \\(t\\)-score: \\[\n\\begin{align}\nt=\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}}} & \\sim t\\left(n-2\\right)\n\\end{align}\n\\tag{4.14}\\]\nWe call this \\(t\\) statistic, the studentized score.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nIt is important to note the following theorem from math stats presented here without proof:\n\nTheorem 4.4 (Distribution of the sample variance of the residuals) For the sample variance of the residuals \\(s^{2}\\), the quantity \\[\\begin{align*}\n\\frac{\\left(n-2\\right)s^{2}}{\\sigma^{2}} & =\\frac{SSE}{\\sigma^{2}}\n\\end{align*}\\] is distributed as a chi-square distribution with \\(n-2\\) degrees of freedom. That is, \\[\\begin{align*}\n\\frac{SSE}{\\sigma^{2}} & \\sim\\chi^{2}\\left(n-2\\right)\n\\end{align*}\\]\n\nWe will use another important theorem form math stats (again presented without proof):\n\nTheorem 4.5 (Ratio of independent standard normal and chi-square statistics) If \\(Z\\sim N\\left(0,1\\right)\\) and \\(W\\sim\\chi^{2}\\left(\\nu\\right)\\), and \\(Z\\) and \\(W\\) are independent, then the statistic \\[\\begin{align*}\n\\frac{Z}{\\sqrt{\\frac{W}{\\nu}}}\n\\end{align*}\\] is distributed as a Student’s \\(t\\) distribution with \\(\\nu\\) degrees of freedom.\n\nWe will take the standardized score in Equation 4.13 and divide by \\[\\begin{align*}\n\\sqrt{\\frac{\\frac{\\left(n-2\\right)s^{2}}{\\sigma^{2}}}{\\left(n-2\\right)}} & =\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}\n\\end{align*}\\] to give us \\[\\begin{align*}\nt & =\\frac{\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}}{\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}}\\\\\n& =\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}}\\\\\n& =\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}\n\\end{align*}\\] which will have a Student’s \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "04_Sampling.html#sec-modelutility",
    "href": "04_Sampling.html#sec-modelutility",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.6 Assessing the Utility of the Model: Making Inferences About the Slope",
    "text": "4.6 Assessing the Utility of the Model: Making Inferences About the Slope\nSuppose that the independent variable \\(x\\) is completely unrelated to the dependent variable \\(y\\).\nWhat could be said about the values of \\(\\beta_0\\) and \\(\\beta_1\\) in the hypothesized probabilistic model \\[\\begin{align*}\n    y = \\beta_0 +\\beta_1 x + \\varepsilon\n\\end{align*}\\] if \\(x\\) contributes no information for the prediction of \\(y\\)?\nThe implication is that the mean of \\(y\\), does not change as \\(x\\) changes. In other words, the line would just be a horizontal line.\nIf \\(E(y)\\) does not change as \\(x\\) increases, then using \\(x\\) to predict \\(y\\) in the linear model is not useful.\nRegardless of the value of \\(x\\), you always predict the same value of \\(y\\). In the straight-line model, this means that the true slope, \\(\\beta_1\\), is equal to 0.\nTherefore, to test the null hypothesis that \\(x\\) contributes no information for the prediction of \\(y\\) against the alternative hypothesis that these variables are linearly related with a slope differing from 0, we test \\[\\begin{align*}\n    H_0:\\beta_1 = 0\\\\\n    H_a:\\beta_1\\ne 0\n\\end{align*}\\]\nIf the data support the alternative hypothesis, we conclude that \\(x\\) does contribute information for the prediction of \\(y\\) using the straight-line model (although the true relationship between \\(E(y)\\) and \\(x\\) could be more complex than a straight line). Thus, to some extent, this is a test of the utility of the hypothesized model.\nThe appropriate test statistic is the studentized score given above: \\[\n\\begin{align}\n    t &= \\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}\\\\\n    &=\\frac{b_1}{\\sqrt{\\frac{s^{2}}{SS_{xx}}}}\n\\end{align}\n\\tag{4.15}\\]\nAnother way to make inferences about the slope \\(\\beta_1\\) is to estimate it using a confidence interval \\[\n\\begin{align}\n    b_1 \\pm \\left(t_{\\alpha/2}\\right)s_{b_1}\n\\end{align}\n\\tag{4.16}\\] where \\[\\begin{align*}\n    s_{b_1} = \\frac{s}{\\sqrt{SS_{xx}}}\n\\end{align*}\\]\nWe can obtain the p-value for the hypothesis test by using the summary function with an lm object. For the previous example consisting of the mtcars data.\n\nExample 4.1 (Example 3.1 - revisited)  \n\nlibrary(tidyverse)\n\nfit = lm(mpg~wt, data = mtcars)\n\nWe find the least squares estimates as\n\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nFrom the output, we see the p-value is \\(1.29\\times 10^{-10}\\). So we have sufficient evidence to conclude that the true population slope is different than zero.\nTo find the confidence interval, we can use the confint function with the lm object.\n\nconfint(fit, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept) 33.450500 41.119753\nwt          -6.486308 -4.202635\n\n\nWe 95% confident that the true population slope is in the interval \\((-6.486, -4.203)\\)"
  },
  {
    "objectID": "05_Correlation.html#the-coefficient-of-correlation",
    "href": "05_Correlation.html#the-coefficient-of-correlation",
    "title": "5  Correlation Coefficient and the Coefficient of Determination",
    "section": "5.1 The Coefficient of Correlation",
    "text": "5.1 The Coefficient of Correlation\nThe claim is often made that the crime rate and the unemployment rate are “highly correlated.”\nAnother popular belief is that IQ and academic performance are “correlated.” Some people even believe that the Dow Jones Industrial Average and the lengths of fashionable skirts are “correlated.”\nThus, the term correlation implies a relationship or association between two variables.\nFor the data \\((x_i,y_i)\\), \\(i=1,\\ldots,n\\), we want a measure of how well a linear model explains a linear relationship between \\(x\\) and \\(y\\).\nRecall the quantities \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\).\n\n\n\n\n\n\nReview: Different \\(SS\\) quantities\n\n\n\nRecall that \\[\n\\begin{align*}\nSS_{xx} &= \\sum\\left(x_i-\\bar{x}\\right)^2\\\\\nSS_{yy} &= \\sum\\left(y_i-\\bar{y}\\right)^2\\\\\nSS_{xy} &= \\sum\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)\n\\end{align*}\n\\]\n\n\n\\(SS_{xx}\\) and \\(SS_{yy}\\) are measures of variability of \\(x\\) and \\(y\\), respectively. That is, they indicate how \\(x\\) and \\(y\\) varies about their mean, individually.\n\\(SS_{xy}\\) is a measure of how \\(x\\) and \\(y\\) vary together.\n\nExample 5.1 (Data from Table 2.1) For example, consider the data from Table 2.1. Let’s find \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\) in R.\n\nlibrary(tidyverse)\n\nx = c(1, 2 ,2.75, 4, 6, 7, 8, 10)\ny = c(2, 1.4, 1.6, 1.25, 1, 0.5, 0.5, 0.4)\n\ndat =  tibble(x, y)\n\nybar =  mean(y)\nxbar =  mean(x)\n\nggplot(data=dat, aes(x = x, y = y)) +\n  geom_point() +\n  xlim(0,10) +\n  ylim(0,2) +\n  geom_hline(yintercept = ybar,col=\"red\") +\n  geom_vline(xintercept = xbar, col=\"red\")\n\n\n\n\n\ndev_x = x-xbar\ndev_y = y-ybar\n\ndev_xy = dev_x*dev_y\n\ndat1 = tibble(x,y,dev_x^2,dev_y^2,dev_xy)\ndat1\n\n# A tibble: 8 × 5\n      x     y `dev_x^2` `dev_y^2`  dev_xy\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1  1     2       16.8     0.844   -3.76  \n2  2     1.4      9.57    0.102   -0.986 \n3  2.75  1.6      5.49    0.269   -1.22  \n4  4     1.25     1.20    0.0285  -0.185 \n5  6     1        0.821   0.00660 -0.0736\n6  7     0.5      3.63    0.338   -1.11  \n7  8     0.5      8.45    0.338   -1.69  \n8 10     0.4     24.1     0.464   -3.34  \n\n\n\nIn the output of dat1, dev_x^2 represents \\((x_i-\\bar{x})^2\\) and dev_y^2 represents \\((y_i-\\bar{y})^2\\) for each observation. dev_xy represents \\((x_i-\\bar{x})(y_i-\\bar{y})\\) for each observation. Note that each value is negative. This is because as \\(x\\) is below \\(\\bar{x}\\), \\(y\\) is above \\(\\bar{y}\\).\nLikewise, as \\(X\\) is above \\(\\bar{x}\\), \\(Y\\) is below \\(\\bar{y}\\). In the ggplot above, the two red lines represent \\(\\bar{x}\\) (the vertical red line) and \\(\\bar{y}\\) (the horizontal red line). You can see how the observations are below or above these lines.\nWe can find the values of \\(SS_{xx}\\), \\(SS_{yy}\\), and \\(SS_{xy}\\) by\n\n#SS_XX\ndev_x^2 |&gt; sum()\n\n[1] 69.99219\n\n#SS_YY\ndev_y^2 |&gt; sum()\n\n[1] 2.389687\n\n#SS_XY\ndev_xy |&gt; sum()\n\n[1] -12.36094\n\n\n\nExample 5.2 (The trees dataset) For another example, consider the trees dataset.\nIn R, a packaged called datasets include a number of available datasets. One of the datasets is called trees.\n\nlibrary(datasets)\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\nThere are 31 total observations in this dataset. Variables measured are the Girth (actually the diameter measured at 54 in. off the ground), the Height, and the Volume of timber from each black cherry tree.\nSuppose we want to predict Volume from Girth.\nAgain, we plot the data with red lines representing \\(\\bar{x}\\) and \\(\\bar{y}\\).\n\nlibrary(datasets)\nlibrary(tidyverse)\n\nxbar = mean(trees$Girth)\nybar = mean(trees$Volume)\n\nggplot(data=trees, aes(x=Girth, y=Volume)) +\n  geom_point() +\n  geom_hline(yintercept = ybar,col=\"red\") +\n  geom_vline(xintercept = xbar, col=\"red\")\n\n\n\nx = trees$Girth\ny = trees$Volume\n\ndev_x = x-xbar\ndev_y = y-ybar\n\ndev_xy = dev_x*dev_y\n\n\ndat1 = tibble(x,y,dev_x^2,dev_y^2,dev_xy)\ndat1 |&gt; print(n=31)\n\n# A tibble: 31 × 5\n       x     y `dev_x^2` `dev_y^2`  dev_xy\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1   8.3  10.3  24.5        395.    98.3  \n 2   8.6  10.3  21.6        395.    92.4  \n 3   8.8  10.2  19.8        399.    88.8  \n 4  10.5  16.4   7.55       190.    37.8  \n 5  10.7  18.8   6.49       129.    29.0  \n 6  10.8  19.7   5.99       110.    25.6  \n 7  11    15.6   5.06       212.    32.8  \n 8  11    18.2   5.06       143.    26.9  \n 9  11.1  22.6   4.62        57.3   16.3  \n10  11.2  19.9   4.20       105.    21.0  \n11  11.3  24.2   3.80        35.7   11.6  \n12  11.4  21     3.42        84.1   17.0  \n13  11.4  21.4   3.42        76.9   16.2  \n14  11.7  21.3   2.40        78.7   13.7  \n15  12    19.1   1.56       123.    13.8  \n16  12.9  22.2   0.121       63.5    2.78 \n17  12.9  33.8   0.121       13.2   -1.26 \n18  13.3  27.4   0.00266      7.68  -0.143\n19  13.7  25.7   0.204       20.0   -2.02 \n20  13.8  24.9   0.304       27.8   -2.91 \n21  14    34.5   0.565       18.7    3.25 \n22  14.2  31.7   0.906        2.34   1.46 \n23  14.5  36.3   1.57        37.6    7.67 \n24  16    38.3   7.57        66.1   22.4  \n25  16.3  42.6   9.31       154.    37.9  \n26  17.3  55.4  16.4        637.   102.   \n27  17.5  55.7  18.1        652.   109.   \n28  17.9  58.3  21.6        791.   131.   \n29  18    51.5  22.6        455.   101.   \n30  18    51    22.6        434.    99.0  \n31  20.6  77    54.0       2193.   344.   \n\n#SS_XX\ndev_x^2 |&gt; sum()\n\n[1] 295.4374\n\n#SS_YY\ndev_y^2 |&gt; sum()\n\n[1] 8106.084\n\n#SS_XY\ndev_xy |&gt; sum()\n\n[1] 1496.644\n\n\nIn this example, most of the observations have \\((x-\\bar{x})(y-\\bar{y})\\) that are positive. This is because these observations have values of \\(x\\) that are below \\(\\bar{x}\\) and values of \\(y\\) that are below \\(\\bar{y}\\), or values of \\(x\\) that are above \\(\\bar{x}\\) and values of \\(y\\) that are above \\(\\bar{y}\\).\nThere are four observations that have a negative value of \\((x-\\bar{x})(y-\\bar{y})\\). Although they are negative, the value of \\(SS_{xy}\\) is positive due to all the observations with positive values of \\((x-\\bar{x})(y-\\bar{y})\\). Therefore, we say if \\(SS_{xy}\\) is positive, then \\(y\\) tends to increase as \\(x\\) increases. Likewise, if \\(SS_{xy}\\) is negative, then \\(y\\) tends to decrease as \\(x\\) increases.\nIf \\(SS_{xy}\\) is zero (or close to zero), then we say \\(y\\) does not tend to change as \\(x\\) increases.\n\n\n5.1.1 Defining the Correlation Coefficient\nWe first note that \\(SS_{xy}\\) cannot be greater in absolute value than the quantity \\[\n\\sqrt{SS_{xx}SS_{yy}}\n\\] We will not prove this here, but it is a direct application of the Cauchy-Schwarz inequality .\nWe define the linear correlation coefficient as \\[\n\\begin{align}\n    r=\\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}\n\\end{align}\n\\tag{5.1}\\]\n\\(r\\) is also called the Pearson correlation coefficient.\nWe note that \\[\n-1\\le r \\le 1\n\\]\nIf \\(r=0\\), then there is no linear relationship between \\(x\\) and \\(y\\).\nIf \\(r\\) is positive, then the slope of the linear relationship is positive. If \\(r\\) is negative, then the slope of the linear relationship is negative.\nThe closer \\(r\\) is to one in absolute value, the stronger the linear relationship is between \\(x\\) and \\(y\\).\n\n\n5.1.2 Some Examples of \\(r\\)\nThe best way to grasp correlation is to see examples. In Figure 5.1, scatterplots of 200 observations are shown with a least squares line.\n\n\n\n\n\n\n\n(a) \\(r=-0.079\\)\n\n\n\n\n\n\n\n(b) \\(r=-0.672\\)\n\n\n\n\n\n\n\n\n\n(c) \\(r=0.723\\)\n\n\n\n\n\n\n\n(d) \\(r=0.524\\)\n\n\n\n\nFigure 5.1: Examples of correlation\n\n\nNote how the value of \\(r\\) relates to how spread out the points are from the line as well as to the slope of the line.\nThe correlation coefficient, \\(r\\), quantifies the strength of the linear relationship between two variables, \\(x\\) and \\(y\\), similar to the way the least squares slope, \\(b_1\\), does. However, unlike the slope, the correlation coefficient is scaleless. This means that the value of \\(r\\) always falls between \\(\\pm 1\\), regardless of the units used for \\(x\\) and \\(y\\).\nThe calculation of \\(r\\) uses the same data that is used to fit the least squares line. Given that both \\(r\\) and \\(b_1\\) offer insight into the utility of the model, it’s not surprising that their computational formulas are related.\nIt’s also important to remember that a high correlation does not imply causality. If a high positive or negative value of \\(r\\) is observed, this does not mean that changes in \\(x\\) cause changes in \\(y\\). The only valid conclusion is that there may be a linear relationship between \\(x\\) and \\(y\\).\n\n\n5.1.3 The Population Correlation Coefficient\nThe correlation \\(r\\) is for the observed data which is usually from a sample. Thus, \\(r\\) is the sample correlation coefficient.\nWe could make a hypothesis about the correlation of the population based on the sample. We will denote the population correlation with \\(\\rho\\). The hypothesis we will want to test is \\[\\begin{align*}\n  H_0:\\rho = 0\\\\\nH_a:\\rho \\ne 0\n\\end{align*}\\]\nRecall the hypothesis test for the slope in Section 4.6.\nIf we test \\[\\begin{align*}\nH_{0}: & \\beta_{1}=0\\\\\nH_{a}: & \\beta_{1}\\ne0\n\\end{align*}\\] then this is equivalent to testing1 \\[\\begin{align*}\nH_{0}: & \\rho=0\\\\\nH_{a}: & \\rho\\ne0\n\\end{align*}\\] since both hypotheses test to see of there is a linear relationship between \\(x\\) and \\(y\\).\nNow note, using Equation 2.5, that \\(b_1\\) can be rewritten as \\[\n\\begin{align}\nb_1 & =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{SS_{xy}}{SS_{xx}}\\\\\n& =\\frac{rSS_{xy}}{rSS_{xx}}\\\\\n& =\\frac{rSS_{xy}}{\\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}SS_{xx}}\\\\\n& =\\frac{r\\sqrt{SS_{xx}SS_{yy}}}{SS_{xx}}\\\\\n& =r\\frac{\\sqrt{\\frac{SS_{xx}}{n-1}\\frac{SS_{yy}}{n-1}}}{\\frac{SS_{xx}}{n-1}}\\\\\n& =r\\frac{s_{X}s_{Y}}{s_{X}^{2}}\\\\\n& =r\\frac{s_{y}}{s_{X}}\n\\end{align}\n\\tag{5.2}\\] where \\(s_{y}\\) and \\(s_{x}\\) are the sample standard deviation of \\(y\\) and \\(x\\), respectively.\nThe test statistic is \\[\n\\begin{align}\nt & =\\frac{r\\sqrt{\\left(n-2\\right)}}{\\sqrt{1-r^{2}}}\n\\end{align}\n\\tag{5.3}\\]\nIf \\(H_0\\) is true, then \\(t\\) will have a Student’s \\(t\\)-distribution with \\(n-2\\) degrees of freedom.\nThe only real difference between the least squares slope \\(b_1\\) and the coefficient of correlation \\(r\\) is the measurement scale2.\nTherefore, the information they provide about the utility of the least squares model is to some extent redundant.\nFurthermore, the slope \\(b_1\\) gives us additional information on the amount of increase (or decrease) in \\(y\\) for every 1-unit increase in \\(x\\).\nFor this reason, the slope is recommended for making inferences about the existence of a positive or negative linear relationship between two variables."
  },
  {
    "objectID": "05_Correlation.html#the-coefficient-of-determination",
    "href": "05_Correlation.html#the-coefficient-of-determination",
    "title": "5  Correlation Coefficient and the Coefficient of Determination",
    "section": "5.2 The Coefficient of Determination",
    "text": "5.2 The Coefficient of Determination\nThe second measure of how well the model fits the data involves measuring the amount of variability in \\(y\\) that is explained by the model using \\(x\\).\nWe start by examining the variability of the variable we want to learn about. We want to learn about the response variable \\(y\\). One way to measure the variability of \\(y\\) is with \\[\nSS_{yy} = \\sum\\left(y_i-\\bar{y}\\right)^2\n\\]\nNote that \\(SS_{yy}\\) does not include the model or \\(x\\). It is just a measure of how \\(y\\) deviates from its mean \\(\\bar{y}\\).\nWe also have the variability of the points about the line. We can measure this with the sum of squares error \\[\nSSE = \\sum \\left(y_i - \\hat{y}_i\\right)^2\n\\]\nNote that SSE does include \\(x\\). This is because the fitted line \\(\\hat{y}\\) is a function of \\(x\\).\nHere are a couple of key points regarding sums of squares:\n\nIf \\(x\\) provides little to no useful information for predicting \\(y\\), then \\(SS_{yy}\\) and \\(SSE\\) will be nearly equal.\nIf \\(x\\) does provide valuable information for predicting \\(y\\), then \\(SSE\\) will be smaller than \\(SS_{yy}\\).\nIn the extreme case where all points lie exactly on the least squares line, \\(SSE = 0\\).\n\nHere’s an example to illustrate:\nSuppose we have data for two variables, hours studied (x) and test scores (y). If studying time doesn’t help predict the test score, the variation in test scores (measured by \\(SS_{yy}\\)) will be similar to the error in the prediction (measured by \\(SSE\\)). However, if studying time is a good predictor, the prediction errors will be much smaller, making \\(SSE\\) significantly smaller than \\(SS_{yy}\\). If the relationship between study time and test scores is perfect, then the error would be zero, resulting in \\(SSE = 0\\).\n\n5.2.1 Proportion of Variation Explained\nWe want to explain as much of the variation of \\(y\\) as possible. So we want to know just how much of that variation is explained by using linear regression model with \\(x\\). We can quantify this variation explained by taking the difference \\[\n\\begin{align}\n    SSR = SS_{yy}-SSE\n\\end{align}\n\\tag{5.4}\\]\nSSR is called the sum of squares regression.\nWe calculate the proportion of the variation of \\(y\\) explained by the regression model using \\(x\\) by calculating3 \\[\n\\begin{align}\n    r^2 = \\frac{SSR}{SS_{yy}}\n\\end{align}\n\\tag{5.5}\\]\n\\(r^2\\) is called the coefficient of determination4\nPractical Interpretation:\nAbout \\(100(r^2)\\%\\) of the sample variation in \\(y\\) (measured by the total sum of squares of deviations of the sample \\(y\\)-values about their mean \\(\\bar{y}\\)) can be explained by (or attributed to) using \\(x\\) to predict \\(y\\) in the straight-line model.\n\nExample 5.3 (Example 5.2 revisited) We can find the coefficient of determination using the summary function with an lm object.\n\nlibrary(datasets)\n\nfit = lm(Volume~Girth, data = trees)\n\nfit |&gt; summary()\n\n\nCall:\nlm(formula = Volume ~ Girth, data = trees)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that 93.53% of the variability in the volume of the trees can be explained by the linear model using girth to predict the volume.\nIf we want to find the correlation coefficient, we can just use the cor function on the dataframe. This will find the correlation coefficient for each pair of variables in the dataframe. Note that there can only be quantitative variables in the dataframe in order this function to work.\n\ntrees |&gt; cor()\n\n           Girth    Height    Volume\nGirth  1.0000000 0.5192801 0.9671194\nHeight 0.5192801 1.0000000 0.5982497\nVolume 0.9671194 0.5982497 1.0000000\n\n\nSo the correlation between Girth and Volume is 0.9671."
  },
  {
    "objectID": "05_Correlation.html#footnotes",
    "href": "05_Correlation.html#footnotes",
    "title": "5  Correlation Coefficient and the Coefficient of Determination",
    "section": "",
    "text": "Note: The two tests are equivalent in simple linear regression only.↩︎\nThe estimated slope is measured in the same units as \\(y\\). However, the correlation coefficient \\(r\\) is independent of scale.↩︎\nIn simple linear regression, it can be shown that this quantity is equal to the square of the simple linear coefficient of correlation \\(r\\).↩︎\nNote that some software will denote the coefficient of determination as \\(R^2\\).↩︎"
  },
  {
    "objectID": "06_Using.html#using-the-model-for-estimation-and-prediction",
    "href": "06_Using.html#using-the-model-for-estimation-and-prediction",
    "title": "6  Using the Simple Linear Model",
    "section": "6.1 Using the Model for Estimation and Prediction",
    "text": "6.1 Using the Model for Estimation and Prediction\nNow that we have fit the model\n\\[\ny = {\\beta}_0 + {\\beta}_1 x + \\varepsilon\n\\] and assessed how good of fit the model is, we can now use the model estimation and prediction.\nRecall that the mean of \\(y_{i}\\) for some value of \\(x_{i}\\) is the population line \\(\\beta_{0}+\\beta_{1}x_{i}\\) evaluated at \\(x_{i}\\).\nSo we can estimate the mean of \\(y_{i}\\) for some value of \\(x_{i}\\) by evaluating the model estimated with the least squares estimators: \\[\\begin{align*}\n\\hat{y}_{i} & =b_0+b_1x_{i}\n\\end{align*}\\]\nWe say \\(\\hat{y}_{i}\\) is a point estimator for the population mean \\(\\beta_{0}+\\beta_{1}x_{i}\\).\n\n6.1.1 The Sampling Distribution of \\(\\hat{y}\\)\nWe will want to make an inference for the population mean response at some value of the predictor variable \\(x_{i}\\).\nWe have a point estimator \\(\\hat{y}_{i}\\). We will now examine the sampling distribution of \\(\\hat{y}_{i}\\) and use it to make a confidence interval for the mean response \\(\\beta_{0}+\\beta_{1}x_{i}\\).\n\n\n6.1.2 Linear Combination of \\(y\\)\nWe will denote the value of \\(x\\) at which we want to estimate the mean response as \\(x_{h}\\). So the value of \\(y\\) at \\(x_{h}\\) will be \\(y_{h}\\)\nWe write \\(\\hat{y}_{h}\\) as \\[\\begin{align*}\n\\hat{y}_{h} & =\\underbrace{b_0}_{(3.2)}+\\underbrace{b_1}_{(3.1)}x_{h}\\\\\n& =\\sum c_{i}y_{i}+\\sum k_{i}y_{h}x_{h}\\\\\n& =\\sum\\left(c_{i}+k_{i}x_{h}\\right)y_{h}\n\\end{align*}\\]\nThus, \\(\\hat{y}_{j}\\) is a linear combination of the observed \\(y_{i}\\) which are normally distributed. Then by Theorem 4.2, \\(\\hat{y}_{j}\\) is normally distributed.\n\n\n6.1.3 The Mean of \\(\\hat{y}_h\\)\nUsing Theorem 4.2, we have the mean as \\[\\begin{align*}\n\\sum\\left(c_{i}+k_{i}x_{h}\\right)E\\left[y_{h}\\right] & =\\left(\\underbrace{\\sum c_{i}}_{=1}+x_{h}\\underbrace{\\sum k_{i}}_{=0}\\right)\\left(\\beta_{0}+\\beta_{1}x_{h}\\right)\\\\\n& =\\beta_{0}+\\beta_{1}x_{h}\n\\end{align*}\\]\n\n\n6.1.4 The Variance of \\(\\hat{y}_h\\)\nUsing Theorem 4.3, we have the variance as \\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\n\\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sum\\left(c_{i}+k_{i}x_{h}\\right)^{2}{Var\\left[y_{h}\\right]}\\\\\n& =\\sum\\left(\\frac{1}{n}-\\bar{x}\\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}x_{h}\\right)^{2}\\sigma^{2}\\\\\n& =\\sigma^{2}\\sum\\left(\\frac{1}{n}+\\frac{\\left(x_{i}-\\bar{x}\\right)\\left(x_{h}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)^{2}\\\\\n& =\\sigma^{2}\\sum\\left(\\frac{1}{n^{2}}+2\\left(\\frac{1}{n}\\right)\\frac{\\left(x_{i}-\\bar{x}\\right)\\left(x_{h}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{i}-\\bar{x}\\right)^{2}\\left(x_{h}-\\bar{x}\\right)^{2}}{\\left(\\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(\\frac{1}{n}+2\\left(\\frac{1}{n}\\right)\\frac{\\left(x_{h}-\\bar{x}\\right)\\sum\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}{\\left(\\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\n\n\n\nSo the sampling distribution of \\(\\hat{y}_{h}\\) is \\[\n\\begin{align}\n\\hat{y}_{h} & \\sim N\\left(\\beta_{0}+\\beta_{1}x_{h},\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\right)\n\\end{align}\n\\tag{6.1}\\]\nWe will need to estimate \\(\\sigma^{2}\\) with \\(s^{2}\\). This will mean that the confidence interval is a \\(t\\) interval.\n\n\n6.1.5 Confidence Interval for the Mean Response\nA \\(\\left(100-\\alpha\\right)100\\%\\) confidence interval for the mean response is \\[\n\\begin{align}\n\\hat{y}_{h} \\pm t_{\\alpha/2}\\sqrt{s^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)}\n\\end{align}\n\\tag{6.2}\\]"
  },
  {
    "objectID": "06_Using.html#predicting-the-response",
    "href": "06_Using.html#predicting-the-response",
    "title": "6  Using the Simple Linear Model",
    "section": "6.2 Predicting the Response",
    "text": "6.2 Predicting the Response\n\n6.2.1 The Predicted Response\nPreviously, we estimated the mean of all \\(y\\)s for some value of \\(x_{h}\\).\nSuppose we want to predict one value of the response variable \\(y\\) for some value of \\(x_{h}\\). We will denote this predicted value as \\(y_{h\\left(pred\\right)}\\).\n\n\n6.2.2 Prediction When the True Line is Known\nOur best point predictor will be the mean (since it is the most likely value). If we knew the the true regression line, then we could predict at \\[\\begin{align*}\ny_{h\\left(pred\\right)} & =\\beta_{0}+\\beta_{1}x_{h}\n\\end{align*}\\]\nThe variance of \\(y_{h\\left(pred\\right)}\\) would be \\[\\begin{align*}\nVar\\left[y_{h}\\right] & =\\sigma^{2}\n\\end{align*}\\]\nThen we can be \\(\\left(1-\\alpha\\right)100\\%\\) confident that the predicted value \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nz_{\\alpha/2}\\sigma\n\\end{align*}\\] units away from the line.\nIf we don’t know \\(\\sigma\\), then we could estimate it with \\(s\\) and we can be \\(\\left(1-\\alpha\\right)100\\%\\) confident that the predicted value \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nt_{\\alpha/2}s\n\\end{align*}\\] units away from the line.\n\n\n6.2.3 Predicting When the True Line is Unknown\nOf course, we do not know the true regression line. We will need to estimate it first.\nUsing the least squares estimators, we will predict at \\[\\begin{align*}\n\\hat{y}_{h} & =b_0+b_1x_{h}\n\\end{align*}\\]\n\n\n6.2.4 The Variance of the Predicted Response\nSince \\(\\hat{y}_{h}\\) is a random variable, it will have a sampling distribution. From Equation 6.1, that sampling distribution has a variance of \\[\\begin{align*}\nVar\\left[\\hat{y}_{h}\\right] & =\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nThus, the variance of the prediction of \\(y_{h\\left(pred\\right)}\\) will be the sum of the variance of the response variable: \\[\\begin{align*}\n\\sigma^{2}\n\\end{align*}\\] and the variance of the fitted line: \\[\\begin{align*}\n\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nSo the variance of \\(y_{h\\left(pred\\right)}\\) is \\[\\begin{align*}\nVar\\left[y_{h\\left(pred\\right)}\\right] & =\\sigma^{2}+\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\\\\n& =\\sigma^{2}\\left(1+\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\n\\end{align*}\\]\nSince \\(y\\) is normally distributed, then we have a \\(\\left(100-\\alpha\\right)100\\%\\) prediction interval for \\(y_{h\\left(pred\\right)}\\) as \\[\n\\begin{align}\n{\\hat{y}_{h} \\pm t_{\\alpha/2}\\sqrt{s^{2}\\left(1+\\frac{1}{n}+\\frac{\\left(x_{h}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)}}\n\\end{align}\n\\]#eq-w2_21\n\nExample 6.1 (Example 5.2 revisited) Let’s examine the trees data from Example 5.2.\nRecall the least squares fit:\n\nlibrary(datasets)\nlibrary(tidyverse)\n\nggplot(data=trees, aes(x=Girth, y=Volume))+\n  geom_point()+\n  geom_smooth(method='lm',formula=y~x,se = F)\n\n\n\nfit = lm(Volume~Girth, data=trees)\nfit\n\n\nCall:\nlm(formula = Volume ~ Girth, data = trees)\n\nCoefficients:\n(Intercept)        Girth  \n    -36.943        5.066  \n\n\nTo find the confidence and prediction intervals, we must construct a new data frame with the value of \\(X_h\\). This value is then used, along with the lm object, to the predict function. If no value of \\(X_h\\) is provided, then predict will provide intervals for all values of \\(x\\) found in the data used in lm.\nRecall the least squares fit:\n\n# get a 95% confidence interval for the mean Volume\n# when girth is 16\nxh=data.frame(Girth=16)\n\nfit |&gt; predict(xh,interval=\"confidence\",level=0.95)\n\n       fit      lwr      upr\n1 44.11024 42.01796 46.20252\n\n# 95% prediction interval for one value of Volume when\n# girth is 16\nfit |&gt; predict(xh,interval=\"prediction\",level=0.95)\n\n       fit     lwr      upr\n1 44.11024 35.1658 53.05469\n\n\nWe can plot the confidence interval for all values of \\(x\\) by using the geom_smooth command in ggplot:\n\nggplot(data=trees, aes(x=Girth, y=Volume)) + \n  geom_point() + \n  geom_smooth(method='lm',formula=y~x)\n\n\n\n\nWe can plot prediction intervals by adding them manually:\n\npred_int = fit |&gt; predict(interval=\"prediction\",level=0.95) |&gt; as.data.frame()\ndat = cbind(trees, pred_int)\n\nggplot(data=dat, aes(x=Girth, y=Volume)) +\n  geom_point() +\n  geom_smooth(method='lm',formula=y~x) +\n  geom_line(aes(y=lwr), color = \"red\", linetype = \"dashed\") +\n  geom_line(aes(y=upr), color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method=lm, se=TRUE)\n\n\n\n\nNote that the prediction interval (the red dashed lines) is wider than the prediction interval. This is because the prediction interval has the extra source of variability.\n\n\n\n6.2.5 Extrapolation and Precision\nWhen using the least squares prediction equation to estimate the mean value of \\(y\\) or to predict a particular value of \\(y\\) for values of \\(x\\) outside the range of your sample data, you may encounter much larger errors than expected. This practice is known as extrapolation.\nEven though the least squares model might fit the data well within the range of sample \\(x\\) values, it can poorly represent the true model for values of \\(x\\) outside this range.\nAs the sample size \\(n\\) increases, the width of the confidence interval decreases. In theory, you can achieve as precise an estimate of the mean value of \\(y\\) as desired for any given \\(x\\) by selecting a large enough sample.\nSimilarly, the prediction interval for a new value of \\(y\\) also becomes narrower as \\(n\\) increases. However, the prediction interval has a lower limit, which is reflected in the formula:\n\\[\n\\hat{y} \\pm z_{\\alpha/2} \\sigma\n\\]\nThis means that no matter how large the sample, the interval can’t shrink below a certain size unless you reduce the standard deviation of the regression model, \\(\\sigma\\). To make more accurate predictions for new values of \\(y\\), you must improve the model—either by using a curvilinear relationship with \\(x\\), adding new independent variables, or both."
  },
  {
    "objectID": "07_Checking.html",
    "href": "07_Checking.html",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "",
    "text": "“…the statistician knows…that in nature there never was a normal distribution, there never was a straight line, yet with normal and linear assumptions, known to be false, he can often derive results which match, to a useful approximation, those found in the real world.” - George Box\n\n## Residual Diagnostics\n\n7.0.1 Model Assumptions\nLet’s review the assumptions for the simple linear regression model:\n1. The *mean* of the probability distribution of $\\varepsilon$ is 0.\n2. The *variance* of the probability distribution of $\\varepsilon$ is *constant* for all settings of the independent variable $x$. \n3. The probability distribution of $\\varepsilon$ is *normal*.\n4. The errors associated with any two different observations are *independent*.\nAfter fitting the model, we will need to check these assumptions.\n\n\n7.0.2 Residuals\nWe check the assumptions of the model by examining the residuals: \\[\n\\begin{align}\n\\eqblank{e_{i}  =y_{i}-\\hat{y}_{i}}\n\\end{align}\n\\tag{7.1}\\]\nWe do this since the assumptions, with the exception of the linearity assumption, are based on the error terms \\(\\varepsilon_i\\). We can think of \\(e_i\\) as an observed value of \\(\\varepsilon_i\\)."
  },
  {
    "objectID": "07_Checking.html#residual-diagnostics",
    "href": "07_Checking.html#residual-diagnostics",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "7.1 Residual Diagnostics",
    "text": "7.1 Residual Diagnostics\n\n7.1.1 Model Assumptions\nLet’s review the assumptions for the simple linear regression model:\n\nThe mean of the probability distribution of \\(\\varepsilon\\) is 0.\nThe variance of the probability distribution of \\(\\varepsilon\\) is constant for all settings of the independent variable \\(x\\).\nThe probability distribution of \\(\\varepsilon\\) is normal.\nThe errors associated with any two different observations are independent.\n\nAfter fitting the model, we will need to check these assumptions.\n\n\n7.1.2 Residuals\nWe check the assumptions of the model by examining the residuals: \\[\n\\begin{align}\n{e_{i}  =y_{i}-\\hat{y}_{i}}\n\\end{align}\n\\tag{7.1}\\]\nWe do this since the assumptions, with the exception of the linearity assumption, are based on the error terms \\(\\varepsilon_i\\). We can think of \\(e_i\\) as an observed value of \\(\\varepsilon_i\\).\n\n\n7.1.3 Properties of Residuals\nBelow are some properties of the residuals: \\[\n\\begin{align}\n\\sum e_{i} & =0\\\\\n\\sum x_{i}e_{i} & =0\\\\\n\\sum\\hat{y}_{i}e_{i} & =0 \\\\\n\\sum y_{i} & =\\sum\\hat{y}_{i}\n\\end{align}\n\\tag{7.2}\\]\nClearly, from Equation 7.2, the mean of the residuals is \\[\n\\begin{align}\n    \\bar{e}_i=0\n\\end{align}\n\\tag{7.3}\\]\nThe variance of all \\(n\\) residuals, \\(e_1,\\ldots,e_n\\) is \\[\n\\begin{align}\n\\frac{\\sum\\left(e_{i}-\\bar{e}\\right)^{2}}{n-2} & =\\frac{\\sum e_{i}^{2}}{n-2}\\\\\n& =\\frac{SSE}{n-2}\\\\\n& =MSE\\\\\n& =s^{2}\n\\end{align}\n\\tag{7.4}\\]\n\n\n7.1.4 Semistudentized Residuals\nIt will be helpful to studentize each residuals. As always, we do this by subtracting off the mean, \\(\\bar{e}_{i}\\), and dividing by the standard error of \\(e_{i}\\).\nWe know by Equation 7.3 that \\(\\bar{e}_{i}=0\\).\nIn Equation 7.4, we said the variance of the sample of the \\(e_{i}\\)’s is MSE. For each individual \\(e_{i}\\), the standard error is not quite \\(\\sqrt{MSE}\\). The actual standard error is dependent on the predictor variable(s). We will discuss this more in multiple regression.\nFor now, we will use the approximation \\(\\sqrt{MSE}\\) and calculate \\[\\begin{align*}\ne_{i}^{*} & =\\frac{e_{i}-\\bar{e}}{\\sqrt{MSE}}\\\\\n& =\\frac{e_{i}}{\\sqrt{MSE}}\n\\end{align*}\\] We call \\(e_{i}^{*}\\) the semistudentized residual since the standard error is an approximation."
  },
  {
    "objectID": "07_Checking.html#the-linearity-assumption",
    "href": "07_Checking.html#the-linearity-assumption",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "7.2 The Linearity Assumption",
    "text": "7.2 The Linearity Assumption\n\n7.2.1 Residual Plots\nWe can check the linearity assumption by plotting the residuals vs the predictor variable or plotting the residuals vs the fitted values.\nWe usually examine a scatterplot to determine if a linear relationship between \\(x\\) and \\(y\\) is appropriate. There are times when the scatterplot makes it difficult to see if a nonlinear relationship exists. This may be the case if the observed \\(y\\) are close to the fitted line \\(\\hat{y}_i\\). This usually means the slope is steep.\n\nExample 7.1 (Weight and Height Data) In this example, we will consider the weights (in kg) and heights (in m) of 16 women ages 30-39. The dataset is from kaggle.\n\nlibrary(tidyverse)\n\ndat = read_csv(\"Weight_Height.csv\")\n\nggplot(dat, aes(x=Weight, y=Height))+\n  geom_point()\n\n\n\n#fit the model\nfit = lm(Weight~Height, data=dat)\n\n#plot with regression line\nggplot(dat, aes(x=Weight, y=Height))+\n  geom_point()+\n  geom_smooth(method=\"lm\", formula=y~x, se=F)\n\n\n\n#make dataset with Weight, the fitted values, and residuals\ndat2 = tibble(x = dat$Weight, \n              yhat = fit$fitted.values, \n              e = fit$residuals)\n\n#plot x by residuals\nggplot(dat2, aes(x=x, y=e))+\n  geom_point()+\n  geom_hline(yintercept = 0, col=\"red\")\n\n\n\n#plot fitted values by residuals\nggplot(dat2, aes(x=yhat, y=e))+\n  geom_point()+\n  geom_hline(yintercept = 0, col=\"red\")\n\n\n\n\n\n\n\n7.2.2 Plotting against Predictor Variable or Fitted Values\nPlotting the residuals against \\(x\\) will provide the same information as plotting the residuals against \\(\\hat{y}\\) for the simple linear regression model.\nWhen more predictor variables are considered, then plotting against the \\(x\\) variables and plotting against \\(\\hat{y}\\) may provide different information. It is usually helpful to plot both in that case.\n\n\n7.2.3 Data Transformation for Linearity\nWhen the linearity assumption does not hold (as seen in the residual plots), then a nonlinear model may be considered or a transformation on either \\(x\\) or \\(y\\) can be attempted to make the relationship linear.\n\n\n7.2.4 Transforming \\(x\\)\nTransforming the response variable \\(y\\) may lead to issues with other assumptions such as the constant variance assumption or the normality of \\(\\varepsilon\\) assumption.\nIf our only concern is the linearity assumption, then transforming \\(x\\) will be the best option. This transformation may be a square root transformation \\(\\sqrt{X}\\), a log transformation \\(\\log{X}\\), or some power transformation \\(X^{p}\\) were \\(p\\) is some real number.\nSometimes a transformation of \\(x\\) will not be enough to satisfy the linearity assumption. In that case, the simple linear regression model should be abandoned in favor of a nonlinear model."
  },
  {
    "objectID": "07_Checking.html#homogeneity-of-variance",
    "href": "07_Checking.html#homogeneity-of-variance",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "7.3 Homogeneity of Variance",
    "text": "7.3 Homogeneity of Variance\n\n7.3.1 Residual Plots\nAs we did previously, we can plot the residuals against the predictor variable \\(x\\) or against the fitted values \\(\\hat{y}\\) to help determine whether the variance of the error term \\(\\varepsilon\\) is constant.\nWhen the variance is constant, we say the model has homoscedasticity. When the variance is nonconstant, we say the model has heteroscedasticity.\n\n\n7.3.2 Absolute Residuals and Squared Residuals\nWhen examining a residual plot for non-constant variance, we look for any clear changes in the spread of the residuals. One common clear pattern seen when heteroscedasticity is present is a cone pattern.\nWe are usually not concerned about the sign of the residual when examining for heteroscedasticity. Thus, it is common to plot the absolute residuals or the squared residuals vs the predictor variable or fitted values.\nUsually a least squares line is then fit to the absolute residual or squared residuals plot. If this line has a significant slope, then this gives evidence that the variance is nonconstant.\n\nExample 7.2 (Diastolic Blood Pressure Data) We will model the diastolic blood pressure by the age of 54 healthy adult women. The data are found in Kutner et al1.\n\nlibrary(tidyverse)\n\ndat = read.table(\"bloodpressure.txt\", header=T)\n\nfit = lm(dbp~age, data=dat)\n\n#add the fit line without using geom_smooth\nggplot(dat, aes(x=age, y=dbp))+\n  geom_point()+\n  geom_abline(slope = fit$coefficients[2],\n              intercept = fit$coefficients[1])\n\n\n\n\nBy examining the scatterplot of dbp vs age, we already see evidence of nonconstant variance.\n\ndat$e = fit |&gt; resid()\ndat$yhat = fit |&gt; fitted()\n\n#use the geom_smooth function to add a least squares line\n#for the residuals, the least squares line will always be #horizontal at 0\nggplot(dat, aes(x=age, y=e))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n#plot the squared residuals and add least squares line\nggplot(dat, aes(x=age, y=e^2))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n#plot the squared residuals and add least squares line\nggplot(dat, aes(x=age, y=abs(e)))+\n  geom_point()+\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n\nIn any of the residual plots we examine, we see the “cone” shape of the residuals which indicates the variance is nonconstant.\n\n\n\n7.3.3 Tests for Heteroscedasticity\nWe can set up a hypothesis test for heteroscedasticity: \\[\\begin{align*}\nH_0:&\\text{ the variance is constant}\\\\\nH_a:&\\text{ the variance is non-constant}\n\\end{align*}\\]\nThe procedures we will use usually test for variance that increases or decreases over the values of \\(x\\). That is, the spread of the points about the line is a cone shape.\n\n\n7.3.4 Levene’s Test and Brown-Forsythe Test\nThe Levene test2 starts by dividing the range of the predictor variable \\(x\\) into \\(k\\) intervals. For each of the intervals, calculate the mean of the residuals in that interval \\(\\bar{e}_{j}\\) where \\(j\\) denotes the \\(j\\)th interval.\nNow, define the absolute deviation from the mean \\[\nd_{ik}=|e_{ik}-\\bar{e}_{j}|\n\\]\nAn ANOVA F-test is then performed on the \\(k\\) groups. The ANOVA F-test will be discussed more later in the course. A small p-value is evidence that the variance is non-constant over the values of \\(x\\).\nThe Brown-Forsythe test3 is a modification of the Levene test in which the median \\(\\tilde{e}_k\\) is used instead of the mean \\(\\bar{e}_l\\). This test is robust against nonnormal errors.\n\n\n7.3.5 Breusch-Pagan Test\nA second test for non-constant variance is the Breusch-Pagan test4. This test assumes the variance for each \\(\\varepsilon_i\\) is related to the values of \\(x\\) in the following way: \\[\n\\ln \\sigma^2_i = \\gamma_0 + \\gamma_1 X_i\n\\] Hence, a linear regression is assumed between \\(\\sigma^2\\) and \\(x\\). We can fit the line by squaring the residuals and regressing on \\(x\\). The third plot in example above shows the squared residuals plotted against \\(x\\) and the fitted line.\nIf the variance is non-constant, then the slope of this line will be non-zero. Thus, a test for the slope is conducted. A small p-value is evidence that the variance is non-constant.\nBecause it is testing the slope, the Brown-Forsythe test assumes the error terms are independent and normally distributed.\n\nExample 7.3 (Example 7.2 revisited) Let’s examine the blood pressure data from Example 7.2 again.\nIf you want the Brown-Forsythe test, then you will need to determine the \\(k\\) groups yourself and then use the bf.test function in onewaytests package.\nIn the lmtest package, the Breusch-Pagan test can be conducted by using the bptest function.\n\nlibrary(tidyverse)\nlibrary(lmtest)\n\ndat = read.table(\"bloodpressure.txt\", header=T)\n\nfit = lm(dbp~age, data=dat)\n\n#Breusch-Pagan test\nbptest(dbp~age, data=dat)\n\n\n    studentized Breusch-Pagan test\n\ndata:  dbp ~ age\nBP = 12.541, df = 1, p-value = 0.0003981\n\n\nSince the p-value is low, then there is sufficient evidence to conclude the variance is not constant through different values of \\(x\\)."
  },
  {
    "objectID": "07_Checking.html#diastolic-blood-pressure-data",
    "href": "07_Checking.html#diastolic-blood-pressure-data",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "7.4 Diastolic Blood Pressure Data",
    "text": "7.4 Diastolic Blood Pressure Data\nWe will model the diastolic blood pressure by the age of 54 healthy adult women. The data are found in Kutner et al1.\n1: Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied Linear Statistical Models McGraw-Hill/lrwin series operations and decision sciences."
  },
  {
    "objectID": "07_Checking.html#footnotes",
    "href": "07_Checking.html#footnotes",
    "title": "7  Checking the Linearity and Constant Variance Assumptions",
    "section": "",
    "text": "Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2004). Applied Linear Statistical Models McGraw-Hill/lrwin series operations and decision sciences.↩︎\nLevene, H. (1960) Robust Tests for Equality of Variances. In: Olkin, I., Ed., Contributions to Probability and Statistics, Stanford University Press, Palo Alto, 278-292.↩︎\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for the equality of variances. Journal of the American statistical association, 69(346), 364-367.↩︎\nBreusch, T. S., & Pagan, A. R. (1979). A simple test for heteroscedasticity and random coefficient variation. Econometrica: Journal of the econometric society, 1287-1294.↩︎"
  },
  {
    "objectID": "08_Checking2.html#checking-for-outliers",
    "href": "08_Checking2.html#checking-for-outliers",
    "title": "8  Checking the Normality and Independence Assumptions and Outliers",
    "section": "8.1 Checking for Outliers",
    "text": "8.1 Checking for Outliers\n\n8.1.1 Outliers With Respect to the Predictor Variable and the Response Variable\nWhen checking for outliers, we must think in terms of two dimensions since we have two variables, \\(x\\) and \\(y\\).\nSince we are interested in modeling the response variable \\(y\\) in the model, we will mainly be concerned with outliers with respect to \\(y\\). However, outliers with respect to \\(x\\) may also be of concern if it affects the fitted line.\n\n\n8.1.2 Effect on the Fitted Line\nA demonstration on the effect of outliers on the fitted line can be found at http://www.jpstats.org/Regression/ch_03_04.html#sub1_1\n\n\n\n8.1.3 Detecting Outliers with Semistudentized Residual Plots\nSince the effect on the fitted line is determined mainly by how far the point is from the line, we will identify outliers by examining the residuals, in particular, the semistudentized residuals \\[\\begin{align*}\ne_{i}^{*} & =\\frac{e_{i}-\\bar{e}}{\\sqrt{MSE}}\\\\\n& =\\frac{e_{i}}{\\sqrt{MSE}}\n\\end{align*}\\]\nWe can plot \\(e_{i}^{*}\\) against \\(x\\) or against \\(\\hat{y}\\). A general rule of thumb is any value of \\(e_{i}^{*}\\) below -4 or above 4 should be considered an outlier. Note that this rule is only applicable to large \\(n\\).\nWe will discuss other methods for detecting outliers after we cover multiple regression.\n\nExample 8.1 (Calculating Semistudentized Residuals) Let’s examine 50 observations of \\(x\\) and \\(y\\):\n\nlibrary(tidyverse)\n\ndat = read_csv(\"example_08_01.csv\")\n\ndat |&gt; \n  ggplot(aes(x=x,y=y))+\n    geom_point()+\n    geom_smooth(method = \"lm\")\n\n\n\nfit = lm(y~x, data=dat)\n\n#calculate semistudentized residuals\ndat = dat |&gt; \n  mutate(e.star = fit$residuals / summary(fit)$sigma)\n\ndat |&gt; \n  ggplot(aes(x=x, y=e.star))+\n    geom_point()\n\n\n\n\nWe can see from the semistudentized plot that there is one observation with a value of \\(e^*\\) below -3. Although this is not below the rule of thumb of -4, we note that our sample size \\(n\\) is only moderate and not large. So we may want to investigate an observation with a value of \\(e^*\\) below -3.\n\nOnce we see there is a potential outlier, we must investigate why it is an outlier. If the observation is unusually small or large due to a data recording error, then perhaps the value can be corrected or just deleted from the dataset. If we cannot determine this is the cause of the outlier for certain, then we should not remove the observation. This observation could be unusual just due to chance."
  },
  {
    "objectID": "08_Checking2.html#correlated-error-terms",
    "href": "08_Checking2.html#correlated-error-terms",
    "title": "8  Checking the Normality and Independence Assumptions and Outliers",
    "section": "8.2 Correlated Error Terms",
    "text": "8.2 Correlated Error Terms\n\n8.2.1 Assumption of Independence\nSince we are assuming the random error term \\(\\varepsilon\\) are normal, we want to check to see the uncorrelated errors assumption. If there is no correlation between the residuals, then we can assume independence.\n\n\n8.2.2 Residual Sequence Plots\nThe usual cause of correlation in the residuals is data taken in some type of sequence such as time or space. When the error terms are correlated over time or some other sequence, we say they are serially correlated or autocorrelated.\nWhen the data are taken in some sequence, a sequence plot of the residuals may show a pattern indicating autocorrelation. In a sequence plot, the residuals are plotted against the observation index \\(i\\). If there is no autocorrelation, then the residuals should be “randomly” spread about zero. If there is a pattern, then there is evidence of autocorrelation.\n\n\n8.2.3 Autocorrelation Function Plot\nSometime a residual sequence plot may not show an obvious pattern but autocorrelation may still exist.\nAnother plot that helps examine correlation that may not be visible in the sequence plot is the autocorrelation function plot (ACF).\nIn the ACF plot, correlations are calculated between residuals some \\(k\\) index away. That is, \\[\n\\begin{align}\nr_{k} & =\\widehat{Cor}\\left[e_{i},e_{i+k}\\right]\\\\\n& =\\frac{\\sum_{i=1}^{n-k}\\left(e_{i}-\\bar{e}\\right)\\left(e_{i+k}-\\bar{e}\\right)}{\\sum_{i=1}^{n}\\left(e_{i}-\\bar{e}\\right)^{2}}\n\\end{align}\n\\tag{8.1}\\]\nIn an ACF plot, \\(r_k\\) is plotted for varying values of \\(k\\). If the value of \\(r_k\\) is larger in magnitude than some threshold shown on the plot (usually a 95% confidence interval), then we consider this evidence of autocorrelation.\n\n\n8.2.4 Tests for Autocorrelation\nIn addition to examining serial plots and ACF plots, tests can be conducted for significant autocorrelation. In each of these tests, the null hypothesis is there is no autocorrelation.\n\n\n8.2.5 Durbin-Watson Test\nThe Durbin-Watson1 test is for autocorrelation at \\(k=1\\) in Equation 8.1. That is, it tests for correlation one index (one time point) away.\nThe Durbin-Watson test can be conducted in R with the dwtest function in the lmtest package.\n\n\n8.2.6 Ljung-Box Test\nThe Ljung-Box2 test differs from the Durbin-Watson test in that it tests for overall correlation over all lags up to \\(k\\) in Equation 8.1. For example, if \\(k=4\\) then the Ljung-Box test is for significant autocorrelation over all lags up to \\(k=4\\).\nThe Ljung-Box test can be conducted in R with the Box.test function with the argument type=(\"Ljung\"). This function is in base R.\n\n\n8.2.7 Breusch-Godfrey Test\nThe Breusch-Godfrey34 test is similar to the Ljung-Box test in that it tests for overall correlation over all lags up to \\(k\\). The difference between the two test is not of concern in the regression models we will examine in this course. When using time series models, then the Breusch-Godfrey test is preferred over the Ljung-Box test due to asymptotic justification.\nThe Breusch-Godfrey test can be conducted in R with the bgtest function in the lmtest package.\n\nExample 8.2 (Portland Crime Data) Let’s look at data collected on the mean temperature for each day in Portland, OR, and the number of non-violent crimes reported that day. The crime data was part of a public database gathered from www.portlandoregon.gov. The data are presented in order by day. The variable \\(x\\) in the dataset is the day index number.\n\nlibrary(tidyverse)\nlibrary(lmtest)\nlibrary(forecast)\n\ndat = read_csv(\"PortlandWeatherCrime.csv\")\n\n#the file does not have a name for the index variable\nnames(dat)[1] = \"day\"\n\ndat |&gt; \n  ggplot(aes(x=Mean_Temp, y=Num_Total_Crimes))+\n    geom_point()+\n    geom_smooth(method=\"lm\")\n\n\n\nfit = lm(Num_Total_Crimes~Mean_Temp, data=dat)\nfit |&gt; summary()\n\n\nCall:\nlm(formula = Num_Total_Crimes ~ Mean_Temp, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-168.198  -41.055    0.149   40.455  183.680 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 281.0344     6.4456   43.60   &lt;2e-16 ***\nMean_Temp     4.3061     0.1116   38.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.69 on 1765 degrees of freedom\nMultiple R-squared:  0.4576,    Adjusted R-squared:  0.4573 \nF-statistic:  1489 on 1 and 1765 DF,  p-value: &lt; 2.2e-16\n\ndat$res = fit |&gt; resid()\n\nggplot(dat, aes(x=day, y=res))+\n  geom_point()\n\n\n\n\nWe can see that the residuals have a pattern where the values at the lower levels of the index tend to be below zero whereas the values at the higher levels of the index tend to be above zero. This is evidence of autocorrelation in the residuals.\n\nggAcf(dat$res)\n\n\n\n\nThe values of the ACF at all lags are beyond the blue guideline for significant autocorreleation.\nNote that in the Ljung-Box test and the Breusch-Godfrey test below, we tested up to lag 7. We chose this lag since the data was taken over time and it would make sense for values at seven days apart to be similar. That is, we expect the number of crimes on Mondays to be similar, the number of crimes on Tuesdays to be similar, etc.\n\ndwtest(fit)\n\n\n    Durbin-Watson test\n\ndata:  fit\nDW = 0.66764, p-value &lt; 2.2e-16\nalternative hypothesis: true autocorrelation is greater than 0\n\nBox.test(dat$res, lag=7,type=\"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  dat$res\nX-squared = 3865, df = 7, p-value &lt; 2.2e-16\n\nbgtest(fit, order=7)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 7\n\ndata:  fit\nLM test = 977.84, df = 7, p-value &lt; 2.2e-16\n\n\n\nWhen the assumption of independence is violated, then a difference in the \\(y\\) values could help remove the autocorrelation. This difference is \\[\ny^{\\prime} = y_i - y_{i-k}\n\\] where \\(k\\) is some max lag where autocorrelation is significant. This difference \\(Y^{\\prime}\\) is then regressed on \\(x\\). This difference may not help, in which case a time series model would be necessary."
  },
  {
    "objectID": "08_Checking2.html#normality-of-the-residuals",
    "href": "08_Checking2.html#normality-of-the-residuals",
    "title": "8  Checking the Normality and Independence Assumptions and Outliers",
    "section": "8.3 Normality of the Residuals",
    "text": "8.3 Normality of the Residuals\n\n8.3.1 The Normality Assumption\nIf we do not have normality of the error terms, then the t-tests and t-intervals for \\({\\beta}_0\\) and \\({\\beta}_1\\) would not be valid.\nFurthermore, the confidence interval for the mean response and the prediction interval for the response would not be valid.\nWe can check the normality of error terms by examining the residuals of the fitted line.\n\n\n8.3.2 Graphically Checking Normality\nWe can graphically check the distribution of the residuals. The two most common ways to do this is with a histogram or with a normal probability plot.\nAnother (more general) name for a normal probability plot is a normal quantile-quantile (QQ) plot.\nFor a histogram, we check to see if the shape is approximately close to that of a normal distribution.\nFor a QQ plot, we check to see if the points approximately follow a straight line. Major departures from a straight line indicates nonnormality.\nIt is important to note that we will never see an exact normal distribution is real-world data. Thus, we will always look for approximate normality in the residuals.\nThe inferences discussed previously are still valid for small departure of normality. However, major departures from normality will lead to incorrect p-values in the hypothesis tests and incorrect coverages in the intervals.\n\n\n8.3.3 Examples of QQ-plots\nBelow are some examples of histograms and QQ-plots for some simulated datasets.\n\n\n\n\n\n\n\n(a) Normal - Histogram\n\n\n\n\n\n\n\n(b) Normal - QQ plot\n\n\n\n\n\n\n\n\n\n(c) Right skewed - Histogram\n\n\n\n\n\n\n\n(d) Right skewed - QQ plot\n\n\n\n\n\n\n\n\n\n(e) Heavy right skewed - Histogram\n\n\n\n\n\n\n\n(f) Heavy right skewed - QQ plot\n\n\n\n\n\n\n\n\n\n(g) Left skewed - Histogram\n\n\n\n\n\n\n\n(h) Left skewed - QQ plot\n\n\n\n\n\n\n\n\n\n(i) Heavy tails - Histogram\n\n\n\n\n\n\n\n(j) Heavy tails - QQ plot\n\n\n\n\n\n\n\n\n\n(k) No tails - Histogram\n\n\n\n\n\n\n\n(l) No tails - QQ plot\n\n\n\n\nFigure 8.1: Examples of QQ-Plots\n\n\n\n\n8.3.4 The Shapiro-Wilk Test\nThere are a number of hypothesis test for normality. The most popular test is the Shapiro-Wilk5 test. This test has been found to have the most power among many of the other tests for normality6.\nIn the Shapiro-Wilk test, the null hypothesis is that the data are normally distributed and the alternative is that the data are not normally distributed.\nThis test can be conducted using the shapiro.test function in base R."
  },
  {
    "objectID": "08_Checking2.html#footnotes",
    "href": "08_Checking2.html#footnotes",
    "title": "8  Checking the Normality and Independence Assumptions and Outliers",
    "section": "",
    "text": "Durbin, J., & Watson, G. S. (1951). Testing for Serial Correlation in Least Squares Regression. II. Biometrika. Vol 38. (pp. 159-177).↩︎\nLjung, G. M., & Box, G. E. (1978). On a measure of lack of fit in time series models. Biometrika, 65(2), 297-303.↩︎\nBreusch, T. S. (1978). Testing for Autocorrelation in Dynamic Linear Models. Australian Economic Papers. 17: 334–355.↩︎\nGodfrey, L. G. (1978). Testing Against General Autoregressive and Moving Average Error Models when the Regressors Include Lagged Dependent Variables. Econometrica. 46: 1293–1301↩︎\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52(3-4), 591-611.↩︎\nRazali, N. M., & Wah, Y. B. (2011). Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of statistical modeling and analytics, 2(1), 21-33.↩︎"
  }
]