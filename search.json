[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 3386 Regression Analysis",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 3386 - Regression Analysis.\nPrerequisites: MTH 2311 - Linear Algebra, MTH 2321 - Calculus III, and STA 3381 - Probability and Statistics\n\nCourse Description:\nA development of regression techniques including simple linear regression, multiple regression, logistic regression and Poisson regression with emphasis on model assumptions, parameter estimation, variable selection and diagnostics."
  },
  {
    "objectID": "01_Intro.html#the-probabilistic-model",
    "href": "01_Intro.html#the-probabilistic-model",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.1 The Probabilistic Model",
    "text": "1.1 The Probabilistic Model\nMost students who take an intro stats course are familiar with the idea of a random variable. Students are usually introduced to random variables using the notation \\(X\\).\nIn the technical sense, capital \\(X\\) should denote the random variable (that is, the function itself), while lowercase \\(x\\) denotes the values of that random variable. We will be loose on that convention, so you will see the lowercase used almost extensively even when discussing the random variable itself.\n\n\n\n\n\n\nReview: Random Variable\n\n\n\nA random variable is a function that assigns a numeric value to the outcomes in the sample space.\n\n\nIn regression, the random variable of interest is usually denoted as \\(y\\).\nWe want to predict or model (explain) this variable. Thus, we call this the response (or dependent) variable.\nIf we have measurements of this random variable, then we can express each value \\(y\\) as the mean value of \\(y\\) plus some random error.\nThat is, we can model the variable as \\[\n    y = E(y) + \\varepsilon\n\\tag{1.1}\\]\nwhere \\[\\begin{align*}\n    y = &\\text{ dependent variable}\\\\\n    E(y) =& \\text{ mean (or expected) value of } y\\\\\n    \\varepsilon =& \\text{ random error}\n\\end{align*}\\]\nThis model is referred to as a probabilistic model for \\(y\\). The term “probabilistic” is used because, under certain assumptions, we can make probability-based statements about the extent of the difference between \\(y\\) and \\(E(y)\\).\nFor example, we might assert that the error term, \\[\n    \\varepsilon = y - E(y)\n\\] follows a normal distribution.\nIn practice, we will use sample data to estimate the parameters of the probabilistic model—specifically, the mean \\(E(y)\\) and the random error \\(\\varepsilon\\).\nWe will later discuss a common assumption in regression: that the mean error is zero.\nIn other words, \\[\n    E(\\varepsilon) = 0\n\\]\nGiven this assumption, our best estimate of \\(\\varepsilon\\) is zero. Therefore, we only need to estimate \\(E(y)\\).\nThe simplest method of estimating \\(E(y)\\) is to use the sample mean of \\(y\\) which we will denote as \\[\\begin{align*}\n    \\bar y= \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{align*}\\]\nIf we desired to predict a value of \\(y\\), then our best prediction would be just the sample mean: \\[\\begin{align*}\n    \\hat y = \\bar y\n\\end{align*}\\] where \\(\\hat y\\) denotes a predicted value of \\(y\\).\nThis would be the case with univariate data (we only have one variable in our data: \\(y\\)).\nUnfortunately, this simple model does not take into consideration a number of variables, called independent variables, that may help predict the response variable.\nIndependent variables are also called predictor or explanatory variables.\nThe process of identifying the mathematical model that describes the relationship between \\(y\\) and a set of independent variables, and that best fits the data, is known as regression analysis."
  },
  {
    "objectID": "01_Intro.html#sec-regoverview",
    "href": "01_Intro.html#sec-regoverview",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.2 Overview of Regression Analysis",
    "text": "1.2 Overview of Regression Analysis\nWe will denote the independent variables as \\[\\begin{align*}\n    x_1, x_2, \\ldots, x_k\n\\end{align*}\\] where \\(k\\) is the number of independent variables.\nThe goal of regression analysis is to create a prediction equation that accurately relates \\(y\\) to independent variables, allowing us to predict \\(y\\) for given values of \\(x_1, x_2, \\ldots, x_k\\) with minimal prediction error.\nWhen predicting \\(y\\), we also need a measure of the reliability of our prediction, indicating how large the prediction error might be.\nThese elements form the core of regression analysis.\nBeyond predicting \\(y\\), a regression model can also estimate the mean value of \\(y\\) for specific values of \\(x_1, x_2, \\ldots, x_k\\) and explore the relationship between \\(y\\) and one or more independent variables.\nThe process of regression analysis typically involves six key steps:\n\nHypothesize the form of the model for \\(E(y)\\).\nCollect sample data.\nEstimate the model’s unknown parameters using the sample data.\nDefine the probability distribution of the random error term, estimate any unknown parameters, and validate the assumptions made about this distribution.\nStatistically assess the model’s usefulness.\nIf the model is effective, use it for prediction, estimation, and other purposes."
  },
  {
    "objectID": "01_Intro.html#collecting-the-data-for-regression",
    "href": "01_Intro.html#collecting-the-data-for-regression",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.3 Collecting the Data for Regression",
    "text": "1.3 Collecting the Data for Regression\nThe first step listed above will be discussed later.\nOnce you’ve proposed a model for \\(E(y)\\), the next step is to gather sample data to estimate the model.\nThis means collecting data on both the response variable \\(y\\) and the independent variables \\(x_1, x_2, \\ldots, x_k\\) for each observation in your sample. In regression analysis, the sample includes data on multiple variables: \\[\ny, x_1, x_2, \\ldots, x_k\n\\] This is known as multivariate data.\nRegression data can be either observational or experimental:\nFor observational data no control is exerted over the independent variables (\\(x\\)’s). For example, recording people’s ages and their corresponding blood pressure levels without influencing either.\nFor experimental data the independent variables are controlled or manipulated. For instance, setting different fertilizer amounts for crops to observe the impact on growth.\nSuppose you want to model a student’s annual GPA (\\(y\\)). One approach is to randomly select a sample of \\(n=100\\) students and record their GPA along with the values of each predictor variable.\nData for the first three students in the sample are shown in Table 1.1.\n\n\nTable 1.1: Values of the response variable and predictor variables for the first three students.\n\n\n\nStudent 1\nStudent 2\nStudent 3\n\n\n\n\nAnnual GPA \\(y\\)\n3.8\n2.7\n3.5\n\n\nStudy Hours per Week, \\(x_1\\)\n15\n5\n10\n\n\nClass Attendance, \\(x_2\\) (days)\n30\n20\n25\n\n\nExtracurriculars, \\(x_3\\)\n2\n1\n3\n\n\nAge, \\(x_4\\) (years)\n21\n19\n22\n\n\nEmployed, \\(x_5\\) (1 if yes, 0 if no)\n0\n1\n0\n\n\nLives On Campus, \\(x_6\\) (1 if yes, 0 if no)\n1\n0\n1\n\n\n\n\nIn this example, the \\(x\\) values, like study hours, class attendance, and extracurricular activities, are not predetermined before observing GPA \\(y\\); thus, the \\(x\\) values are uncontrolled. Therefore, the sample data are observational.\n\nDetermining Sample Size for Regression with Observational Data\nWhen applying regression to observational data, the required sample size for estimating the mean \\(E(y)\\) depends on three key factors:\n\nEstimated population standard deviation\nConfidence level\nDesired margin of error (half-width of the confidence interval)\n\nHowever, unlike the univariate case, \\(E(y)\\) is modeled as a function of multiple independent variables, which adds complexity. The sample size must be large enough to estimate and test all parameters in the model.\nTo ensure a sufficient sample size, a common guideline is to select a sample size \\(n\\) that is at least 10 times the number of parameters in the model.\nFor instance, if a university registrar’s office uses the following model for the annual GPA \\(y\\) of a current student:\n\\[E(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_6 x_6\\]\nwhere \\(x_1, x_2, \\dots, x_6\\) are defined in Table 1.1, the model includes six \\(\\beta\\) parameters (excluding \\(\\beta_0\\)). Therefore, they should include at least:\n\\[ 10 \\times 6 = 60 \\]\nstudents in the sample.\n\n\nExperimental Data\nThe second type of data in regression, experimental data, is generated through designed experiments where the independent variables are set in advance (i.e., controlled) before observing the value of \\(y\\).\nFor instance, consider a scenario where a researcher wants to study the effect of two independent variables—say, fertilizer amount \\(x_1\\) and irrigation level \\(x_2\\)—on the growth rate \\(y\\) of plants. The researcher could choose three levels of fertilizer (10g, 20g, and 30g) and three levels of irrigation (1L, 2L, and 3L) and measure the growth rate in one plant for each of the \\(3\\times 3=9\\) fertilizer–irrigation combinations (see Table 1.2 below).\n\n\nTable 1.2: Values of the response variable and two independent variables for the growth rate of plants.\n\n\nFertilizer, \\(x_1\\)\nIrrigation, \\(x_2\\)\nGrowth Rate, \\(y\\)\n\n\n\n\n10g\n1L\n5.2\n\n\n10g\n2L\n6.1\n\n\n10g\n3L\n5.8\n\n\n20g\n1L\n7.0\n\n\n20g\n2L\n7.5\n\n\n20g\n3L\n7.3\n\n\n30g\n1L\n8.4\n\n\n30g\n2L\n8.7\n\n\n30g\n3L\n8.1\n\n\n\n\nIn this experiment, the settings of the independent variables are controlled, in contrast to the uncontrolled nature of observational data, like in the real estate sales example.\nIn many studies, it is often not possible to control the values of the \\(x\\)’s, so most data collected for regression are observational.\nSo, why do we differentiate between these two types of data? We will learn that inferences from regression studies based on observational data have more limitations than those based on experimental data. Specifically, establishing a cause-and-effect relationship between variables is much more challenging with observational data than with experimental data."
  },
  {
    "objectID": "02_Fitting.html#the-straight-line-probabilistic-model",
    "href": "02_Fitting.html#the-straight-line-probabilistic-model",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.1 The Straight-Line Probabilistic Model",
    "text": "2.1 The Straight-Line Probabilistic Model\nWhen studying environmental science, consider modeling the monthly carbon dioxide (CO₂) emissions \\(y\\) of a city as a function of its monthly industrial activity \\(x\\).\nThe first question to ask is: Do you believe an exact (deterministic) relationship exists between these two variables?\nIn other words, can we predict the exact value of CO₂ emissions if industrial activity is known?\nIt’s unlikely. CO₂ emissions depend on various factors beyond industrial activity, such as weather conditions, regulatory policies, and transportation levels. Even with multiple variables in the model, it’s improbable that we could predict monthly emissions precisely.\nThere will almost certainly be some variation in emissions due to random phenomena that cannot be fully explained or modeled.\nTherefore, we should propose a probabilistic model for CO₂ emissions that accounts for this random variation:\n\\[\ny = E(y) + \\varepsilon\n\\]\nThe random error component,\\(\\varepsilon\\), captures all unexplained variations in emissions caused by omitted variables or unpredictable random factors.\nThe random error plays a key role in hypothesis testing, determining confidence intervals for the model’s deterministic portion, and estimating the prediction error when using the model to predict future values of \\(y\\).\nLet’s start with the simplest probabilistic model—a first-order linear model that graphs as a straight line."
  },
  {
    "objectID": "02_Fitting.html#a-first-order-linear-model",
    "href": "02_Fitting.html#a-first-order-linear-model",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.2 A First-Order Linear Model",
    "text": "2.2 A First-Order Linear Model\nThe first-order linear model is expessed as \\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nwhere:\n\n\\(y\\) is the dependent variable (also known as the response variable)\n\\(x\\) is the independent variable (used as a predictor of \\(y\\))\n\\(E(y) = \\beta_0 + \\beta_1 x\\) is the deterministic component\n\\(\\varepsilon\\) is the random error component\n\\(\\beta_0\\) is the y-intercept of the line (the point where the line intersects the y-axis)\n\\(\\beta_1\\) is the slope of the line (the change in the mean of \\(y\\) for every 1-unit increase in \\(x\\))\n\nWe use Greek symbols \\(\\beta_0\\) and \\(\\beta_1\\) to denote the y-intercept and slope of the line. These are population parameters with values that would only be known if we had access to the entire population of \\((x, y)\\) measurements.\n\n\n\n\n\n\nReview: Greek Letters in Notation\n\n\n\nUsually, in Statistics, lower-case Greek letters are used to denote population parameters. In our model above, we have an exception. The Greek letter \\(\\varepsilon\\) is not a parameter, but a random variable (parameters are not random variables in frequentist statistics).\n\n\nAs discussed in Section 1.2, regression can be viewed as a six-step process. For now, we’ll focus on steps 2-6, using the simple linear regression model. We’ll explore more complex models later."
  },
  {
    "objectID": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "href": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.3 Fitting the Model: The Method of Least Squares",
    "text": "2.3 Fitting the Model: The Method of Least Squares\nSuppose we have the data shown in Table 2.1 below and plotted in the scatterplot in Figure 2.1.\n\n\nTable 2.1: Data for Scatterplot\n\n\nx\ny\n\n\n\n\n1\n2\n\n\n2\n1.4\n\n\n2.75\n1.6\n\n\n4\n1.25\n\n\n6\n1\n\n\n7\n0.5\n\n\n8\n0.5\n\n\n10\n0.4\n\n\n\n\n\nlibrary(tidyverse)\nx = c(1, 2, 2.75, 4, 6, 7, 8, 10)\ny = c(2, 1.4, 1.6, 1.25, 1, 0.5, 0.5, 0.4)\ndat = tibble(x, y)\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point()\n\n\n\n\nFigure 2.1: Scatterplot of the data in Table 2.1\n\n\n\n\nWe hypothesize that a straight-line model relates y to x, as follows:\n\\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nHow can we use the data from the eight observations in Table 2.1 to estimate the unknown y-intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\))?\nWe can start by trying some lines and see how well they fit the data. But how do we measure how well a line fits the data?\nA quantitative method to evaluate how well a straight line fits a set of data is by measuring the deviations of the data points from the line.\n\n\n\n\n\n\nReview: Deviations of Response Variable\n\n\n\n\\(y\\) is the variable of interest, so we are focused on the differences between observed \\(y\\) and the predicted value of \\(y\\)\n\n\nWe calculate the magnitude of the deviations (the differences between observed and predicted values of \\(y\\)).\nThese deviations, or prediction errors, represent the vertical distances between observed and predicted values of \\(y\\).\nSuppose we try to fit the line \\[\n    \\hat{y} =2-.2x\n\\tag{2.1}\\]\nThis line can be seen in Figure 2.2.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 2, slope = -0.2, color = \"red\")\n\n\n\n\nFigure 2.2: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.1\n\n\n\n\nThe observed and predicted values of \\(y\\), their differences, and their squared differences are shown in the table below.\n\n\nTable 2.2: Deviations and squared deviations of the line in Equation 2.1 and the data in Table 2.1.\n\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\((y - \\hat{y})\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n\n1\n2\n1.8\n0.2\n0.004\n\n\n2\n1.4\n1.6\n-0.2\n0.004\n\n\n2.75\n1.6\n1.45\n0.15\n0.0225\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.8\n0.2\n0.04\n\n\n7\n0.5\n0.6\n-0.1\n0.01\n\n\n8\n0.5\n0.4\n0.1\n0.01\n\n\n10\n0.4\n0\n0.4\n0.16\n\n\n\n\nNote that the sum of the errors (SE) is 0.8, and the sum of squares of the errors (SSE), which emphasizes larger deviations from the line, is 0.325.\nWe can try another line to see if we do better at predicting \\(y\\) (that is, have smaller SSE).\nLet’s try the line \\[\n    \\hat{y} =1.8-.15x\n\\tag{2.2}\\]\nThis line can be seen in Figure 2.3.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 1.8, slope = -0.15, color = \"red\")\n\n\n\n\nFigure 2.3: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.2\n\n\n\n\nThe fit results are shown in Table 2.3.\n\n\nTable 2.3: Deviations and squared deviations of the line in Equation 2.2 and the data in Table 2.1.\n\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\(y - \\hat{y}\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n\n1\n2\n1.65\n0.35\n0.1225\n\n\n2\n1.4\n1.5\n-0.1\n0.01\n\n\n2.75\n1.6\n1.3875\n0.2125\n0.04515625\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.9\n0.1\n0.01\n\n\n7\n0.5\n0.75\n-0.25\n0.0625\n\n\n8\n0.5\n0.6\n-0.1\n0.01\n\n\n10\n0.4\n0.3\n0.1\n0.01\n\n\n\n\nThe SSE for this line is 0.2727, which is lower than the SSE for the previous line, indicating a better fit.\nWhile we could try additional lines to achieve a lower SSE, there are infinitely many possibilities since \\(\\beta_0\\) and \\(\\beta_1\\) can take any real value.\nUsing Calculus, we can attempt to minimize the SSE for the generic line \\[\\begin{align*}\n    \\hat{y} = b_0 +b_1 x\n\\end{align*}\\]\nWe will denote the sum of the squared distances with \\(Q\\): \\[\nQ=\\sum \\left(y_i-\\hat{y}_i\\right)^2\n\\tag{2.3}\\]\nWe determine the “best” line as the one that minimizes \\(Q\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo minimize \\(Q\\), we differentiate it with respect to \\(b_{0}\\) and \\(b_{1}\\): \\[\\begin{align*}\n\\frac{\\partial Q}{\\partial b_{0}} & =-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{1}} & =-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\n\\end{align*}\\]\nSetting these partial derivatives equal to 0, we have \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\\\\\n-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\n\\end{align*}\\] Looking at the first equation, we can simplify as \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum y_{i}-\\sum b_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}-nb_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}=nb_{0}+b_{1}\\sum x_{i}\n\\end{align*}\\]\nSimplifying the second equation gives us \\[\\begin{align*}\n-2\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}-b_0\\sum x_{i}-b_1\\sum x_{i}^{2}=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align*}\\]\n\n\n\nThe two equations \\[\n\\begin{align}\n\\sum y_{i} & =nb_0+b_1\\sum x_{i}\\nonumber\\\\\n\\sum x_{i}y_{i} & =b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align}\n\\tag{2.4}\\]\nare called the normal equations.\nWe now have two equations and two unknowns (\\(b_0\\) and \\(b_1\\)). We can solve the equations simultaneously. We solve the first equation for \\(b_0\\) which gives us \\[\\begin{align*}\nb_0 & =\\frac{1}{n}\\left(\\sum y_{i}-b_1\\sum x_{i}\\right)\\\\\n& =\\bar{y}-b_1\\bar{x}.\n\\end{align*}\\]\nWe now substitute this into the second equation in Equation 2.4. Solving this for \\(b_1\\) gives us \\[\\begin{align*}\n& \\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n& \\quad\\Longrightarrow\\sum x_{i}y_{i}=\\left(\\bar{y}-b_1\\bar{x}\\right)\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n&\\quad\\Longrightarrow b_1=\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}.\n\\end{align*}\\]\nThe equations \\[\n\\begin{align}\nb_0 & =\\bar{y}-b_1\\bar{x}\\\\\nb_1 & =\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{2.5}\\] are called the least squares estimators.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo show these estimators are the minimum, we take the second partial derivatives of \\(Q\\): \\[\\begin{align*}\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{0}\\right)^{2}} & =2n\\\\\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{1}\\right)^{2}} & =2\\sum x_{i}^{2}\n\\end{align*}\\] Since these second partial derivatives are both positives, then we know the least squares estimators are the minimum.\n\n\n\nThe least squares estimators in Equation 2.5 can be expressed in simpler terms if we let \\[\\begin{align*}\nSS_{xx} &= \\sum \\left(x_i-\\bar x\\right)^2 \\\\\nSS_{xy} &= \\sum \\left(x_i-\\bar x\\right)\\left(y_i - \\bar y\\right)\n\\end{align*}\\]\nThe least squares estimates become \\[\\begin{align}\n{b_1=\\frac{SS_{xy}}{SS_{xx}}}\\\\\n{b_0=\\bar{y}-b_1\\bar{x}}\n\\end{align}\\]\nTo recap: The straight line model for the response \\(y\\) in terms of \\(x\\) is \\[\\begin{align*}\n{y = \\beta_0 + \\beta_1 x + \\varepsilon}\n\\end{align*}\\]\nThe line of means is \\[\\begin{align*}\n{E(y) = \\beta_0 + \\beta_1 x }\n\\end{align*}\\]\nThe fitted line (also called the least squares line) is \\[\\begin{align*}\n{\\hat{y} = b_0 + b_1 x }\n\\end{align*}\\]\nFor a given data point, \\((x_i, y_i)\\), the observed value of \\(y\\) is denoted as \\(y_i\\) and the predicted value of \\(y\\) is obtained by substituting \\(x_i\\) into the prediction equation: \\[\\begin{align*}\n{\\hat{y}_i = b_0 + b_1 x_i }\n\\end{align*}\\]\nThe deviation of the \\(i\\)th value of \\(y\\) from its predicted value, called the \\(i\\)th residual, is \\[\\begin{align*}\n{ \\left(y_i-\\hat{y}_i\\right) }\n\\end{align*}\\] Thus, SSE is just the sum of the squared residuals."
  },
  {
    "objectID": "03_Properties.html#properties-of-the-least-squares-estimators",
    "href": "03_Properties.html#properties-of-the-least-squares-estimators",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.1 Properties of the Least Squares Estimators",
    "text": "3.1 Properties of the Least Squares Estimators\n\n3.1.1 Linear Estimators\nNote that the least squares estimators are linear functions of the observations \\(y_{1},\\ldots,y_{n}\\). That is, both \\(b_0\\) and \\(b_1\\) can be written as a linear combination of the \\(y\\)’s.\nSince \\(y\\) is the variable that we want to model, we call an estimator for some parameter that takes the form of a linear combination of \\(y\\) a linear estimator.\n\n\n3.1.2 \\(b_1\\) as a Linear Estimator\nWe can express \\(b_1\\) as \\[\n\\begin{align}\nb_1 & =\\sum k_{i}y_{i}\n\\end{align}\n\\tag{3.1}\\] where \\[\\begin{align*}\nk_{i} & =\\frac{x_{i}-\\bar{x}}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align*}\\]\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe first note that \\(\\sum\\left(x_{i}-\\bar{x}\\right)=0\\).\nWe now rewrite \\(b_1\\) as \\[\\begin{align*}\nb_1 & =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}-\\bar{y}\\sum\\left(x_{i}-\\bar{x}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n& =\\left(\\frac{1}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)\\sum\\left(x_{i}-\\bar{x}\\right)y_{i}\n\\end{align*}\\]\n\n\n\nFrom Equation 3.1, we see that \\(b_1\\) is a linear combination of the \\(y\\)’s since \\(k_{i}\\) are known constants (recall that \\(x_{i}\\) are treated as known constants).\n\n\n3.1.3 \\(b_0\\) as a Linear Estimator\nWe can write \\(b_0\\) as \\[\n\\begin{align}\nb_0 & =\\sum c_{i}y_{i}\n\\end{align}\n\\tag{3.2}\\] where \\[\\begin{align*}\nc_{i} & =\\frac{1}{n}-\\bar{x}k_{i}\n\\end{align*}\\]\nTherefore, \\(b_0\\) is a linear combination of \\(y_{i}\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nWe can rewrite \\(b_0\\) as \\[\\begin{align*}\nb_0 & =\\bar{y}-b_{1}\\bar{x}\\\\\n& =\\frac{1}{n}\\sum y_{i}-\\bar{x}\\sum k_{i}y_{i}\\\\\n& =\\sum\\left(\\frac{1}{n}-\\bar{x}k_{i}\\right)y_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "03_Properties.html#model-assumptions",
    "href": "03_Properties.html#model-assumptions",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.2 Model Assumptions",
    "text": "3.2 Model Assumptions\nLet’s now focus on the random component \\(\\varepsilon\\) in the probabilistic model and its connection to the errors in estimating \\(\\beta_0\\) and \\(\\beta_1\\).\nSpecifically, we’ll explore how the probability distribution of \\(\\varepsilon\\) influences the accuracy of the model in representing the true relationship between the dependent variable \\(y\\) and the independent variable \\(x\\).\nWe make four key assumptions about the probability distribution of \\(\\varepsilon\\):\n\nThe mean of \\(\\varepsilon\\)’s probability distribution is 0. This means that, on average, the errors cancel out over an infinitely large number of experiments for each value of the independent variable \\(x\\). Consequently, the mean value of \\(y\\), \\(E(y)\\), for a given \\(x\\) is \\(E(y) = \\beta_0 + \\beta_1 x\\).\nThe variance of \\(\\varepsilon\\)’s probability distribution is constant across all values of the independent variable \\(x\\). For our linear model, this implies that the variance of \\(\\varepsilon\\) is a constant, say, \\(\\sigma^2\\), regardless of the value of \\(x\\).\nThe probability distribution of \\(\\varepsilon\\) is normal.\nThe errors for different observations are independent. In other words, the error for one value of \\(y\\) does not influence the errors for other \\(y\\) values.\n\nThe implications of the first three assumptions can be seen in Figure 3.1;, which shows distributions of errors for four particular values of \\(x\\).\n\n\n\n\n\nFigure 3.1: The probability distribution of \\(\\varepsilon\\). At every value of \\(x\\), there is a distribution of \\(y\\) values that satisfy the four assumptions listed above. The red points are observed values.\n\n\n\n\nFrom Figure 3.1, we see that the probability distributions of the errors are normal, with a mean of 0 and a constant variance \\(\\sigma^2\\).\nThe line in the middle of the curve that goes to the regression line represents the mean value of \\(y\\) for a given value of \\(x\\). The line of means is given by the equation: \\[\nE(y) = \\beta_0 + \\beta_1 x\n\\]\nThese assumptions allow us to create measures of reliability for the least squares estimators and to develop hypothesis tests to evaluate the utility of the least squares line.\nVarious diagnostic techniques are available for checking the validity of these assumptions, and these diagnostics suggest remedies when the assumptions seem invalid.\nTherefore, it is crucial to apply these diagnostic tools in every regression analysis. We will discuss these techniques in detail later.\nIn practice, the assumptions do not need to hold exactly for least squares estimators and test statistics to have the reliability we expect from a regression analysis. The assumptions will be sufficiently satisfied for many real-world applications."
  },
  {
    "objectID": "03_Properties.html#an-estimator-of-sigma2",
    "href": "03_Properties.html#an-estimator-of-sigma2",
    "title": "3  Properties of the Least Squares Estimators and Model Assumptions",
    "section": "3.3 An Estimator of \\(\\sigma^2\\)",
    "text": "3.3 An Estimator of \\(\\sigma^2\\)\nThe variability of random error, measured by its variance \\(\\sigma^2\\), plays a crucial role in the accuracy of estimating model parameters \\(\\beta_0\\) and \\(\\beta_1\\), as well as in the precision of predictions when using \\(\\hat{y}\\) to estimate \\(y\\) for a given value of \\(x\\). As a result, it is expected that \\(\\sigma^2\\) will appear in the formulas for confidence intervals and test statistics.\nIn most real-world scenarios, \\(\\sigma^2\\) is unknown and must be estimated using the available data. The best estimate for \\(\\sigma^2\\) is \\(s^2\\), calculated by dividing the sum of squares of residuals by the associated degrees of freedom (df). The sum of squares of residuals is given by: \\[\nSSE = \\sum \\left(y_i - \\hat{y}_i\\right)^2\n\\]\nIn a simple linear regression model, 2 degrees of freedom are used to estimate the y-intercept and slope, leaving \\((n - 2)\\) degrees of freedom for estimating the error variance. Thus, the estimate of \\(\\sigma^2\\) is: \\[\n\\begin{align*}\ns^2 &= \\frac{SSE}{n-2}\\\\\n&= \\frac{\\sum \\left(y_i - \\hat{y}_ i\\right)^2}{n-2}\n\\end{align*}\n\\]\nThis \\(s^2\\) serves as the basis for further statistical analysis, including the construction of confidence intervals and hypothesis testing.\nThe value of \\(s^2\\) is referred to as the mean square error (MSE).\nThe value \\[\\begin{align*}\ns &= \\sqrt{s^2}\n\\end{align*}\\] is referred to as the standard error of the regression model or as the root MSE (RMSE).\nUsing the empirical rule, we expect approximately 95% of the observed \\(y\\) values to lie within \\(2s\\) of their respective least squares predicted values, \\(\\hat y\\).\n\n\n\n\n\n\nReview: The Empirical Rule\n\n\n\nRecall the empirical rule applies to distributions that are mound-shaped and symmetric. It state that approximately 68% of the distribution is within one standard deviation of the mean, approximately 95% of the distribution is within two standard deviations of the mean, and approximatley 99.7% of the distribution is withing three standard deviations of the mean. Since we assume \\(\\varepsilon\\) is normally distributed, then the empirical rule holds.\n\n\n\nExample 3.2 (Example 3.1 - revisited) We can use the summary() function with the fit from lm to obtain the summary stats of the fit.\n\nfit = lm(mpg~wt, data = mtcars)\n\nfit |&gt; summary()\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe standard error (RMSE) of the fit is 3.046.\nWe can obtain the MSE with the following code:\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nwt         1 847.73  847.73  91.375 1.294e-10 ***\nResiduals 30 278.32    9.28                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe MSE is 9.28."
  },
  {
    "objectID": "04_Sampling.html#properties-of-the-constants-k_i-and-c_i",
    "href": "04_Sampling.html#properties-of-the-constants-k_i-and-c_i",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.1 Properties of the constants \\(k_i\\) and \\(c_i\\)",
    "text": "4.1 Properties of the constants \\(k_i\\) and \\(c_i\\)\nIn the previous section, we examined how the least squares estimators are linear combinations of the response variable \\(y\\). Let’s know look at the properties of the coefficients in Equation 3.1 and Equation 3.2. We will not present the proofs in this course but they are not complicated.\nThe coefficients \\(k_{i}\\) have the following properties: \\[\n\\begin{align}\n\\sum k_{i} & =0\n\\end{align}\n\\tag{4.1}\\]\n\\[\n\\begin{align}\n\\sum k_{i}x_i & =1\n\\end{align}\n\\tag{4.2}\\]\n\\[\n\\begin{align}\n\\sum k_{i}^{2} & =\\frac{1}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.3}\\]\nLikewise, the coefficients \\(c_{i}\\) have the following properties: \\[\n\\begin{align}\n\\sum c_{i} & =1\n\\end{align}\n\\tag{4.4}\\]\n\\[\n\\begin{align}\n\\sum c_{i}x_i & =0\n\\end{align}\n\\tag{4.5}\\]\n\\[\n\\begin{align}\n\\sum c_{i}^{2} & =\\frac{1}{n}+\\frac{\\left(\\bar{x}\\right)^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.6}\\]\n\n\n\n\n\n\nReview: The Expected Value of a Linear Combination\n\n\n\nRecall that the expected value of a linear combination of the random variable \\(Y\\) is \\[\nE(aY+b)=aE(Y)+b\n\\] where \\(a\\) and \\(b\\) are constants."
  },
  {
    "objectID": "04_Sampling.html#expected-values-of-b_0-and-b_1",
    "href": "04_Sampling.html#expected-values-of-b_0-and-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.2 Expected Values of \\(b_0\\) and \\(b_1\\)",
    "text": "4.2 Expected Values of \\(b_0\\) and \\(b_1\\)\nBefore finding the expectations, recall \\(E\\left(y_i\\right)=\\beta_{0}+\\beta_{1}x_i\\).\n\n4.2.1 Expected Value of \\(b_1\\)\nThe expected value of \\(b_1\\) is \\[\n\\begin{align*}\nE\\left[b_1\\right] & =E\\left[\\underbrace{\\sum k_{i}y_i}_{(3.1)}\\right]\\\\\n& =\\sum k_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\underbrace{\\sum k_{i}}_{(4.1)}+\\beta_{1}\\underbrace{\\sum k_{i}x_i}_{(4.2)}\\\\\n& =\\beta_{1}\n\\end{align*}\n\\]\n\n\n4.2.2 Expected Value of \\(b_0\\)\nThe expected value of \\(b_0\\) is \\[\n\\begin{align*}\nE\\left[b_0\\right] & =E\\left[\\underbrace{\\sum c_{i}y_i}_{(3.2)}\\right]\\\\\n& =\\sum c_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\underbrace{\\sum c_{i}}_{(4.4)}+\\beta_{1}\\underbrace{\\sum c_{i}x_i}_{(4.5)}\\\\\n& =\\beta_{0}\n\\end{align*}\n\\]\nTherefore, \\(b_0\\) is an unbiased estimator of \\(\\beta_0\\) and \\(b_1\\) is an unbiased estimator of \\(\\beta_1\\).\n\n\n\n\n\n\nReview: Unbiased Estimator\n\n\n\nRecall that an unbiased estimator for some parameter is an estimator that has an expected value equal to that parameter."
  },
  {
    "objectID": "04_Sampling.html#variances-of-b_0-and-b_1",
    "href": "04_Sampling.html#variances-of-b_0-and-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.3 Variances of \\(b_0\\) and \\(b_1\\)",
    "text": "4.3 Variances of \\(b_0\\) and \\(b_1\\)\nTo find the variances, we will use a result from mathematical statistics: Let \\(Y_{1},\\ldots,Y_{n}\\) be uncorrelated random variables and let \\(a_{1},\\ldots,a_{n}\\) be constants. Then \\[\n\\begin{align}\nVar\\left[\\sum a_{i}Y_i\\right] & =\\sum a_{i}^{2}Var\\left[Y_i\\right]\n\\end{align}\n\\tag{4.7}\\]\nRecall that we assume the response variables \\(y_i\\)’s are independent.\nTechnically, we assume the \\(y_i\\)’s are uncorrelated. In general, uncorrelated does not imply independent. However, if the random variables are jointly normally distributed (recall our third assumption of the model), then uncorrelated does imply independent.\nAlso, note that \\[\n\\begin{align*}\n    Var\\left[Y\\right]& = Var\\left[\\beta_0 + \\beta_1 x + \\varepsilon\\right]\\\\\n    & = Var\\left[\\varepsilon\\right]\\\\\n    & = \\sigma^2\n\\end{align*}\n\\]\n\n4.3.1 Variance of \\(b_1\\)\nThe variance of \\(b_1\\) is \\[\n\\begin{align}\nVar\\left[b_1\\right] & =Var\\left[\\underbrace{\\sum k_{i}y_i}_{(3.1)}\\right]\\\\\n& =\\underbrace{\\sum k_{i}^{2}}_{(4.3)}Var\\left[y_i\\right]\\\\\n& =\\frac{\\sigma^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.8}\\]\n\n\n4.3.2 Variance of \\(b_0\\)\nThe variance of \\(b_0\\) is \\[\n\\begin{align}\nVar\\left[b_0\\right] & =Var\\left[\\underbrace{\\sum c_{i}y_i}_{(3.2)}\\right]\\nonumber\\\\\n& =\\underbrace{\\sum c_{i}^{2}}_{(4.6)}Var\\left[y_i\\right]\\\\\n& =\\sigma^{2}\\left[\\frac{1}{n}+\\frac{\\left(\\bar{x}\\right)^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\\right]\n\\end{align}\n\\tag{4.9}\\]"
  },
  {
    "objectID": "04_Sampling.html#best-linear-unbiased-estimators-blues",
    "href": "04_Sampling.html#best-linear-unbiased-estimators-blues",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.4 Best Linear Unbiased Estimators (BLUEs)",
    "text": "4.4 Best Linear Unbiased Estimators (BLUEs)\nWe see from Equation 3.1 and Equation 3.2 that \\(b_0\\) and \\(b_1\\) are linear estimators.\nAny estimator for \\(\\beta_{1}\\), which we will denote as \\(\\hat{\\beta}_{0}\\), that takes the form \\[\n\\begin{align*}\n\\hat{\\beta}_{1} & =\\sum a_{i}y_i\n\\end{align*}\n\\] where \\(a_{i}\\) is some constant, is called a linear estimator.\nOf all linear estimators for \\(\\beta_0\\) and \\(\\beta_1\\) that are unbiased, the least squares estimators, \\(b_0\\) and \\(b_1\\), have the smallest variance.\nThis is summarized in the following well known theorem:\n\nTheorem 4.1 (Gauss Markov Theorem) For the simple linear regression model, the least squares estimators \\(b_0\\) and \\(b_1\\) are unbiased and have minimum variance among all unbiased linear estimators.\n\nAn estimator that is linear, unbiased, and has the smallest variance of all unbiased linear estimators is called the best linear unbiased estimator (BLUE).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nProof of the Gauss Markov Theorem:\nFor all linear estimators that are unbiased, we must have \\[\n\\begin{align*}\nE\\left[\\hat{\\beta}_{1}\\right] & =E\\left[\\sum a_{i}y_i\\right]\\\\\n& =\\sum a_{i}E\\left[y_i\\right]\\\\\n& =\\beta_{1}\n\\end{align*}\n\\] Since \\(E\\left[y_i\\right]=\\beta_{0}+\\beta_{1}x_i\\), then we must have \\[\n\\begin{align*}\nE\\left[\\hat{\\beta}_{1}\\right] & =\\sum a_{i}\\left(\\beta_{0}+\\beta_{1}x_i\\right)\\\\\n& =\\beta_{0}\\sum a_{i}+\\beta_{1}\\sum a_{i}x_i\\\\\n& =\\beta_{1}\n\\end{align*}\n\\] Therefore, \\[\n\\begin{align*}\n\\sum a_{i} & =0\\\\\n\\sum a_{i}x_i & =1\n\\end{align*}\n\\] We now examine the variance of \\(\\hat{\\beta}_{1}\\): \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =\\sum a_{i}^{2}Var\\left[y_i\\right]\\\\\n& =\\sigma^{2}\\sum a_{i}^{2}\n\\end{align*}\n\\] Let’s now define \\(a_{i}=k_{i}+d_{i}\\) where \\(k_{i}\\) is defined in Equation 3.1. and \\(d_{i}\\) is some arbitrary constant.\nWe will show that adding a constant (whether negative or positive) to \\(k_i\\) cannot make the variance smaller. Thus, the smallest variance of the linear estimator \\(\\hat{\\beta}_1\\) is when \\(a_i=k_i\\).\nThe variance of \\(\\hat{\\beta}_{1}\\) can now be written as \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =\\sigma^{2}\\sum a_{i}^{2}\\\\\n& =\\sigma^{2}\\sum\\left(k_{i}+d_{i}\\right)^{2}\\\\\n& =\\sigma^{2}\\sum\\left(k_{i}^{2}+2k_{i}d_{i}+d_{i}^{2}\\right)\\\\\n& =Var\\left[b_1\\right]+2\\sigma^{2}\\sum k_{i}d_{i}+\\sigma^{2}\\sum d_{i}^{2}\n\\end{align*}\n\\] Examining the second term and using the expression of \\(k_{i}\\) in Equation 3.1, we see that \\[\n\\begin{align*}\n\\sum k_{i}d_{i} & =\\sum k_{i}\\left(a_{i}-k_{i}\\right)\\\\\n& =\\sum a_{i}k_{i}-\\underbrace{\\sum k_{i}^{2}}_{(4.3)}\\\\\n& =\\sum a_{i}\\frac{x_i-\\bar{x}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{\\sum a_{i}x_i-\\bar{x}\\sum a_{i}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =\\frac{1-\\bar{x}\\left(0\\right)}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}-\\frac{1}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\\\\n& =0\n\\end{align*}\n\\]\nWe now have the variance of \\(\\hat{\\beta}_{1}\\) as \\[\n\\begin{align*}\nVar\\left[\\hat{\\beta}_{1}\\right] & =Var\\left[b_1\\right]+\\sigma^{2}\\sum d_{i}^{2}\n\\end{align*}\n\\] This variance is minimized when \\(\\sum d_{i}^{2}=0\\) which only happens when \\(d_{i}=0\\).\nThus, the unbiased linear estimator with the smallest variance is when \\(a_{i}=k_{i}\\). That is, the least squares estimator \\(b_1\\) in Equation 3.1 has the smallest variance of all unbiased linear estimators of \\(\\beta_{1}\\).\nA similar argument can be used to show that \\(b_0\\) has the smallest variance of all unbiased linear estimators of \\(\\beta_{0}\\)."
  },
  {
    "objectID": "04_Sampling.html#sampling-distribution-for-b_1",
    "href": "04_Sampling.html#sampling-distribution-for-b_1",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.5 Sampling Distribution for \\(b_1\\)",
    "text": "4.5 Sampling Distribution for \\(b_1\\)\nNow that we see that the least squares estimator \\(b_1\\) is the BLUE for \\(\\beta_{1}\\), we will now examine the sampling distribution for \\(b_1\\).\nWe previously discussed that the mean of the sampling distribution of \\(b_1\\) is \\[\nE[b_1]=\\beta_1\n\\] with a variance of \\[\n\\begin{align}\nVar\\left[b_1\\right]\n& =\\frac{\\sigma^{2}}{\\sum \\left(x_i-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{4.10}\\]\nNote that in our model with our four assumptions, \\(y\\) is normally distributed. That is, \\[\n\\begin{align}\n    y\\sim N\\left(\\beta_0+\\beta_1 x, \\sigma^2\\right)\n\\end{align}\n\\tag{4.11}\\]\nTo learn about the sampling distributions of the least squares estimators, we will use the following theorems from mathematical statistics:\n\nTheorem 4.2 (Sum of Independent Normal Random Variables) If \\[\nY_i\\sim N\\left(\\mu_i,\\sigma_i^2\\right)\n\\] are independent, then the linear combination \\(\\sum_i a_iY_i\\) is also normally distributed where \\(a_i\\) are constants. In particular \\[\n\\sum_i a_iY_i \\sim N\\left(\\sum_i a_i\\mu_i, \\sum_i a_i^2\\sigma_i^2\\right)\n\\]\n\n\nTheorem 4.3 (Adding a Constant to a Normal Random Variable) If \\[\nY\\sim N\\left(\\mu,\\sigma^2\\right)\n\\] then for any real constant \\(c\\), \\[\nY+c\\sim N\\left(\\mu+c,\\sigma^2\\right)\n\\]\n\nSince \\(Y\\) is normally distributed by Equation 4.11, then we can apply Theorem 4.2 which implies that \\(b_1\\) is normally distributed. That is, \\[\n\\begin{align}\nb_1 & \\sim N\\left(\\beta_{1},\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}\\right)\n\\end{align}\n\\tag{4.12}\\]\n\n4.5.1 Standardized Score\nSince \\(b_1\\) is normally distributed, we can standardize it so that the resulting statistic will have a standard normal distribution.\nTherefore, we have \\[\n\\begin{align}\nz=\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}}} & \\sim N\\left(0,1\\right)\n\\end{align}\n\\tag{4.13}\\]\n\n\n4.5.2 Studentized Score\nIn practice, the standardized score \\(z\\) is not useful since we do not know the value of \\(\\sigma^{2}\\). We can estimate \\(\\sigma^{2}\\) with the statistic \\[\ns^2 = \\frac{SSE}{n-2}\n\\]\nUsing this estimate for \\(\\sigma^2\\) leads us to a \\(t\\)-score: \\[\n\\begin{align}\nt=\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\bar{x}\\right)^{2}}}} & \\sim t\\left(n-2\\right)\n\\end{align}\n\\tag{4.14}\\]\nWe call this \\(t\\) statistic, the studentized score.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nIt is important to note the following theorem from math stats presented here without proof:\n\nTheorem 4.4 (Distribution of the sample variance of the residuals) For the sample variance of the residuals \\(s^{2}\\), the quantity \\[\\begin{align*}\n\\frac{\\left(n-2\\right)s^{2}}{\\sigma^{2}} & =\\frac{SSE}{\\sigma^{2}}\n\\end{align*}\\] is distributed as a chi-square distribution with \\(n-2\\) degrees of freedom. That is, \\[\\begin{align*}\n\\frac{SSE}{\\sigma^{2}} & \\sim\\chi^{2}\\left(n-2\\right)\n\\end{align*}\\]\n\nWe will use another important theorem form math stats (again presented without proof):\n\nTheorem 4.5 (Ratio of independent standard normal and chi-square statistics) If \\(Z\\sim N\\left(0,1\\right)\\) and \\(W\\sim\\chi^{2}\\left(\\nu\\right)\\), and \\(Z\\) and \\(W\\) are independent, then the statistic \\[\\begin{align*}\n\\frac{Z}{\\sqrt{\\frac{W}{\\nu}}}\n\\end{align*}\\] is distributed as a Student’s \\(t\\) distribution with \\(\\nu\\) degrees of freedom.\n\nWe will take the standardized score in Equation 4.13 and divide by \\[\\begin{align*}\n\\sqrt{\\frac{\\frac{\\left(n-2\\right)s^{2}}{\\sigma^{2}}}{\\left(n-2\\right)}} & =\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}\n\\end{align*}\\] to give us \\[\\begin{align*}\nt & =\\frac{\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}}{\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}}\\\\\n& =\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{\\sigma^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}\\sqrt{\\frac{s^{2}}{\\sigma^{2}}}}\\\\\n& =\\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}\n\\end{align*}\\] which will have a Student’s \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "04_Sampling.html#assessing-the-utility-of-the-model-making-inferences-about-the-slope",
    "href": "04_Sampling.html#assessing-the-utility-of-the-model-making-inferences-about-the-slope",
    "title": "4  Sampling Distribution of the Least Squares Estimators and Testing the Slope",
    "section": "4.6 Assessing the Utility of the Model: Making Inferences About the Slope",
    "text": "4.6 Assessing the Utility of the Model: Making Inferences About the Slope\nSuppose that the independent variable \\(x\\) is completely unrelated to the dependent variable \\(y\\).\nWhat could be said about the values of \\(\\beta_0\\) and \\(\\beta_1\\) in the hypothesized probabilistic model \\[\\begin{align*}\n    y = \\beta_0 +\\beta_1 x + \\varepsilon\n\\end{align*}\\] if \\(x\\) contributes no information for the prediction of \\(y\\)?\nThe implication is that the mean of \\(y\\), does not change as \\(x\\) changes. In other words, the line would just be a horizontal line.\nIf \\(E(y)\\) does not change as \\(x\\) increases, then using \\(x\\) to predict \\(y\\) in the linear model is not useful.\nRegardless of the value of \\(x\\), you always predict the same value of \\(y\\). In the straight-line model, this means that the true slope, \\(\\beta_1\\), is equal to 0.\nTherefore, to test the null hypothesis that \\(x\\) contributes no information for the prediction of \\(y\\) against the alternative hypothesis that these variables are linearly related with a slope differing from 0, we test \\[\\begin{align*}\n    H_0:\\beta_1 = 0\\\\\n    H_a:\\beta_1\\ne 0\n\\end{align*}\\]\nIf the data support the alternative hypothesis, we conclude that \\(x\\) does contribute information for the prediction of \\(y\\) using the straight-line model (although the true relationship between \\(E(y)\\) and \\(x\\) could be more complex than a straight line). Thus, to some extent, this is a test of the utility of the hypothesized model.\nThe appropriate test statistic is the studentized score given above: \\[\n\\begin{align}\n    t &= \\frac{b_1-\\beta_{1}}{\\sqrt{\\frac{s^{2}}{\\sum\\left(x_i-\\overline{X}\\right)^{2}}}}\\\\\n    &=\\frac{b_1}{\\sqrt{\\frac{s^{2}}{SS_{xx}}}}\n\\end{align}\n\\tag{4.15}\\]\nAnother way to make inferences about the slope \\(\\beta_1\\) is to estimate it using a confidence interval \\[\n\\begin{align}\n    b_1 \\pm \\left(t_{\\alpha/2}\\right)s_{b_1}\n\\end{align}\n\\tag{4.16}\\] where \\[\\begin{align*}\n    s_{b_1} = \\frac{s}{\\sqrt{SS_{xx}}}\n\\end{align*}\\]\nWe can obtain the p-value for the hypothesis test by using the summary function with an lm object. For the previous example consisting of the mtcars data.\n\nExample 4.1 (Example 3.1 - revisited)  \n\nlibrary(tidyverse)\n\nfit = lm(mpg~wt, data = mtcars)\n\nWe find the least squares estimates as\n\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nFrom the output, we see the p-value is \\(1.29\\times 10^{-10}\\). So we have sufficient evidence to conclude that the true population slope is different than zero.\nTo find the confidence interval, we can use the confint function with the lm object.\n\nconfint(fit, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept) 33.450500 41.119753\nwt          -6.486308 -4.202635\n\n\nWe 95% confident that the true population slope is in the interval \\((-6.486, -4.203)\\)"
  }
]