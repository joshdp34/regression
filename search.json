[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 3386 Regression Analysis",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 3386 - Regression Analysis.\nPrerequisites: MTH 2311 - Linear Algebra, MTH 2321 - Calculus III, and STA 3381 - Probability and Statistics\n\nCourse Description:\nA development of regression techniques including simple linear regression, multiple regression, logistic regression and Poisson regression with emphasis on model assumptions, parameter estimation, variable selection and diagnostics."
  },
  {
    "objectID": "01_Intro.html#the-probabilistic-model",
    "href": "01_Intro.html#the-probabilistic-model",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.1 The Probabilistic Model",
    "text": "1.1 The Probabilistic Model\nMost students who take an intro stats course are familiar with the idea of a random variable. Students are usually introduced to random variables using the notation \\(X\\).\nIn the technical sense, capital \\(X\\) should denote the random variable (that is, the function itself), while lowercase \\(x\\) denotes the values of that random variable. We will be loose on that convention, so you will see the lowercase used almost extensively even when discussing the random variable itself.\n\n\n\n\n\n\nReview: Random Variable\n\n\n\nA random variable is a function that assigns a numeric value to the outcomes in the sample space.\n\n\nIn regression, the random variable of interest is usually denoted as \\(y\\).\nWe want to predict or model (explain) this variable. Thus, we call this the response (or dependent) variable.\nIf we have measurements of this random variable, then we can express each value \\(y\\) as the mean value of \\(y\\) plus some random error.\nThat is, we can model the variable as \\[\n    y = E(y) + \\varepsilon\n\\tag{1.1}\\]\nwhere \\[\\begin{align*}\n    y = &\\text{ dependent variable}\\\\\n    E(y) =& \\text{ mean (or expected) value of } y\\\\\n    \\varepsilon =& \\text{ random error}\n\\end{align*}\\]\nThis model is referred to as a probabilistic model for \\(y\\). The term “probabilistic” is used because, under certain assumptions, we can make probability-based statements about the extent of the difference between \\(y\\) and \\(E(y)\\).\nFor example, we might assert that the error term, \\[\n    \\varepsilon = y - E(y)\n\\] follows a normal distribution.\nIn practice, we will use sample data to estimate the parameters of the probabilistic model—specifically, the mean \\(E(y)\\) and the random error \\(\\varepsilon\\).\nWe will later discuss a common assumption in regression: that the mean error is zero.\nIn other words, \\[\n    E(\\varepsilon) = 0\n\\]\nGiven this assumption, our best estimate of \\(\\varepsilon\\) is zero. Therefore, we only need to estimate \\(E(y)\\).\nThe simplest method of estimating \\(E(y)\\) is to use the sample mean of \\(y\\) which we will denote as \\[\\begin{align*}\n    \\bar y= \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{align*}\\]\nIf we desired to predict a value of \\(y\\), then our best prediction would be just the sample mean: \\[\\begin{align*}\n    \\hat y = \\bar y\n\\end{align*}\\] where \\(\\hat y\\) denotes a predicted value of \\(y\\).\nThis would be the case with univariate data (we only have one variable in our data: \\(y\\)).\nUnfortunately, this simple model does not take into consideration a number of variables, called independent variables, that may help predict the response variable.\nIndependent variables are also called predictor or explanatory variables.\nThe process of identifying the mathematical model that describes the relationship between \\(y\\) and a set of independent variables, and that best fits the data, is known as regression analysis."
  },
  {
    "objectID": "01_Intro.html#sec-regoverview",
    "href": "01_Intro.html#sec-regoverview",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.2 Overview of Regression Analysis",
    "text": "1.2 Overview of Regression Analysis\nWe will denote the independent variables as \\[\\begin{align*}\n    x_1, x_2, \\ldots, x_k\n\\end{align*}\\] where \\(k\\) is the number of independent variables.\nThe goal of regression analysis is to create a prediction equation that accurately relates \\(y\\) to independent variables, allowing us to predict \\(y\\) for given values of \\(x_1, x_2, \\ldots, x_k\\) with minimal prediction error.\nWhen predicting \\(y\\), we also need a measure of the reliability of our prediction, indicating how large the prediction error might be.\nThese elements form the core of regression analysis.\nBeyond predicting \\(y\\), a regression model can also estimate the mean value of \\(y\\) for specific values of \\(x_1, x_2, \\ldots, x_k\\) and explore the relationship between \\(y\\) and one or more independent variables.\nThe process of regression analysis typically involves six key steps:\n\nHypothesize the form of the model for \\(E(y)\\).\nCollect sample data.\nEstimate the model’s unknown parameters using the sample data.\nDefine the probability distribution of the random error term, estimate any unknown parameters, and validate the assumptions made about this distribution.\nStatistically assess the model’s usefulness.\nIf the model is effective, use it for prediction, estimation, and other purposes."
  },
  {
    "objectID": "01_Intro.html#collecting-the-data-for-regression",
    "href": "01_Intro.html#collecting-the-data-for-regression",
    "title": "1  Introduction to Regression Analysis",
    "section": "1.3 Collecting the Data for Regression",
    "text": "1.3 Collecting the Data for Regression\nThe first step listed above will be discussed later.\nOnce you’ve proposed a model for \\(E(y)\\), the next step is to gather sample data to estimate the model.\nThis means collecting data on both the response variable \\(y\\) and the independent variables \\(x_1, x_2, \\ldots, x_k\\) for each observation in your sample. In regression analysis, the sample includes data on multiple variables: \\[\ny, x_1, x_2, \\ldots, x_k\n\\] This is known as multivariate data.\nRegression data can be either observational or experimental:\nFor observational data no control is exerted over the independent variables (\\(x\\)’s). For example, recording people’s ages and their corresponding blood pressure levels without influencing either.\nFor experimental data the independent variables are controlled or manipulated. For instance, setting different fertilizer amounts for crops to observe the impact on growth.\nSuppose you want to model a student’s annual GPA (\\(y\\)). One approach is to randomly select a sample of \\(n=100\\) students and record their GPA along with the values of each predictor variable.\nData for the first three students in the sample are shown in Table 1.1.\n\n\nTable 1.1: Values of the response variable and predictor variables for the first three students.\n\n\n\nStudent 1\nStudent 2\nStudent 3\n\n\n\n\nAnnual GPA \\(y\\)\n3.8\n2.7\n3.5\n\n\nStudy Hours per Week, \\(x_1\\)\n15\n5\n10\n\n\nClass Attendance, \\(x_2\\) (days)\n30\n20\n25\n\n\nExtracurriculars, \\(x_3\\)\n2\n1\n3\n\n\nAge, \\(x_4\\) (years)\n21\n19\n22\n\n\nEmployed, \\(x_5\\) (1 if yes, 0 if no)\n0\n1\n0\n\n\nLives On Campus, \\(x_6\\) (1 if yes, 0 if no)\n1\n0\n1\n\n\n\n\nIn this example, the \\(x\\) values, like study hours, class attendance, and extracurricular activities, are not predetermined before observing GPA \\(y\\); thus, the \\(x\\) values are uncontrolled. Therefore, the sample data are observational.\n\nDetermining Sample Size for Regression with Observational Data\nWhen applying regression to observational data, the required sample size for estimating the mean \\(E(y)\\) depends on three key factors:\n\nEstimated population standard deviation\nConfidence level\nDesired margin of error (half-width of the confidence interval)\n\nHowever, unlike the univariate case, \\(E(y)\\) is modeled as a function of multiple independent variables, which adds complexity. The sample size must be large enough to estimate and test all parameters in the model.\nTo ensure a sufficient sample size, a common guideline is to select a sample size \\(n\\) that is at least 10 times the number of parameters in the model.\nFor instance, if a university registrar’s office uses the following model for the annual GPA \\(y\\) of a current student:\n\\[E(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_6 x_6\\]\nwhere \\(x_1, x_2, \\dots, x_6\\) are defined in Table 1.1, the model includes six \\(\\beta\\) parameters (excluding \\(\\beta_0\\)). Therefore, they should include at least:\n\\[ 10 \\times 6 = 60 \\]\nstudents in the sample.\n\n\nExperimental Data\nThe second type of data in regression, experimental data, is generated through designed experiments where the independent variables are set in advance (i.e., controlled) before observing the value of \\(y\\).\nFor instance, consider a scenario where a researcher wants to study the effect of two independent variables—say, fertilizer amount \\(x_1\\) and irrigation level \\(x_2\\)—on the growth rate \\(y\\) of plants. The researcher could choose three levels of fertilizer (10g, 20g, and 30g) and three levels of irrigation (1L, 2L, and 3L) and measure the growth rate in one plant for each of the \\(3\\times 3=9\\) fertilizer–irrigation combinations (see Table 1.2 below).\n\n\nTable 1.2: Values of the response variable and two independent variables for the growth rate of plants.\n\n\nFertilizer, \\(x_1\\)\nIrrigation, \\(x_2\\)\nGrowth Rate, \\(y\\)\n\n\n\n\n10g\n1L\n5.2\n\n\n10g\n2L\n6.1\n\n\n10g\n3L\n5.8\n\n\n20g\n1L\n7.0\n\n\n20g\n2L\n7.5\n\n\n20g\n3L\n7.3\n\n\n30g\n1L\n8.4\n\n\n30g\n2L\n8.7\n\n\n30g\n3L\n8.1\n\n\n\n\nIn this experiment, the settings of the independent variables are controlled, in contrast to the uncontrolled nature of observational data, like in the real estate sales example.\nIn many studies, it is often not possible to control the values of the \\(x\\)’s, so most data collected for regression are observational.\nSo, why do we differentiate between these two types of data? We will learn that inferences from regression studies based on observational data have more limitations than those based on experimental data. Specifically, establishing a cause-and-effect relationship between variables is much more challenging with observational data than with experimental data."
  },
  {
    "objectID": "02_Fitting.html#the-straight-line-probabilistic-model",
    "href": "02_Fitting.html#the-straight-line-probabilistic-model",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.1 The Straight-Line Probabilistic Model",
    "text": "2.1 The Straight-Line Probabilistic Model\nWhen studying environmental science, consider modeling the monthly carbon dioxide (CO₂) emissions \\(y\\) of a city as a function of its monthly industrial activity \\(x\\).\nThe first question to ask is: Do you believe an exact (deterministic) relationship exists between these two variables?\nIn other words, can we predict the exact value of CO₂ emissions if industrial activity is known?\nIt’s unlikely. CO₂ emissions depend on various factors beyond industrial activity, such as weather conditions, regulatory policies, and transportation levels. Even with multiple variables in the model, it’s improbable that we could predict monthly emissions precisely.\nThere will almost certainly be some variation in emissions due to random phenomena that cannot be fully explained or modeled.\nTherefore, we should propose a probabilistic model for CO₂ emissions that accounts for this random variation:\n\\[\ny = E(y) + \\varepsilon\n\\]\nThe random error component,\\(\\varepsilon\\), captures all unexplained variations in emissions caused by omitted variables or unpredictable random factors.\nThe random error plays a key role in hypothesis testing, determining confidence intervals for the model’s deterministic portion, and estimating the prediction error when using the model to predict future values of \\(y\\).\nLet’s start with the simplest probabilistic model—a first-order linear model that graphs as a straight line."
  },
  {
    "objectID": "02_Fitting.html#a-first-order-linear-model",
    "href": "02_Fitting.html#a-first-order-linear-model",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.2 A First-Order Linear Model",
    "text": "2.2 A First-Order Linear Model\nThe first-order linear model is expessed as \\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nwhere:\n\n\\(y\\) is the dependent variable (also known as the response variable)\n\\(x\\) is the independent variable (used as a predictor of \\(y\\))\n\\(E(y) = \\beta_0 + \\beta_1 x\\) is the deterministic component\n\\(\\varepsilon\\) is the random error component\n\\(\\beta_0\\) is the y-intercept of the line (the point where the line intersects the y-axis)\n\\(\\beta_1\\) is the slope of the line (the change in the mean of \\(y\\) for every 1-unit increase in \\(x\\))\n\nWe use Greek symbols \\(\\beta_0\\) and \\(\\beta_1\\) to denote the y-intercept and slope of the line. These are population parameters with values that would only be known if we had access to the entire population of \\((x, y)\\) measurements.\n\n\n\n\n\n\nReview: Greek Letters in Notation\n\n\n\nUsually, in Statistics, lower-case Greek letters are used to denote population parameters. In our model above, we have an exception. The Greek letter \\(\\varepsilon\\) is not a parameter, but a random variable (parameters are not random variables in frequentist statistics).\n\n\nAs discussed in Section 1.2, regression can be viewed as a six-step process. For now, we’ll focus on steps 2-6, using the simple linear regression model. We’ll explore more complex models later."
  },
  {
    "objectID": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "href": "02_Fitting.html#fitting-the-model-the-method-of-least-squares",
    "title": "2  Fitting the Simple Linear Regression Model",
    "section": "2.3 Fitting the Model: The Method of Least Squares",
    "text": "2.3 Fitting the Model: The Method of Least Squares\nSuppose we have the data shown in Table 2.1 below and plotted in the scatterplot in Figure 2.1.\n\n\nTable 2.1: Data for Scatterplot\n\n\nx\ny\n\n\n\n\n1\n2\n\n\n2\n1.4\n\n\n2.75\n1.6\n\n\n4\n1.25\n\n\n6\n1\n\n\n7\n0.5\n\n\n8\n0.5\n\n\n10\n0.4\n\n\n\n\n\nlibrary(tidyverse)\nx = c(1, 2, 2.75, 4, 6, 7, 8, 10)\ny = c(2, 1.4, 1.6, 1.25, 1, 0.5, 0.5, 0.4)\ndat = tibble(x, y)\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point()\n\n\n\n\nFigure 2.1: Scatterplot of the data in Table 2.1\n\n\n\n\nWe hypothesize that a straight-line model relates y to x, as follows:\n\\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nHow can we use the data from the eight observations in Table 2.1 to estimate the unknown y-intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\))?\nWe can start by trying some lines and see how well they fit the data. But how do we measure how well a line fits the data?\nA quantitative method to evaluate how well a straight line fits a set of data is by measuring the deviations of the data points from the line.\n\n\n\n\n\n\nReview: Deviations of Response Variable\n\n\n\n\\(y\\) is the variable of interest, so we are focused on the differences between observed \\(y\\) and the predicted value of \\(y\\)\n\n\nWe calculate the magnitude of the deviations (the differences between observed and predicted values of \\(y\\)).\nThese deviations, or prediction errors, represent the vertical distances between observed and predicted values of \\(y\\).\nSuppose we try to fit the line \\[\n    \\hat{y} =2-.2x\n\\tag{2.1}\\]\nThis line can be seen in Figure 2.2.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 2, slope = -0.2, color = \"red\")\n\n\n\n\nFigure 2.2: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.1\n\n\n\n\nThe observed and predicted values of \\(y\\), their differences, and their squared differences are shown in the table below.\n\n\nTable 2.2: Deviations and squared deviations of the line in Equation 2.1 and the data in Table 2.1.\n\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\((y - \\hat{y})\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n\n1\n2\n1.8\n0.2\n0.004\n\n\n2\n1.4\n1.6\n-0.2\n0.004\n\n\n2.75\n1.6\n1.45\n0.15\n0.0225\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.8\n0.2\n0.04\n\n\n7\n0.5\n0.6\n-0.1\n0.01\n\n\n8\n0.5\n0.4\n0.1\n0.01\n\n\n10\n0.4\n0\n0.4\n0.16\n\n\n\n\nNote that the sum of the errors (SE) is 0.8, and the sum of squares of the errors (SSE), which emphasizes larger deviations from the line, is 0.325.\nWe can try another line to see if we do better at predicting \\(y\\) (that is, have smaller SSE).\nLet’s try the line \\[\n    \\hat{y} =1.8-.15x\n\\tag{2.2}\\]\nThis line can be seen in Figure 2.3.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_point() +\n  geom_abline(intercept = 1.8, slope = -0.15, color = \"red\")\n\n\n\n\nFigure 2.3: Scatterplot of data in Table 2.1 with the red line representing the line in Equation 2.2\n\n\n\n\nThe fit results are shown in Table 2.3.\n\n\nTable 2.3: Deviations and squared deviations of the line in Equation 2.2 and the data in Table 2.1.\n\n\n\\(x\\)\n\\(y\\)\n\\(\\hat{y}\\)\n\\(y - \\hat{y}\\)\n\\((y - \\hat{y})^2\\)\n\n\n\n\n1\n2\n1.65\n0.35\n0.1225\n\n\n2\n1.4\n1.5\n-0.1\n0.01\n\n\n2.75\n1.6\n1.3875\n0.2125\n0.04515625\n\n\n4\n1.25\n1.2\n0.05\n0.0025\n\n\n6\n1\n0.9\n0.1\n0.01\n\n\n7\n0.5\n0.75\n-0.25\n0.0625\n\n\n8\n0.5\n0.6\n-0.1\n0.01\n\n\n10\n0.4\n0.3\n0.1\n0.01\n\n\n\n\nThe SSE for this line is 0.2727, which is lower than the SSE for the previous line, indicating a better fit.\nWhile we could try additional lines to achieve a lower SSE, there are infinitely many possibilities since \\(\\beta_0\\) and \\(\\beta_1\\) can take any real value.\nUsing Calculus, we can attempt to minimize the SSE for the generic line \\[\\begin{align*}\n    \\hat{y} = b_0 +b_1 x\n\\end{align*}\\]\nWe will denote the sum of the squared distances with \\(Q\\): \\[\nQ=\\sum \\left(y_i-\\hat{y}_i\\right)^2\n\\tag{2.3}\\]\nWe determine the “best” line as the one that minimizes \\(Q\\).\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo minimize \\(Q\\), we differentiate it with respect to \\(b_{0}\\) and \\(b_{1}\\): \\[\\begin{align*}\n\\frac{\\partial Q}{\\partial b_{0}} & =-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\\\\\n\\frac{\\partial Q}{\\partial b_{1}} & =-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)\n\\end{align*}\\]\nSetting these partial derivatives equal to 0, we have \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\\\\\n-2\\sum x_{i}\\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right) & =0\n\\end{align*}\\] Looking at the first equation, we can simplify as \\[\\begin{align*}\n-2\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum \\left(y_{i}-\\left(b_{0}+b_{1}x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum y_{i}-\\sum b_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}-nb_{0}-b_{1}\\sum x_{i}=0\\\\\n& \\Longrightarrow\\sum y_{i}=nb_{0}+b_{1}\\sum x_{i}\n\\end{align*}\\]\nSimplifying the second equation gives us \\[\\begin{align*}\n-2\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0 & \\Longrightarrow\\sum x_{i}\\left(y_{i}-\\left(b_0+b_1x_{i}\\right)\\right)=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}-b_0\\sum x_{i}-b_1\\sum x_{i}^{2}=0\\\\\n& \\Longrightarrow\\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align*}\\]\n\n\n\nThe two equations \\[\n\\begin{align}\n\\sum y_{i} & =nb_0+b_1\\sum x_{i}\\nonumber\\\\\n\\sum x_{i}y_{i} & =b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\n\\end{align}\n\\tag{2.4}\\]\nare called the normal equations.\nWe now have two equations and two unknowns (\\(b_0\\) and \\(b_1\\)). We can solve the equations simultaneously. We solve the first equation for \\(b_0\\) which gives us \\[\\begin{align*}\nb_0 & =\\frac{1}{n}\\left(\\sum y_{i}-b_1\\sum x_{i}\\right)\\\\\n& =\\bar{y}-b_1\\bar{x}.\n\\end{align*}\\]\nWe now substitute this into the second equation in Equation 2.4. Solving this for \\(b_1\\) gives us \\[\\begin{align*}\n& \\sum x_{i}y_{i}=b_0\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n& \\quad\\Longrightarrow\\sum x_{i}y_{i}=\\left(\\bar{y}-b_1\\bar{x}\\right)\\sum x_{i}+b_1\\sum x_{i}^{2}\\\\\n&\\quad\\Longrightarrow b_1=\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}.\n\\end{align*}\\]\nThe equations \\[\n\\begin{align}\nb_0 & =\\bar{y}-b_1\\bar{x}\\\\\nb_1 & =\\frac{\\sum \\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum \\left(x_{i}-\\bar{x}\\right)^{2}}\n\\end{align}\n\\tag{2.5}\\] are called the least squares estimators.\n\n\n\n\n\n\nFor those who want to see the math:\n\n\n\n\n\nTo show these estimators are the minimum, we take the second partial derivatives of \\(Q\\): \\[\\begin{align*}\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{0}\\right)^{2}} & =2n\\\\\n\\frac{\\partial^{2}Q}{\\partial\\left(b_{1}\\right)^{2}} & =2\\sum x_{i}^{2}\n\\end{align*}\\] Since these second partial derivatives are both positives, then we know the least squares estimators are the minimum.\n\n\n\nThe least squares estimators in Equation 2.5 can be expressed in simpler terms if we let \\[\\begin{align*}\nSS_{xx} &= \\sum \\left(x_i-\\bar x\\right)^2 \\\\\nSS_{xy} &= \\sum \\left(x_i-\\bar x\\right)\\left(y_i - \\bar y\\right)\n\\end{align*}\\]\nThe least squares estimates become \\[\\begin{align}\n{b_1=\\frac{SS_{xy}}{SS_{xx}}}\\\\\n{b_0=\\bar{y}-b_1\\bar{x}}\n\\end{align}\\]\nTo recap: The straight line model for the response \\(y\\) in terms of \\(x\\) is \\[\\begin{align*}\n{y = \\beta_0 + \\beta_1 x + \\varepsilon}\n\\end{align*}\\]\nThe line of means is \\[\\begin{align*}\n{E(y) = \\beta_0 + \\beta_1 x }\n\\end{align*}\\]\nThe fitted line (also called the least squares line) is \\[\\begin{align*}\n{\\hat{y} = b_0 + b_1 x }\n\\end{align*}\\]\nFor a given data point, \\((x_i, y_i)\\), the observed value of \\(y\\) is denoted as \\(y_i\\) and the predicted value of \\(y\\) is obtained by substituting \\(x_i\\) into the prediction equation: \\[\\begin{align*}\n{\\hat{y}_i = b_0 + b_1 x_i }\n\\end{align*}\\]\nThe deviation of the \\(i\\)th value of \\(y\\) from its predicted value, called the \\(i\\)th residual, is \\[\\begin{align*}\n{ \\left(y_i-\\hat{y}_i\\right) }\n\\end{align*}\\] Thus, SSE is just the sum of the squared residuals."
  }
]