<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STA 3386 Regression Analysis - 23&nbsp; Introduction to Logistic Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./22_Validation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./23_Logistic.html"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Introduction to Logistic Regression</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"></a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Regression Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_Fitting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fitting the Simple Linear Regression Model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_Properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Properties of the Least Squares Estimators and Model Assumptions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_Sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sampling Distribution of the Least Squares Estimators and Testing the Slope</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_Correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Correlation Coefficient and the Coefficient of Determination</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_Using.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Using the Simple Linear Model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_Checking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Checking the Linearity and Constant Variance Assumptions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_Checking2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Checking the Normality and Independence Assumptions and Outliers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_Tidymodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Simple Linear Regression with Tidymodels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_Intro_Multiple.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">An Intro to Multiple Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_Regression_Matrix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">The Regression Model in Matrix Terms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Assumptions and the ANOVA F-test</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_Inferences.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Model Inferences and Second-Order Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_Multicollinearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Multicollinearity and Principal Component Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15_Indicator.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Indicator Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_Linearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">The Linearity Assumption</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17_Criteria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Comparison Criteria</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18_Stepwise.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Stepwise and Best Subsets Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19_Ridge.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Ridge Regression and the LASSO</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20_Outliers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Outliers and Influential Observations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21_Residual.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Residual Analysis and Remedial Measures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22_Validation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Model Validation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23_Logistic.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Introduction to Logistic Regression</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#binary-response-variable" id="toc-binary-response-variable" class="nav-link active" data-scroll-target="#binary-response-variable"><span class="header-section-number">23.1</span> Binary Response Variable</a>
  <ul class="collapse">
  <li><a href="#response-function-meaning" id="toc-response-function-meaning" class="nav-link" data-scroll-target="#response-function-meaning"><span class="header-section-number">23.1.1</span> Response Function Meaning</a></li>
  <li><a href="#problems-when-response-is-binary" id="toc-problems-when-response-is-binary" class="nav-link" data-scroll-target="#problems-when-response-is-binary"><span class="header-section-number">23.1.2</span> Problems when response is binary</a></li>
  </ul></li>
  <li><a href="#sigmoidal-response-functions" id="toc-sigmoidal-response-functions" class="nav-link" data-scroll-target="#sigmoidal-response-functions"><span class="header-section-number">23.2</span> Sigmoidal Response Functions</a>
  <ul class="collapse">
  <li><a href="#probit-mean-response-function" id="toc-probit-mean-response-function" class="nav-link" data-scroll-target="#probit-mean-response-function"><span class="header-section-number">23.2.1</span> Probit Mean Response Function</a></li>
  </ul></li>
  <li><a href="#logistic-mean-response-function" id="toc-logistic-mean-response-function" class="nav-link" data-scroll-target="#logistic-mean-response-function"><span class="header-section-number">23.3</span> Logistic Mean Response Function</a></li>
  <li><a href="#interpretation-of-the-coefficients" id="toc-interpretation-of-the-coefficients" class="nav-link" data-scroll-target="#interpretation-of-the-coefficients"><span class="header-section-number">23.4</span> Interpretation of the Coefficients</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Introduction to Logistic Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<blockquote class="blockquote">
<p>“If all you have is a hammer, everything looks like a nail.” - Bernard Baruch</p>
</blockquote>
<section id="binary-response-variable" class="level2" data-number="23.1">
<h2 data-number="23.1" class="anchored" data-anchor-id="binary-response-variable"><span class="header-section-number">23.1</span> Binary Response Variable</h2>
<p>In a variety of regression applications, the response variable of interest has only two possible <em>qualitative</em> outcomes, and therefore can be represented by a binary indicator variable taking on values 0 and 1.</p>
<p>A binary response variable is said to involve binary responses or dichotomous responses.</p>
<p>We consider first the meaning of the response function when the outcome variable is binary, and then we take up some special problems that arise with this type of response variable.</p>
<section id="response-function-meaning" class="level3" data-number="23.1.1">
<h3 data-number="23.1.1" class="anchored" data-anchor-id="response-function-meaning"><span class="header-section-number">23.1.1</span> Response Function Meaning</h3>
<p>Consider the simple linear regression model <span class="math display">\[
\begin{align*}
y_{i} &amp; =\beta_{0}+\beta_{1}x_{i}+\varepsilon_{i}\qquad y_{i}=0,1
\end{align*}
\]</span> where the outcome <span class="math inline">\(y_{i}\)</span> is binary taking on the value of either 0 or 1.</p>
<p>The expected response $E$ has a special meaning in this case.</p>
<p>Since <span class="math inline">\(E\left[ \varepsilon_{i}\right] =0\)</span>, we have <span class="math display">\[
\begin{align*}
E\left[ y_{i}\right]  &amp; =\beta_{0}+\beta_{1}x_{i}
\end{align*}
\]</span></p>
<p>Consider <span class="math inline">\(y_{i}\)</span> to be a Bernoulli random variable for which we can state the probability distribution as follows <span class="math display">\[
\begin{align*}
y_{i} &amp; \quad\textbf{ Probability}\\
1 &amp; \quad P\left(y_{i}=1\right)={\pi_{i}}\\
0 &amp; \quad P\left(y_{i}=0\right)={1-\pi_{i}}
\end{align*}
\]</span></p>
<p>Thus, <span class="math inline">\(\pi_{i}\)</span> is the probability that <span class="math inline">\(y_{i}=1\)</span> and <span class="math inline">\(1-\pi_{i}\)</span> is the probability that <span class="math inline">\(y_{i}=0\)</span></p>
<p>By the definition of a discrete random variable, we have <span class="math display">\[
{\begin{align*}
E\left[ y_{i}\right]  &amp; =1\left(\pi_{i}\right)+0\left(1-\pi_{i}\right)\\
&amp; =\pi_{i}\\
&amp; =P\left(y_{i}=1\right)
\end{align*}}
\]</span></p>
<p>Thus, <span class="math display">\[
{\begin{align*}
E\left[ y_{i}\right]  &amp; =\beta_{0}+\beta_{1}x_{i}\\
&amp; =\pi_{i}
\end{align*}}
\]</span></p>
<p>The mean response <span class="math inline">\(E\{y_i\}\)</span> as given by the response function is therefore simply the probability that <span class="math inline">\(y_i = 1\)</span> when the level of the predictor variable is <span class="math inline">\(x_i\)</span>.</p>
<p>This interpretation of the mean response applies whether the response function is a simple linear one, as here, or a complex multiple regression one.</p>
<p>The mean response, when the outcome variable is a 0, 1 indicator variable, always represents the probability that <span class="math inline">\(Y = 1\)</span> for the given levels of the predictor variables.</p>
</section>
<section id="problems-when-response-is-binary" class="level3" data-number="23.1.2">
<h3 data-number="23.1.2" class="anchored" data-anchor-id="problems-when-response-is-binary"><span class="header-section-number">23.1.2</span> Problems when response is binary</h3>
<p>Special problems arise, unfortunately, when the response variable is an indicator variable. We consider three of these now, using a simple linear regression model as an illustration.</p>
<ol type="1">
<li><p><em>Nonnormal error terms</em>: For a binary 0, 1 response variable, each error term can take on only two values: <span class="math display">\[
\begin{align*}
\text{When }y_{i}=1: &amp; \quad\varepsilon_{i}=1-\beta_{0}-\beta_{1}x_{i}\\
\text{When }y_{i}=0: &amp; \quad\varepsilon_{i}=0-\beta_{0}-\beta_{1}x_{i}
\end{align*}
\]</span></p>
<p>Clearly, normal error regression model, which assumes that the <span class="math inline">\(\varepsilon_i\)</span> are normally distributed, is not appropriate.</p></li>
<li><p><em>Nonconstant Error Variance</em>: Another problem with the error terms is that they do not have equal variances when the response variable is an indicator variable.</p>
<p>To see this, note that <span class="math display">\[
\begin{align*}
Var[\varepsilon_i] = (\beta_{0}+\beta_{1}x_{i})(1-\beta_{0}-\beta_{1}x_{i})
\end{align*}
\]</span></p>
<p>Note that <span class="math inline">\(Var\{\varepsilon_i\}\)</span> depends on <span class="math inline">\(x_i\)</span>. Hence, the error variances will differ at different levels of X, and ordinary least squares will no longer be optimal.</p></li>
<li><p><em>Constraints one Response Function</em>: Since the response function represents probabilities when the outcome variable is a 0, 1 indicator variable, the mean responses should be constrained as follows: <span class="math display">\[
0\le E\{Y\}=\pi \le 1
\]</span></p>
<p>The difficulties created by the need for the restriction on the response function are the most serious.</p>
<p>One could use weighted least squares to handle the problem of unequal error variances.</p>
<p>In addition, with large sample sizes the method of least squares provides estimators that are asymptotically normal under quite general conditions, even if the distribution of the error terms is far from normal.</p>
<p>However, the constraint on the mean responses to fall between 0 and 1 frequently will rule out a linear response function.</p></li>
</ol>
</section>
</section>
<section id="sigmoidal-response-functions" class="level2" data-number="23.2">
<h2 data-number="23.2" class="anchored" data-anchor-id="sigmoidal-response-functions"><span class="header-section-number">23.2</span> Sigmoidal Response Functions</h2>
<p>In this section, we will examine two response functions for modeling binary responses.</p>
<p>These functions are bounded between 0 and 1, have a characteristic <em>sigmoidal</em>- or S-shape, and approach 0 and 1 asymptotically.</p>
<p>These functions arise naturally when the binary response variable results from a zero-one recoding (or dichotomization) of an underlying continuous response variable, and they are often appropriate for discrete binary responses as well.</p>
<section id="probit-mean-response-function" class="level3" data-number="23.2.1">
<h3 data-number="23.2.1" class="anchored" data-anchor-id="probit-mean-response-function"><span class="header-section-number">23.2.1</span> Probit Mean Response Function</h3>
<p>Consider a health researcher studying the effect of a mother’s use of alcohol (<span class="math inline">\(x\)</span> -an index of degree of alcohol use during pregnancy) on the duration of her pregnancy (<span class="math inline">\(y^C\)</span>).</p>
<p>Here we use the superscript <span class="math inline">\(c\)</span> to emphasize that the response variable, pregnancy duration, is a <em>continuous</em> response.</p>
<p>This can be represented by a simple linear regression model: <span class="math display">\[
y^C_i = \beta_0^c+\beta_1^c x_i +\varepsilon_i^c
\]</span> and we will assume that <span class="math inline">\(\varepsilon^c_i\)</span> is normally distributed with mean zero and variance <span class="math inline">\(\sigma^2_c\)</span>.</p>
<p>If the continuous response variable, pregnancy duration, were available, we might proceed with the usual simple linear regression analysis. However, in this instance, researchers coded each pregnancy duration as preterm or full term using the following rule: <span class="math display">\[
\begin{align*}
y_{i} &amp; =\begin{cases}
1 &amp; \text{ if }y_{i}^{c}\le38\text{ weeks (preterm)}\\
0 &amp; \text{ if }y_{i}^{c}&gt;38\text{ weeks (full term)}
\end{cases}
\end{align*}
\]</span></p>
<p>It follows then that <span class="math display">\[
\begin{align*}
P\left(y_{i}=1\right)=\pi_{i} &amp; =P\left(y_{i}^{c}\le38\right)\\
&amp; =P\left(\beta_{0}^{c}+\beta_{1}^{c}x_{i}+\varepsilon_{i}^{c}\le38\right)\\
&amp; =P\left(\varepsilon_{i}^{c}\le38-\beta_{0}^{c}-\beta_{1}^{c}x_{i}\right)\\
&amp; =P\left(\frac{\varepsilon_{i}^{c}}{\sigma_{c}}\le\frac{38-\beta_{0}^{c}}{\sigma_{c}}-\frac{\beta_{1}^{c}}{\sigma_{c}}x_{i}\right)\\
&amp; =P\left(Z\le\beta_{0}^{*}+\beta_{1}^{*}x_{i}\right)
\end{align*}
\]</span> where <span class="math display">\[
\begin{align*}
\beta_{0}^{c} &amp; =\frac{38-\beta_{0}^{c}}{\sigma_{c}}\\
\beta_{1}^{c} &amp; =-\frac{\beta_{1}^{c}}{\sigma_{c}}\\
Z &amp; =\frac{\varepsilon_{i}^{c}}{\sigma_{c}}.
\end{align*}
\]</span></p>
<p>Note that <span class="math inline">\(Z\)</span> follows a standard normal distribution.</p>
<p>If we let <span class="math inline">\(P\left(Z\le z\right)=\Phi\left(z\right)\)</span>, we have <span class="math display">\[
\begin{align*}
P\left(y_{i}=1\right) &amp; =\Phi\left(\beta_{0}^{*}+\beta_{1}^{*}x_{i}\right)
\end{align*}
\]</span></p>
<p>From this we have what is known as the probit mean response function <span class="math display">\[
\begin{align*}
E\left[ y_{i}\right]  &amp; =\pi_{i}=\Phi\left(\beta_{0}^{*}+\beta_{1}^{*}x_{i}\right)
\end{align*}
\]</span></p>
<p>The inverse function <span class="math inline">\(\Phi^{-1}\)</span> of the standard normal cumulative distribution function is sometimes called the <em>probit transformation</em>.</p>
<p>We solve for the linear predictor <span class="math inline">\(\beta_{0}^{*}+\beta_{1}^{*}x_{i}\)</span> by applying the probit transformation to both sides of the expression: <span class="math display">\[
\begin{align*}
\Phi^{-1}\left(\pi_{i}\right) &amp; =\pi_{i}^{\prime}=\beta_{0}^{*}+\beta_{1}^{*}x_{i}
\end{align*}
\]</span></p>
<p>The resulting expression <span class="math inline">\(\pi_{i}^{\prime}=\beta_{0}^{*}+\beta_{1}^{*}x_{i}\)</span> is called the probit response function, or more generally, the linear predictor.</p>
<section id="example-beta_00" class="level4">
<h4 class="anchored" data-anchor-id="example-beta_00">Example: <span class="math inline">\(\beta_0^*=0\)</span>}</h4>
<p>black line: <span class="math inline">\(\beta_1^*=5\)</span></p>
<p>red line: <span class="math inline">\(\beta_1^*=1\)</span></p>
<p><img src="ex1.png" class="img-fluid"></p>
</section>
<section id="example-beta_1-1" class="level4">
<h4 class="anchored" data-anchor-id="example-beta_1-1">Example: <span class="math inline">\(\beta_1^*=-1\)</span>}</h4>
<p>black line: <span class="math inline">\(\beta_0^*=0\)</span></p>
<p>red line: <span class="math inline">\(\beta_0^*=5\)</span></p>
<p><img src="ex2.png" class="img-fluid"></p>
</section>
</section>
</section>
<section id="logistic-mean-response-function" class="level2" data-number="23.3">
<h2 data-number="23.3" class="anchored" data-anchor-id="logistic-mean-response-function"><span class="header-section-number">23.3</span> Logistic Mean Response Function</h2>
<p>We have seen that the assumption of normally distributed errors for the underlying continuous response variable led to the use of the standard normal cumulative distribution function, <span class="math inline">\(\Phi\)</span> to model <span class="math inline">\(\pi_i\)</span>.</p>
<p>An alternative error distribution that is very similar to the normal distribution is the <em>logistic</em> distribution.</p>
<p>Plots of the standard normal density function and the logistic density function, each with mean zero and variance one are nearly indistinguishable, although the logistic distribution has slightly heavier tails.</p>
<p><img src="dists.png" class="img-fluid"></p>
<p>Note the cumulative distribution function of a logistic random variable <span class="math inline">\(\varepsilon_{L}\)</span> having mean 0 and standard deviation <span class="math inline">\(\sigma=\pi/\sqrt{3}\)</span> is: <span class="math display">\[
\begin{align*}
F_{L}\left(\varepsilon_{L}\right) &amp; =\frac{\exp\left(\varepsilon_{L}\right)}{1+\exp\left(\varepsilon_{L}\right)}
\end{align*}
\]</span></p>
<p>Suppose now that <span class="math inline">\(\varepsilon_{i}^{c}\)</span> has a logistic distribution with mean 0 and standard deviation <span class="math inline">\(\sigma_{c}\)</span>. Then we have <span class="math display">\[
\begin{align*}
P\left(y_{i}=1\right) &amp; =P\left(\frac{\varepsilon_{i}^{c}}{\sigma_{c}}\le\beta_{0}^{*}+\beta_{1}^{*}x_{1}\right)
\end{align*}
\]</span> where <span class="math inline">\(\varepsilon_{i}^{c}/\sigma_{c}\)</span> follows a logistic distribution with mean zero and standard deviation one.</p>
<p>Multiplying both sides of the inequality inside the probability statement on the right by <span class="math inline">\(\pi/\sqrt{3}\)</span> gives us <span class="math display">\[
\begin{align*}
P\left(y_{i}=1\right)=\pi_{i} &amp; =P\left(\frac{\pi}{\sqrt{3}}\frac{\varepsilon_{i}^{c}}{\sigma_{c}}\le\frac{\pi}{\sqrt{3}}\beta_{0}^{*}+\frac{\pi}{\sqrt{3}}\beta_{1}^{*}x_{i}\right)\\
&amp; =P\left(\varepsilon_{L}\le\beta_{0}+\beta_{1}x_{i}\right)\\
&amp; =F_{L}\left(\beta_{0}+\beta_{1}x_{i}\right)\\
&amp; =\frac{\exp\left(\beta_{0}+\beta_{1}x_{i}\right)}{1+\exp\left(\beta_{0}+\beta_{1}x_{i}\right)}
\end{align*}
\]</span> where <span class="math display">\[
\begin{align*}
\beta_{0} &amp; =\frac{\pi}{\sqrt{3}}\beta_{0}^{*}\\
\beta_{1} &amp; =\frac{\pi}{\sqrt{3}}\beta_{1}^{*}
\end{align*}
\]</span> denote the logistic regression parameters.</p>
<p>To summarize, the logistic mean response function is <span class="math display">\[
\begin{align*}
E\left[ y_{i}\right]  &amp; =\pi_{i}\\
&amp; =F_{L}\left(\beta_{0}+\beta_{1}x_{i}\right)\\
&amp; =\frac{\exp\left(\beta_{0}+\beta_{1}x_{i}\right)}{1+\exp\left(\beta_{0}+\beta_{1}x_{i}\right)}\\
&amp; =\frac{1}{1+\exp\left(-\beta_{0}-\beta_{1}x_{i}\right)}
\end{align*}
\]</span></p>
<p>Applying the inverse of the cumulative distribution function <span class="math inline">\(F_{L}\)</span> gives <span class="math display">\[
\begin{align*}
F_{L}^{-1}\left(\pi_{i}\right) &amp; =\beta_{0}+\beta_{1}x_{1}=\pi_{i}^{\prime}
\end{align*}
\]</span></p>
<p><span class="math inline">\(F_{L}^{-1}\left(\pi_{i}\right)\)</span> is called the <em>logit transformation</em> and is given by <span class="math display">\[
\begin{align*}
F_{L}^{-1}\left(\pi_{i}\right) &amp; =\log\left(\frac{\pi_{i}}{1-\pi_{i}}\right)
\end{align*}
\]</span> where the ratio <span class="math inline">\(\pi_{i}/\left(1-\pi_{i}\right)\)</span> is called the <em>odds} </em>ratio}.</p>
<section id="example-beta_00-1" class="level4">
<h4 class="anchored" data-anchor-id="example-beta_00-1">Example: <span class="math inline">\(\beta_0^*=0\)</span></h4>
<p>black line: <span class="math inline">\(\beta_1^*=5\)</span></p>
<p>red line: <span class="math inline">\(\beta_1^*=1\)</span></p>
<p><img src="ex3.png" class="img-fluid"></p>
</section>
<section id="example-beta_1-1-1" class="level4">
<h4 class="anchored" data-anchor-id="example-beta_1-1-1">Example: <span class="math inline">\(\beta_1^*=-1\)</span></h4>
<p>black line: <span class="math inline">\(\beta_0^*=0\)</span></p>
<p>red line: <span class="math inline">\(\beta_0^*=5\)</span></p>
<p><img src="ex4.png" class="img-fluid"></p>
</section>
</section>
<section id="interpretation-of-the-coefficients" class="level2" data-number="23.4">
<h2 data-number="23.4" class="anchored" data-anchor-id="interpretation-of-the-coefficients"><span class="header-section-number">23.4</span> Interpretation of the Coefficients</h2>
<p>The interpretation of the estimated regression coefficient <span class="math inline">\(\hat{\beta}_1\)</span> in the fitted logistic response function is not the straightforward interpretation of the slope in a linear regression model.</p>
<p>The reason is that the effect of a unit increase in <span class="math inline">\(x\)</span> varies for the logistic regression model according to the location of the starting point on the <span class="math inline">\(x\)</span> scale.</p>
<p>An interpretation of <span class="math inline">\(\hat{\beta}_1\)</span> is found in the property of the fitted logistic function that the estimated odds <span class="math display">\[
\frac{\hat{\pi}}{1-\hat{\pi}}
\]</span> are multiplied by <span class="math display">\[
\exp(\hat{\beta}_1)
\]</span> for any unit increase in <span class="math inline">\(x\)</span>.</p>
<p>To see this, we consider the value of the fitted logit response function at <span class="math inline">\(X = x_j\)</span>; <span class="math display">\[
\hat{\pi}^\prime (x_j)={\hat{\beta}_0+\hat{\beta}_1 x_j}
\]</span></p>
<p>The notation <span class="math inline">\(\hat{\pi}^\prime (x_j)\)</span> indicates specifically the <span class="math inline">\(x\)</span> level associated with the fitted value.</p>
<p>We also consider the value of the fitted logit response function at <span class="math inline">\(X = x_j + 1\)</span>; <span class="math display">\[
\hat{\pi}^\prime (x_j+1)={\hat{\beta}_0+\hat{\beta}_1( x_j+1)}
\]</span> The difference between the two fitted values is simply <span class="math display">\[
{\begin{align*}
\hat{\pi}^\prime (x_j+1)-\hat{\pi}^\prime (x_j)&amp; = \hat{\beta}_0+\hat{\beta}_1( x_j+1) - \hat{\beta}_0-\hat{\beta}_1 x_j \\
&amp; = \hat{\beta}_1
\end{align*}}
\]</span></p>
<p>Now <span class="math inline">\(\hat{\pi}^\prime (x_j)\)</span> is the logarithm of the estimated odds when <span class="math inline">\(X=x_j\)</span>; we shall denote it by <span class="math inline">\(\ln(\text{odds}_1)\)</span>.</p>
<p>Similarly, <span class="math inline">\(\hat{\pi}^\prime (x_j+1)\)</span> is the logarithm of the estimated odds when <span class="math inline">\(X=x_j+1\)</span>; we shall denote it by <span class="math inline">\(\ln(\text{odds}_2)\)</span>.</p>
<p>Hence, the difference between the two fitted logit response values can be expressed as follows: <span class="math display">\[
\begin{align*}
\ln(\text{odds}_2)-\ln(\text{odds}_1) = \ln\left(\frac{\text{odds}_2}{\text{odds}_1}\right) = \hat{\beta}_1
\end{align*}
\]</span></p>
<p>Taking antilogs (exponentials) of each side, we see that the estimated ratio of the odds, called the odds ratio, <span class="math inline">\(\hat{OR}\)</span>, is <span class="math display">\[
\hat{OR} = \frac{\text{odds}_2}{\text{odds}_1}=\exp(\hat{\beta}_1)
\]</span></p>
<!-- ## Multiple Logistic Regression} -->
<!-- The simple logistic regression model  is easily extended to more than one predictor -->
<!-- variable.  -->
<!-- In fact, several predictor variables are usually required with logistic regression to -->
<!-- obtain adequate description and useful predictions. -->
<!-- In matrix notation, the logistic response function becomes -->
<!-- $$ -->
<!-- E\{Y\} = \frac{\exp{\left(\textbf{X}^\prime\boldsymbol{\beta}\right)}}{1+\exp{\left(\textbf{X}^\prime\boldsymbol{\beta}\right)}} -->
<!-- $$ -->
<!-- Like the simple logistic response function, the multiple logistic-response function -->
<!-- is monotonic and sigmoidal in shape with respect to $\textbf{X}^\prime\boldsymbol{\beta}$ and is almost linear -->
<!-- when $\pi$ is between .2 and .8.  -->
<!-- \marginpar{} -->
<!-- The X variables may be different predictor variables, or -->
<!-- some may represent curvature and/or interaction effects.  -->
<!-- Also, the predictor variables may -->
<!-- be quantitative, or they may be qualitative and represented by indicator variables.  -->
<!-- This -->
<!-- flexibility makes the multiple logistic regression model very attractive. -->
<!-- \begin{example} -->
<!-- In a health study to investigate an epidemic outbreak of a disease that is spread by\\ mosquitoes, -->
<!--                             individuals were randomly sampled within two sectors in a city to determine if the person -->
<!--                             had recently contracted the disease under study.  -->
<!--                             This was ascertained by the interviewer, -->
<!--                             who asked pertinent questions to assess whether certain specific symptoms associated with -->
<!--                             the disease were present during the specified period.  -->
<!--                             The response variable Y was coded 1 -->
<!--                             if this disease was determined to have been present, and 0 if not. -->
<!--                             Three predictor variables were included in the study, representing known or potential -->
<!--                             risk factors. -->
<!--                             They are age, socioeconomic status of household, and sector within city.  -->
<!-- \newpage -->
<!--                             Age -->
<!--                             ($x_1$) is a quantitative variable. Socioeconomic status is a categorical variable with three -->
<!--                             levels. It is represented by two indicator variables ($x_2$ and $x_3$), as follows: -->
<!--                             \begin{align*} -->
<!--                             Class &\quad x_1 & x_2\\ -->
<!--                             Upper & \quad 0 & 0\\ -->
<!--                             Middle & \quad 1 & 0\\ -->
<!--                             Lower & \quad 0 & 1 -->
<!--                             \end{align*} -->
<!--                             City sector is also a categorical variable. Since there were only two sectors in the study, -->
<!--                             one indicator variable ($x_4$ ) was used, defined so that $x_4 = 0$ for sector 1 and $x_4 = 1$ for -->
<!--                             sector 2. -->
<!-- \marginpar{} -->
<!-- \begin{lstlisting} -->
<!-- library(boot) -->

<!-- dat = read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/ -->
<!--      data/textdatasets/KutnerData/Chapter%2014%20Data%20Sets/CH14TA03.txt") -->
<!-- names(dat) = c("id", "X1", "X2", "X3", "X4", "Y") -->
<!-- #fit a logistic regression model using glm -->
<!-- fit = glm(Y~.-id, data=dat,family = binomial(link="logit")) -->
<!-- pred.prob = fit %>% fitted.values() -->
<!-- link = predict(fit, type="link") -->
<!-- plot(link, pred.prob, col= dat$Y+1) -->
<!-- \end{lstlisting} -->
<!-- \includegraphics[scale=.7]{plots/ex08_01_01a.png} -->
<!-- \begin{lstlisting}[firstnumber=last] -->
<!-- #if we pedict probabilities greater than .5 as predicting a  -->
<!-- #success (predict Y=1), then we can predict the outcomes as -->
<!-- pred.y = ifelse(pred.prob >=.5, "Y","N") -->

<!-- table(pred.y, dat$Y) -->
<!-- pred.y  0  1 -->
<!--      N 58 19 -->
<!--      Y  9 12 -->
<!-- summary(fit) -->
<!-- Call: -->
<!-- glm(formula = Y ~ . - id, family = binomial(link = "logit"),  -->
<!--     data = dat) -->
<!-- Deviance Residuals:  -->
<!--     Min       1Q   Median       3Q      Max   -->
<!-- -1.6552  -0.7529  -0.4788   0.8558   2.0977   -->
<!-- Coefficients: -->
<!--             Estimate Std. Error z value Pr(>|z|)     -->
<!-- (Intercept) -2.31293    0.64259  -3.599 0.000319 *** -->
<!-- X1           0.02975    0.01350   2.203 0.027577 *   -->
<!-- X2           0.40879    0.59900   0.682 0.494954     -->
<!-- X3          -0.30525    0.60413  -0.505 0.613362     -->
<!-- X4           1.57475    0.50162   3.139 0.001693 **  -->
<!-- --- -->
<!-- Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- (Dispersion parameter for binomial family taken to be 1) -->
<!--     Null deviance: 122.32  on 97  degrees of freedom -->
<!-- Residual deviance: 101.05  on 93  degrees of freedom -->
<!-- AIC: 111.05 -->
<!-- Number of Fisher Scoring iterations: 4 -->
<!-- fit %>% coef() %>% exp() -->
<!-- (Intercept)          X1          X2          X3          X4  -->
<!--  0.09897037  1.03019705  1.50499600  0.73693576  4.82953038  -->
<!-- cv = cv.glm(dat, fit) -->
<!-- cv$delta -->
<!-- [1] 0.1962685 0.1961593 -->
<!-- #compare with probit -->
<!-- #fit a logistic regression model using glm -->
<!-- fit = glm(Y~.-id, data=dat,family = binomial(link="probit")) -->
<!-- pred.prob = fit %>% fitted.values() -->
<!-- link = predict(fit, type="link") -->
<!-- plot(link, pred.prob, col= dat$Y+1) -->
<!-- \end{lstlisting} -->
<!-- \marginpar{} -->
<!-- \includegraphics[scale=.7]{plots/ex08_01_01b.png} -->
<!-- \begin{lstlisting}[firstnumber=last] -->
<!-- pred.y = ifelse(pred.prob >=.5, "Y","N") -->

<!-- table(pred.y, dat$Y) -->
<!-- pred.y  0  1 -->
<!--      N 57 19 -->
<!--      Y 10 12 -->
<!-- cv = cv.glm(dat, fit) -->
<!-- cv$delta -->
<!-- [1] 0.1949919 0.1948840 -->
<!-- \end{lstlisting} -->
<!-- \end{example} -->
<!--  \subsection*{Multinomial Logistic Regression} -->
<!-- ## Nominal Response} -->
<!-- \marginpar{} -->
<!-- Logistic regression is most frequently used to model the relationship between a dichotomous -->
<!--                             response variable and a set of predictor variables.  -->
<!--                             On occasion, however. the response -->
<!--                             variable may have *more} than two levels.  -->
<!--                             Logistic regression can still be employed by -->
<!--                             means of a *polytomous}-or\\ *multinomial}-logistic regression model. -->
<!--                             Multinomial logistic -->
<!--                             regression models are used in many fields. In business, for instance, a market researcher -->
<!--                             may wish to relate a consumer's choice of product (product A, product B. product C) to -->
<!--                             the consumer's age, gender, geographic location and several other potential explanatory -->
<!--                             variables.  -->
<!--                             This is an example of nominal multinomial regression because the response -->
<!--                             categories are purely qualitative and not *ordered} in any way.  -->
<!-- *Ordinal} response categories can -->
<!--                             also be modeled using multinomial regression.  -->
<!--                             For example, the relation between severity -->
<!--                             of disease measured on an ordinal scale (mild, moderate, severe) and age of patient, gender -->
<!--                             of patient, and some other explanatory variables may be of interest.  -->
<!--                             The response variable $Y$ must be coded in the same way as indicator variables except we will need an indicator variable for each category of $Y$.  -->
<!--                             Instead of going through the details, we will illustrate with R. -->
<!-- \begin{example} -->
<!-- A study was undertaken to determine the strength of association between several risk factors -->
<!--                             and the duration of pregnancies.  -->
<!-- \marginpar{} -->
<!--                             The risk factors considered were mother's age, nutritional -->
<!--                             status, history of tobacco use, and history of alcohol use.  -->
<!--                             The response of interest, pregnancy -->
<!--                             duration, is a three-category variable that was coded as follows: -->
<!--                             \begin{align*} -->
<!--                             y_{i} &\quad \textbf{Pregnancy Duration Category}\\ -->
<!--                             1 &\quad \text{Preterm (less than 36 weeks)}\\ -->
<!--                             2 &\quad \text{Intermediate term (36 to 37 weeks)}\\ -->
<!--                             3 &\quad \text{Full term (38 weeks or greater)} -->
<!--                             \end{align*} -->
<!--                             Relevant data for 102 women who had recently given birth at a large metropolitan hospital -->
<!--                             were obtained.  -->
<!--                             X variables include:  -->
<!--                             - Nutritional status ($x_1$), is an index of nutritional status (higher score denotes better nutritional status).  -->
<!--                             - Age was categorized into three groups: less than 20 years of age (coded 1), from 21 -->
<!--                             to 30 years of age (coded 2), and greater than 30 years of age (coded 3). It is represented by -->
<!--                             two indicator variables (X2 and X3): -->
<!--                             \begin{align*} -->
<!--                             x_{2} &\quad x_{3} & \textbf{Class}\\ -->
<!--                             1 &\quad 0 & \text{Less than or equal to 20}\\ -->
<!--                             0 &\quad 0 & \text{21 to 30}\\ -->
<!--                             0 &\quad 1 & \text{Greater than 30} -->
<!--                             \end{align*} -->
<!--                             - Alcohol ($x_4$) and smoking history ($x_5$) were also qualitative predictors; the categories were "Yes" (coded 1) and "No" (coded 0). -->
<!-- \begin{lstlisting} -->

<!-- dat = read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/ -->
<!--       textdatasets/KutnerData/Chapter%2014%20Data%20Sets/CH14TA13.txt") -->
<!-- names(dat) = c("id", "Y", "Y1", "Y2", "Y3", "X1", "X2", "X3", "X4", "X5") -->
<!-- head(dat) -->
<!--   id Y Y1 Y2 Y3  X1 X2 X3 X4 X5 -->
<!-- 1  1 1  1  0  0 150  0  0  0  1 -->
<!-- 2  2 1  1  0  0 124  1  0  0  0 -->
<!-- 3  3 1  1  0  0 128  0  0  0  1 -->
<!-- 4  4 1  1  0  0 128  1  0  0  1 -->
<!-- 5  5 1  1  0  0 133  0  0  1  1 -->
<!-- 6  6 1  1  0  0 130  0  0  1  1 -->
<!-- \end{lstlisting} -->
<!--                             Note that the Y variable is already coded with the variables Y1, Y2, and Y3 in the dataset. -->
<!-- \marginpar{} -->
<!--                             We do not have to do this ourselves, R will code the indicator variables for us if we tell R the variable is a factor.  -->
<!-- \begin{lstlisting} -->
<!-- library(nnet) -->
<!-- reg = multinom(factor(Y)~X1+X2+X3+X4+X5, data=dat) -->
<!-- summary(reg) -->
<!-- Call: -->
<!-- multinom(formula = factor(Y) ~ X1 + X2 + X3 + X4 + X5, data = dat) -->
<!-- Coefficients: -->
<!--   (Intercept)         X1          X2         X3         X4         X5 -->
<!-- 2   -1.517009 0.01897219 -0.04355881 -0.1721573 -0.9758857 -0.2218629 -->
<!-- 3   -5.475353 0.06542052 -2.95693397 -2.0596355 -2.0428964 -2.4522828 -->
<!-- Std. Errors: -->
<!--   (Intercept)         X1        X2        X3        X4        X5 -->
<!-- 2    2.047984 0.01631570 0.7373261 0.6943946 0.5869073 0.5856117 -->
<!-- 3    2.271691 0.01823932 0.9644844 0.8947673 0.7097457 0.7315046 -->
<!-- Residual Deviance: 168.6754  -->
<!-- AIC: 192.6754  -->
<!-- \end{lstlisting} -->
<!--                             Note in the summary, we have two regression fits. What we actually have done is model the odds of the probability of being in category 2 to the probability of being in category 1. -->
<!--                             We also modeled the odds of the probability of being in category 3 to the probability of being in category 1.  -->
<!--                             So we call category 1 the baseline category.  -->
<!--                             To get the effect on the odds ratio for each coefficient: -->
<!-- \begin{lstlisting}[firstnumber=last] -->
<!-- exp(coef(reg)) -->
<!--   (Intercept)       X1         X2        X3        X4         X5 -->
<!-- 2 0.219366942 1.019153 0.95737625 0.8418467 0.3768584 0.80102519 -->
<!-- 3 0.004188751 1.067608 0.05197804 0.1275004 0.1296526 0.08609682 -->
<!-- \end{lstlisting} -->
<!-- \marginpar{} -->
<!--                             We can use a different baseline category such as full term: -->
<!-- \begin{lstlisting}[firstnumber=last] -->
<!-- dat$Y = as.factor(dat$Y) -->
<!-- dat$Y = relevel(dat$Y, ref = "3") -->
<!-- reg.nom = multinom(factor(Y)~X1+X2+X3+X4+X5, data=dat) -->
<!-- summary(reg.nom) -->
<!-- Call: -->
<!-- multinom(formula = factor(Y) ~ X1 + X2 + X3 + X4 + X5, data = dat) -->
<!-- Coefficients: -->
<!--   (Intercept)          X1       X2       X3       X4       X5 -->
<!-- 1    5.475147 -0.06541919 2.957028 2.059662 2.042900 2.452362 -->
<!-- 2    3.958370 -0.04644903 2.913475 1.887550 1.067001 2.230492 -->
<!-- Std. Errors: -->
<!--   (Intercept)         X1        X2        X3        X4        X5 -->
<!-- 1    2.271677 0.01823916 0.9644921 0.8947727 0.7097461 0.7315106 -->
<!-- 2    1.941063 0.01488581 0.8575544 0.8088255 0.6495262 0.6681955 -->
<!-- Residual Deviance: 168.6754  -->
<!-- AIC: 192.6754  -->
<!-- #to get the predicted probability for each observation -->
<!-- preds.nom = fitted(reg.nom) -->
<!-- preds.nom[1:10,] -->
<!--             3           1          2 -->
<!-- 1  0.62078219 0.094217907 0.28499991 -->
<!-- 2  0.18455857 0.254210377 0.56123105 -->
<!-- 3  0.34297651 0.219535548 0.43748794 -->
<!-- 4  0.02716468 0.334553819 0.63828151 -->
<!-- 5  0.13335384 0.474690286 0.39195587 -->
<!-- 6  0.11480795 0.497290719 0.38790133 -->
<!-- 7  0.95144461 0.009569423 0.03898596 -->
<!-- 8  0.09738287 0.335195611 0.56742152 -->
<!-- 9  0.02775016 0.658559402 0.31369044 -->
<!-- 10 0.40441329 0.186642234 0.40894447 -->
<!-- # if we pedict the category with the highest probability: -->
<!-- ind = apply(preds.nom, 1, which.max) -->
<!-- preds.nom.cat = colnames(preds.nom)[ind] -->
<!-- preds.nom.cat -->
<!--   [1] "3" "2" "2" "2" "1" "1" "3" "2" "1" "2" "3" "1" "2" "1" "1" "3" "2" "2" "1" "1" "1" -->
<!--  [22] "1" "1" "2" "1" "2" "3" "3" "2" "2" "2" "2" "2" "2" "3" "2" "2" "2" "2" "3" "2" "2" -->
<!--  [43] "2" "2" "2" "2" "3" "2" "1" "2" "2" "1" "2" "3" "2" "2" "2" "1" "3" "1" "3" "3" "3" -->
<!--  [64] "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" -->
<!--  [85] "2" "3" "3" "3" "1" "3" "3" "3" "3" "3" "2" "2" "2" "2" "1" "1" "3" "1" -->
<!-- #compare to observed categories: -->

<!-- table(preds.nom.cat, dat$Y) -->
<!-- preds.nom.cat  3  1  2 -->
<!--             1  4 12  4 -->
<!--             2  5 10 23 -->
<!--             3 32  4  8 -->
<!-- \end{lstlisting} -->
<!-- \end{example} -->
<!--  \subsection*{Ordinal Regression} -->
<!-- ## Ordinal Response} -->
<!-- When the response variable $Y$ takes on categorical responses that can be put into order, then  the multinomial logistic regression discussed in Section 8.2 can still be used.  -->
<!-- \marginpar{} -->
<!--                             However, a more effective strategy, yielding a more parsimonious and more easily interpreted model, results if the *ordering} of the categories is taken into account explicitly. -->
<!--                             The model that is usually employed is called the *proportional} *odds}  model -->
<!--                             The proportional odds model for ordinal logistic regression models the cumulative probabilities -->
<!--                             $$ -->
<!--                             P(y_i\le j) -->
<!--                             $$ -->
<!--                             rather than the specific category probabilities  -->
<!--                             $$ -->
<!--                             P(y_i=j) -->
<!--                             $$ -->
<!--                             as was the case -->
<!--                             for nominal logistic regression. -->
<!--                             Again, we will skip the details and illustrate with R using the pregnancy data from the last section. -->
<!-- \marginpar{} -->
<!-- \begin{example} -->
<!-- \ -->
<!-- \begin{lstlisting} -->
<!-- library(MASS) -->

<!-- dat = read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/ -->
<!--                  textdatasets/KutnerData/Chapter%2014%20Data%20Sets/CH14TA13.txt") -->
<!-- names(dat) = c("id", "Y", "Y1", "Y2", "Y3", "X1", "X2", "X3", "X4", "X5") -->
<!-- #tell R the order of the factors -->
<!-- dat$Y = factor(dat$Y,levels = c("1", "2", "3"), ordered=T) -->
<!-- reg.ord = polr(Y~X1+X2+X3+X4+X5,data=dat) -->
<!-- summary(reg.ord) -->
<!-- Call: -->
<!-- polr(formula = Y ~ X1 + X2 + X3 + X4 + X5, data = dat) -->
<!-- Coefficients: -->
<!--       Value Std. Error t value -->
<!-- X1  0.04887    0.01182   4.133 -->
<!-- X2 -1.97601    0.57616  -3.430 -->
<!-- X3 -1.36348    0.54648  -2.495 -->
<!-- X4 -1.66987    0.47537  -3.513 -->
<!-- X5 -1.59154    0.45165  -3.524 -->
<!-- Intercepts: -->
<!--     Value   Std. Error t value -->
<!-- 1|2  2.9301  1.4929     1.9627 -->
<!-- 2|3  5.0249  1.5445     3.2535 -->
<!-- Residual Deviance: 173.5122  -->
<!-- AIC: 187.5122  -->
<!-- #to get the predicted probability for each observation -->
<!-- preds.ord = fitted(reg.ord) -->
<!-- preds.ord[1:10,] -->
<!--              1          2          3 -->
<!-- 1  0.056827016 0.27179270 0.67138028 -->
<!-- 2  0.239742367 0.47950086 0.28075677 -->
<!-- 3  0.150070798 0.43915298 0.41077622 -->
<!-- 4  0.560196430 0.35167985 0.08812372 -->
<!-- 5  0.423470637 0.43299765 0.14353172 -->
<!-- 6  0.459606967 0.41396070 0.12643233 -->
<!-- 7  0.009988634 0.06576689 0.92424447 -->
<!-- 8  0.384644784 0.45082830 0.16452692 -->
<!-- 9  0.751897195 0.20907082 0.03903198 -->
<!-- 10 0.121488681 0.40757743 0.47093389 -->
<!-- # if we pedict the category with the highest probability: -->
<!-- ind = apply(preds.ord, 1, which.max) -->
<!-- preds.ord.cat = colnames(preds.ord)[ind] -->
<!-- preds.ord.cat -->
<!--   [1] "3" "2" "2" "1" "2" "1" "3" "2" "1" "3" "3" "1" "2" "1" "1" "3" "2" "2" "1" "1" "1" -->
<!--  [22] "1" "1" "2" "1" "1" "3" "3" "1" "2" "2" "2" "2" "2" "3" "2" "2" "3" "2" "2" "2" "2" -->
<!--  [43] "1" "2" "2" "2" "3" "2" "2" "1" "2" "2" "1" "3" "1" "2" "1" "1" "2" "2" "3" "3" "3" -->
<!--  [64] "3" "3" "3" "3" "3" "3" "3" "2" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" -->
<!--  [85] "3" "3" "3" "3" "2" "3" "2" "3" "3" "3" "3" "2" "2" "3" "2" "1" "3" "2" -->
<!-- #compare to observed categories: -->

<!-- table(preds.ord.cat, dat$Y) -->
<!-- preds.ord.cat  1  2  3 -->
<!--             1 13  7  1 -->
<!--             2  8 21  7 -->
<!--             3  5  7 33 -->
<!-- \end{lstlisting} -->
<!-- \end{example} -->
<!--  \subsection*{Poisson Regression} -->
<!-- ## Introduction} -->
<!-- \marginpar{} -->
<!-- We consider now another nonlinear regression model where the response outcomes are -->
<!--                             discrete. -->
<!--                             Poisson regression is useful when the outcome is a count, with large-count outcomes -->
<!--                             being rare events. -->
<!--                             For instance, the number of times a household shops at a particular -->
<!--                             supermarket in a week is a count, with a large number of shopping trips to the store during -->
<!--                             the week being a rare event. A researcher may wish to study the relation between a family's -->
<!--                             number of shopping trips to the store during a particular week and the family's income, -->
<!--                             number of children, distance from the store, and some other explanatory variables. -->
<!--                             As another -->
<!--                             example, the relation between the number of hospitalizations of a member of a health -->
<!--                             maintenance organization during the past year and the member's age, income, and previous -->
<!--                             health status may be of interest. -->
<!--                             The Poisson distribution can be utilized for outcomes that are counts ($y_i = 0, 1,2, ...$ ), -->
<!--                             with a large count or frequency being a rare event. -->
<!-- ## The Poisson Distribution} -->
<!-- The Poisson probability distribution is -->
<!--                             $$ -->
<!--                             f(Y) = \frac{\mu^Y \exp(-\mu)}{Y!} -->
<!--                             $$ -->
<!--                             The mean and variance of a Poisson distribution are -->
<!--                             \begin{align*} -->
<!--                             E\{Y\} &= \mu\\ -->
<!--                             \sigma^2\{Y\} &=\mu -->
<!--                             \end{align*} -->
<!--                             Note that the variance is the same as the mean. -->
<!-- \marginpar{} -->
<!--                             Hence, if the number of store trips follows -->
<!--                             the Poisson distribution and the mean number of store trips for a family with three children -->
<!--                             is larger than the mean number of trips for a family with no children, the variances of the -->
<!--                             distributions of outcomes for the two families will also differ. -->
<!-- ## Poisson Regression} -->
<!-- We start with the regression model -->
<!--                             \begin{align*} -->
<!--                             y_{i} & =E\left[ y_{i}\right] +\varepsilon_{i}\qquad i=1,2,\ldots,n -->
<!--                             \end{align*} -->
<!--                             The mean response for the $i$th case, to be denoted now my $\mu_{i}$ -->
<!--                             for simplicity, is assumed as always to be a function of the set of -->
<!--                             predictor variables $x_{1},\ldots,x_{p-1}$. -->
<!--                             We use the notation $\mu\left(\textbf{X}_{i},\boldsymbol{\beta}\right)$ -->
<!--                             to denote the function that relates the mean response $\mu_{i}$ to -->
<!--                             $\textbf{X}_{i}$, the values of the predictor variables for case -->
<!--                             $i$, and $\boldsymbol{\beta}$, the values of the regression coefficients. -->
<!--                             Some commonly used functions for Poisson regression are -->
<!--                             \begin{align*} -->
<!--                             \mu_{i}= & \mu\left(\textbf{X}_{i},\boldsymbol{\beta}\right)=\textbf{X}_{i}^{\prime}\boldsymbol{\beta}\\ -->
<!--                             \mu_{i}= & \mu\left(\textbf{X}_{i},\boldsymbol{\beta}\right)=\exp\left(\textbf{X}_{i}^{\prime}\boldsymbol{\beta}\right)\\ -->
<!--                             \mu_{i}= & \mu\left(\textbf{X}_{i},\boldsymbol{\beta}\right)=\ln\left(\textbf{X}_{i}^{\prime}\boldsymbol{\beta}\right) -->
<!--                             \end{align*} -->
<!--                             In all three cases, the mean response $\mu_{i}$ must be nonnegative. -->
<!--                             Since the distribution of the error terms $\varepsilon_{i}$ for Poisson -->
<!--                             regression is a function of the distribution of the response $y_{i}$, -->
<!--                             which is Poisson, it is easiest to state the Poisson regression model -->
<!--                             in the following form: -->
<!--                             $y_{i}$ are independent Poisson random variables with expected values $\mu_{i}$ where -->
<!--                             \begin{align*} -->
<!--                             \mu_{i} & =\mu\left(\textbf{X}_{i},\boldsymbol{\beta}\right) -->
<!--                             \end{align*} -->
<!--                             The most commonly used response function is $\mu_{i}=\exp\left(\textbf{X}_{i}^{\prime}\boldsymbol{\beta}\right)$. -->
<!-- \begin{example} -->
<!-- The Miller Lumber Company is a large retailer of lumber and paint, as well as of plumbing, -->
<!--                             electrical, and other household supplies. -->
<!-- \marginpar{} -->
<!--                             During a representative two-week period, in-store -->
<!--                             surveys were conducted and addresses of customers were obtained. The addresses were -->
<!--                             then used to identify the metropolitan area census tracts in which the customers reside. -->
<!--                             At -->
<!--                             the end of the survey period, the total number of customers who visited the store from each -->
<!--                             census tract within a 10-mile radius was determined and relevant demographic information -->
<!--                             for each tract (average income, number of housing units, etc.) was obtained. -->
<!--                             Several other -->
<!--                             variables expected to be related to customer counts were constructed from maps, including -->
<!--                             distance from census tract to nearest competitor and distance to store. -->
<!--                             Initial screening of the potential predictor variables was conducted which led to the -->
<!--                             retention of five predictor variables: -->
<!--                             - $x_1$: Number of housing units -->
<!--                             - $x_2$: Average income, in dollars -->
<!--                             - $x_3$: Average housing unit age, in years -->
<!--                             - $x_4$ : Distance to nearest competitor, in miles -->
<!--                             - $x_5$: Distance to store, in miles -->
<!--                             - $y_i$ : Number of customers who visited store from census tract -->
<!-- \begin{lstlisting} -->

<!-- dat = read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/ -->
<!--                textdatasets/KutnerData/Chapter%2014%20Data%20Sets/CH14TA14.txt") -->
<!-- names(dat) = c("Y", "X1", "X2", "X3", "X4", "X5") -->
<!-- reg = glm(Y~., family = "poisson", data=dat) -->
<!-- summary(reg) -->
<!-- Call: -->
<!-- glm(formula = Y ~ ., family = "poisson", data = dat) -->
<!-- Deviance Residuals:  -->
<!--      Min        1Q    Median        3Q       Max   -->
<!-- -2.93195  -0.58868  -0.00009   0.59269   2.23441   -->
<!-- Coefficients: -->
<!--               Estimate Std. Error z value Pr(>|z|)     -->
<!-- (Intercept)  2.942e+00  2.072e-01  14.198  < 2e-16 *** -->
<!-- X1           6.058e-04  1.421e-04   4.262 2.02e-05 *** -->
<!-- X2          -1.169e-05  2.112e-06  -5.534 3.13e-08 *** -->
<!-- X3          -3.726e-03  1.782e-03  -2.091   0.0365 *   -->
<!-- X4           1.684e-01  2.577e-02   6.534 6.39e-11 *** -->
<!-- X5          -1.288e-01  1.620e-02  -7.948 1.89e-15 *** -->
<!-- --- -->
<!-- Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- (Dispersion parameter for poisson family taken to be 1) -->
<!--     Null deviance: 422.22  on 109  degrees of freedom -->
<!-- Residual deviance: 114.99  on 104  degrees of freedom -->
<!-- AIC: 571.02 -->
<!-- Number of Fisher Scoring iterations: 4 -->
<!-- predict(reg) -->
<!--        1        2        3        4        5        6        7  -->
<!-- 2.512666 2.171006 3.336689 2.129078 1.982460 2.184004 1.458190  -->
<!--        8        9       10  -->
<!-- 2.397791 2.670277 2.453965  -->
<!-- \end{lstlisting} -->
<!-- \end{example} -->
<!-- \marginpar{} -->


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./22_Validation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Model Validation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">STA 3386</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">Fall 2024</div>
  </div>
</footer>



</body></html>